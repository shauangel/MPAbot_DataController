{"result": [{"answers": [{"content": "The sorted() function takes a key= parameter Alternatively, you can use operator.itemgetter instead of defining the function yourself For completeness, add reverse=True to sort in descending order", "id": 73050, "owner_tier": 0.9, "score": 0.9999999999972804}, {"content": "I have been a big fan of a filter with lambda. However, it is not best option if you consider time complexity. 1000000 loops, best of 3: 0.736 \u00b5sec per loop 1000000 loops, best of 3: 0.438 \u00b5sec per loop", "id": 58179903, "owner_tier": 0.1, "score": 0.004079412561871091}, {"content": "You can sort a list of dictionaries with a key as shown below: Output: In addition, you can sort a list of dictionaries with a key and a list of values as shown below: Output:", "id": 76250224, "owner_tier": 0.1, "score": 0.0010878433478379113}, {"content": "It might be better to use dict.get() to fetch the values to sort by in the sorting key. One way it's better than dict[] is that a default value may be used to if a key is missing in some dictionary in the list. For example, if a list of dicts were sorted by 'age' but 'age' was missing in some dict, that dict can either be pushed to the back of the sorted list (or to the front) by simply passing inf as a default value to dict.get().", "id": 75935637, "owner_tier": 0.5, "score": 0.0010878433478379113}, {"content": "You can use the following:", "id": 75367181, "owner_tier": 0.5, "score": -2.7196083598654582e-12}, {"content": "Sometime we need to use lower() for case-insensitive sorting. For example,", "id": 45094029, "owner_tier": 0.5, "score": 0.00734294261354365}, {"content": "sorting by multiple columns, while in descending order on some of them:\nthe cmps array is global to the cmp function, containing field names and inv == -1 for desc 1 for asc", "id": 72939809, "owner_tier": 0.3, "score": -2.7196083598654582e-12}, {"content": "As indicated by @Claudiu to @monojohnny in comment section of this answer, given: to sort the list of dictionaries by key 'age', 'name'\n(like in SQL statement ORDER BY age, name), you can use: or, likewise print(newlist) [{'name': 'Bart', 'age': 10},  {'name': 'Milhouse', 'age': 10},\n{'name': 'Homer', 'age': 39}]", "id": 69072597, "owner_tier": 0.5, "score": 0.0005439216725591515}], "link": "https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python", "question": {"content": "How do I sort a list of dictionaries by a specific key's value? Given: When sorted by name, it should become:", "id": 72899, "title": "How to sort a list of dictionaries by a value of the dictionary in Python?", "traffic_rate": 245}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "list", "sorting", "dictionary", "data-structures"]}, {"answers": [{"content": "Python has a rule on methods. Either: Mutating methods never return the original object largely as a stylistic choice by Guido van Rossum (the original creator of Python); if a mutated object is returned, he wanted there to be no confusion over whether it was the original object (mutated in-place) or new mutated copy. Doing many different things on a single line was also something he wanted to discourage; without reverse=True, the cleaner way to write it would be: as two lines each doing one unique thing. That general rule is the case for all objects, not just list, but list.sort/list.reverse explicitly document this rationale: To remind users that [list.sort] operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request a new sorted list instance). For the specific case of list.sort, using reverse=True provides a couple benefits: Admittedly, the Python standard sorting algorithm (Timsort) (in place by the time reverse=True was added), optimizes for runs of data sorted in either direction (forward or reverse), so repeated sort-then-reverse would still perform well, but even so, it's costly and pointless to rearrange all the elements of the input only to swap them back again at the end. Sure, in a hypothetical design Python could have made bound list.sort methods a more complex object with methods of its own that tweaked the behavior, making X.sort.reverse() exactly equal to X.sort(reverse=True) (both would sort in reverse order directly without sorting in forward order, then reversing), but no other built-in type in Python does anything like that (bound methods can be called, and that's it, they don't provide extensible objects that can be tweaked). For that matter, I'm not aware of many other languages that allow anything similar (I wouldn't put it past some pure-functional languages, but Python is not pure-functional). Supporting this would complicate a lot of stuff (e.g. you can't just have a common interface for methods, you have to custom-design the descriptors and the objects they return to make new objects with methods of their own; that's a lot of work relative to the existing CPython design that basically just requires a C function definition, plus a one-liner macro mapping the string \"sort\" to that function for the purposes of method lookup, with the process of creating bound methods handled by a single uniform code path for every C-implemented method) and would introduce questions about consistency (should you also allow X.reverse.sort() to mean the same thing as X.sort.reverse()? If not, why not?), violate expectations from tons of other methods which use keyword arguments to make slight tweaks in behavior, and not provide any meaningful benefit. In a language without keyword arguments, sure, this might be useful (mylist.sort(True) or mylist.sort(False, lambda x: x[0]) for forward order with a key would be ugly), but with keyword arguments, X.sort(reverse=True) is not meaningfully worse than X.sort.reverse(), and unlike the latter, it doesn't create confusion between bound methods and full-fledged objects with methods of their own. As an historical note, prior to the introduction of TimSort in 2.3, Python used an unstable samplesort algorithm (with no optimizations for partially ordered data), and only supported a cmp argument (a function that accepted two values and returned a negative, 0, or positive number to indicate if the first argument was less than, equal to, or greater than the second). It was only when 2.4 introduced the key argument (to allow Python to do the far more efficient decorate-sort-undecorate, aka Schwartzian Transform under the hood for you) that the reverse argument was added alongside. When cmp was the only way to customize the comparison, you'd just write it to tweak the sort order yourself as needed, but with key functions, where you compute/extract new objects (typically simple built-ins like int, str, or tuples thereof), you could only say what to compare on, not how to compare that data; it's fairly common to want to sort on the key in reverse order, and if that had required writing a cmp function as well, just to flip the sort order, it would have eliminated most of the benefits of key functions (specifically, that they are called before sorting exactly once per input, where cmp functions must be called, on average, log n times per element). The newly stable sort also meant that, in the event you needed some comparisons to operate in reverse, and some in forward order (and you couldn't just invert values; int can be negated, but str cannot be), you could call sort repeatedly, from least important criteria to most, and the subsequent sorts will preserve the original order of elements that compare equal for that stage (and therefore the ordering imposed by the prior sorts). It's not great when you need to do this, but it's better than also having to add explicit reverse calls on top of all the multiple sort calls.", "id": 75187834, "owner_tier": 0.9, "score": 0.9999999900000002}], "link": "https://stackoverflow.com/questions/75177279/what-is-the-rationale-for-using-x-sort-reverse-true-and-not-x-sort-reverse", "question": {"content": "I have a list called X that I want to sort alphabetically in descending order\nand i want to know the rationale of \"reverse = True\" I would have written the following: But I see that it doesn't work and I want to know what it means to assign true to reverse", "id": 75177279, "title": "what is the rationale for using X.sort.(reverse=True) and not X.sort.reverse()", "traffic_rate": 272}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "python-3.x"]}, {"answers": [{"content": "While there should normally be no reason not to use the key argument for the sorted function or list.sort method, you can of course do without it, by creating a list of pairs (called tmp below) where the first item is the sort key and the second item is the original item. Due to lexicographical sorting, sorting this list will sort by the key first. Then you can take the items in the desired order from the sorted tmp list of pairs. Note that usually this would be written with list comprehensions instead of calling .append in a loop, but the purpose of this answer is to illustrate the underlying algorithm in a way most likely to be understood by beginners.", "id": 74964521, "owner_tier": 0.7, "score": 0.0}], "link": "https://stackoverflow.com/questions/44749903/how-do-i-implement-a-schwartzian-transform-in-python", "question": {"content": "In Perl I sometimes use the Schwartzian Transform to efficiently sort complex arrays: How to implement this transform in Python?", "id": 44749903, "title": "How do I implement a Schwartzian Transform in Python?", "traffic_rate": 0}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "sorting"]}, {"answers": [{"content": "I think this has something to do with the way the hasLayout property is implemented in the older browser. Have you tried your code in IE8 to see if works in there, too?\nIE8 has a Debugger (F12) and can also run in IE7 mode.", "id": 3998843, "owner_tier": 0.3, "score": 0.9999999997222222}], "link": "https://stackoverflow.com/questions/6/why-did-the-width-collapse-in-the-percentage-width-child-element-in-an-absolutel", "question": {"content": "I have an absolutely positioned div containing several children, one of which is a relatively positioned div. When I use a percentage-based width on the child div, it collapses to 0 width on IE7, but not on Firefox or Safari. If I use pixel width, it works. If the parent is relatively positioned, the percentage width on the child works.", "id": 6, "title": "Why did the width collapse in the percentage width child element in an absolutely positioned parent on Internet Explorer 7?", "traffic_rate": 5}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["html", "css", "internet-explorer-7"]}, {"answers": [], "link": "https://stackoverflow.com/questions/61683696/how-to-sort-hierarchical-data-represented-in-a-flat-list", "question": {"content": "I have a Python (version 2.7) list of objects that has an implied hierarchy. If one or more SubThings immediately follow a Thing, they belong with that Thing. I want to sort the Things by their value and when Things are moved for the sorting, I want their SubThings to move with them. All of these objects have values. Example input: I have working code for this in Python 2.7 but it's brute-force and feels inelegant - about 40 lines of code. First I create an intermediate data structure that groups Things with their SubThings, then I sort by Things' values, then I flatten the resulting structure. I have a feeling there is an elegant one or two (or three?)-liner for this. It even sounds like a classic opportunity for a Schwartzian Transform but I'm not making the \"Pythonic\" leap that easily groups SubThings with Things - maybe something with itertools.groupby()? For clarity: SubThings never occur without a parent Thing. Things may not have SubThings. I've simplified by leaving out the reality that the series of Things/SubThings can be preceded and followed by unrelated objects. It'd be awesome to see a solution that passes those through unsorted, i.e. in the position they were, but that's not as intellectually challenging to me.", "id": 61683696, "title": "How to sort hierarchical data represented in a flat list?", "traffic_rate": 452}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "sorting", "data-structures"]}, {"answers": [], "link": "https://stackoverflow.com/questions/46860219/how-do-i-make-a-merge-sort-built-for-a-single-list-sort-a-list-of-list-inste", "question": {"content": "So, let say I have a list of lists like this: If I wanted to sort this by the middle number in each sublist, it would end up like this: How can I cause a merge sort that is built to sort a list like this: [1, 2, 3, 4] To sort a list of list and specify it to merge based on the middle number of each sublist? Is there a trick with python that will allow this to work? Only thing I have been able to find about this is using the built-in sorting function, which I do not want to do. Below is the merge sort I am implementing. It works like this  mergeSort(list_here) And it merges the list. However, I would like to use a trick to make python merge based on a list of the list instead with the middle index of each sublist being what is being compared. Thanks for any guidance. Below is the merge sort:", "id": 46860219, "title": "How do I make a merge sort, built for a single list, sort a &quot;list of list&quot; instead?", "traffic_rate": 383}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python"]}, {"answers": [{"content": "The theoretical minimum number of comparisons needed to sort an array of n elements on average is lg (n!), which is about n lg n - n. There's no way to do better than this on average if you're using comparisons to order the elements. Of the standard O(n log n) comparison-based sorting algorithms, mergesort makes the lowest number of comparisons (just about n lg n, compared with about 1.44 n lg n for quicksort and about n lg n + 2n for heapsort), so it might be a good algorithm to use as a starting point.  Typically mergesort is slower than heapsort and quicksort, but that's usually under the assumption that comparisons are fast. If you do use mergesort, I'd recommend using an adaptive variant of mergesort like natural mergesort so that if the data is mostly sorted, the number of comparisons is closer to linear. There are a few other options available.  If you know for a fact that the data is already mostly sorted, you could use insertion sort or a standard variation of heapsort to try to speed up the sorting.  Alternatively, you could use mergesort but use an optimal sorting network as a base case when n is small.  This might shave off enough comparisons to give you a noticeable performance boost. Hope this helps! ", "id": 18386733, "owner_tier": 0.9, "score": 0.8888888877777777}, {"content": "Quicksort and mergesort are the fastest possible sorting algorithm, unless you have some additional information about the elements you want to sort. They will need O(n log(n)) comparisons, where n is the size of your array.\nIt is mathematically proved that any generic sorting algorithm cannot be more efficient than that. If you want to make the procedure faster, you might consider adding some metadata to accelerate the computation (can't be more precise unless you are, too). If you know something stronger, such as the existence of a maximum and a minimum, you can use faster sorting algorithms, such as radix sort or bucket sort. You can look for all the mentioned algorithms on wikipedia. As far as I know, you can't benefit from the expensive relationship. Even if you know that, you still need to perform such comparisons. As I said, you'd better try and cache some results. EDIT I took some time to think about it, and I came up with a slightly customized solution, that I think will make the minimum possible amount of expensive comparisons, but totally disregards the overall number of comparisons. It will make at most (n-m)*log(k) expensive comparisons, where Here is the description of the algorithm. It's worth nothing saying that it will perform much worse than a simple merge sort, unless m is big and k is little. The total running time is O[n^4 + E(n-m)log(k)], where E is the cost of an expensive comparison (I assumed E >> n, to prevent it from being wiped out from the asymptotic notation. That n^4 can probably be further reduced, at least in the mean case. EDIT The file I posted contained some errors. While trying it, I also fixed them (I overlooked the pseudocode for insert_sorted function, but the idea was correct. I made a Java program that sorts a vector of integers, with delays added as you described. Even if I was skeptical, it actually does better than mergesort, if the delay is significant (I used 1s delay agains integer comparison, which usually takes nanoseconds to execute)", "id": 18381740, "owner_tier": 0.5, "score": -1.111111111111111e-09}, {"content": "We can look at your problem in the another direction, Seems your problem is IO related, then you can use advantage of parallel sorting algorithms, In fact you can run many many threads to run comparison on files, then sort them by one of a best known parallel algorithms like Sample sort algorithm.", "id": 18400500, "owner_tier": 0.7, "score": -1.111111111111111e-09}, {"content": "Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? Merge insertion algorithm, described in D. Knuth's \"The art of computer programming\", Vol 3, chapter 5.3.1, uses less comparisons than other comparison-based algorithms. But still it needs O(N log N) comparisons. Would the existence of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? I think some of existing sorting algorithms may be modified to take into account expensive(i,j) predicate. Let's take the simplest of them - insertion sort. One of its variants, named in Wikipedia as binary insertion sort, uses only O(N log N) comparisons. It employs a binary search to determine the correct location to insert new elements. We could apply expensive(i,j) predicate after each binary search step to determine if it is cheap to compare the inserted element with \"middle\" element found in binary search step. If it is expensive we could try the \"middle\" element's neighbors, then their neighbors, etc. If no cheap comparisons could be found we just return to the \"middle\" element and perform expensive comparison. There are several possible optimizations. If predicate and/or cheap comparisons are not so cheap we could roll back to the \"middle\" element earlier than all other possibilities are tried. Also if move operations cannot be considered as very cheap, we could use some order statistics data structure (like Indexable skiplist) do reduce insertion cost to O(N log N). This modified insertion sort needs O(N log N) time for data movement, O(N2) predicate computations and cheap comparisons and O(N log N) expensive comparisons in the worst case. But more likely there would be only O(N log N) predicates and cheap comparisons and O(1) expensive comparisons. Consider a set of possibly large files. In this application the goal is to find duplicate files among them. If the only goal is to find duplicates, I think sorting (at least comparison sorting) is not necessary. You could just distribute the files between buckets depending on hash value computed for first megabyte of data from each file. If there are more than one file in some bucket, take other 10, 100, 1000, ... megabytes. If still more than one file in some bucket, compare them byte-by-byte. Actually this procedure is similar to radix sort.", "id": 18399928, "owner_tier": 0.7, "score": 0.11111111}, {"content": "Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? Edit: Ah, sorry. There are algorithms that minimize the number of comparisons (below), but not that I know of for specific elements. Would the existence of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? Not that I know of, but perhaps you'll find it in these papers below. I'd like pointers to further material on this topic. On Optimal and E\ufb03cient in Place Merging Stable Minimum Storage Merging by Symmetric Comparisons  Optimal Stable Merging (this one seems to be O(n log2 n) though Practical In-Place Mergesort If you implement any of them, posting them here might be useful for others too! :)", "id": 18387777, "owner_tier": 0.9, "score": 0.22222222111111112}, {"content": "Something to keep in mind is that if you are continuously sorting the list with new additions, and the comparison between two elements is guaranteed to never change, you can memoize the comparison operation which will lead to a performance increase. In most cases this won't be applicable, unfortunately.", "id": 18387652, "owner_tier": 0.1, "score": -1.111111111111111e-09}, {"content": "I'll try to answer each question as best as I can. Traditional sorting methods may have some variation, but in general, there is a mathematical limit to the minimum number of comparisons necessary to sort a list, and most algorithms take advantage of that, since comparisons are often not inexpensive. You could try sorting by something else, or try using a shortcut that may be faster that may approximate the real solution. I don't think you can get around the necessity of doing at least the minimum number of comparisons, but you may be able to change what you compare. If you can compare hashes or subsets of the data instead of the whole thing, that could certainly be helpful. Anything you can do to simplify the comparison operation will make a big difference, but without knowing specific details of the data, it's hard to suggest specific solutions. Check these out:", "id": 18381778, "owner_tier": 0.5, "score": 0.9999999988888888}, {"content": "A technique called the Schwartzian transform can be used to reduce any sorting problem to that of sorting integers. It requires you to apply a function f to each of your input items, where f(x) < f(y) if and only if x < y. (Python-oriented answer, when I thought the question was tagged [python]) If you can define a function f such that f(x) < f(y) if and only if x < y, then you can sort using Python guarantees that key is called at most once for each element of the iterable you are sorting. This provides support for the Schwartzian transform. Python 3 does not support specifying a cmp function, only the key parameter. This page provides a way of easily converting any cmp function to a key function.", "id": 18381772, "owner_tier": 0.9, "score": 0.4444444433333333}, {"content": "Most sorting algorithm out there try minimize the amount of comparisons during sorting. My advice:\nPick quick-sort as a base algorithm and memorize results of comparisons just in case you happen to compare the same problems again. This should help you in the O(N^2) worst case of quick-sort. Bear in mind that this will make you use O(N^2) memory.   Now if you are really adventurous you could try the Dual-Pivot quick-sort.", "id": 18381766, "owner_tier": 0.5, "score": -1.111111111111111e-09}], "link": "https://stackoverflow.com/questions/18381354/what-sorting-techniques-can-i-use-when-comparing-elements-is-expensive", "question": {"content": "I have an application where I want to sort an array a of elements a0, a1,...,an-1. I have a comparison function cmp(i,j) that compares elements ai and aj and a swap function swap(i,j), that swaps elements ai and aj of the array. In the application, execution of the cmp(i,j) function might be extremely expensive, to the point where one execution of cmp(i,j) takes longer than any other steps in the sort (except for other cmp(i,j) calls, of course) together. You may think of cmp(i,j) as a rather lengthy IO operation. Please assume for the sake of this question that there is no way to make cmp(i,j) faster. Assume all optimizations that could possibly make cmp(i,j) faster have already been done. Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? It is possible in my application to write a predicate expensive(i,j) that is true iff a call to cmp(i,j) would take a long time. expensive(i,j) is cheap and expensive(i,j) \u2227 expensive(j,k) \u2192 expensive(i,k) mostly holds in my current application. This is not guaranteed though. Would the existance of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? I'd like pointers to further material on this topic. This is an example that is not entirely unlike the application I have. Consider a set of possibly large files. In this application the goal is to find duplicate files among them. This essentially boils down to sorting the files by some arbitrary criterium and then traversing them in order, outputting sequences of equal files that were encountered. Of course reader in large amounts of data is expensive, therefor one can, for instance, only read the first megabyte of each file and calculate a hash function on this data. If the files compare equal, so do the hashes, but the reverse may not hold. Two large file could only differ in one byte near the end. The implementation of expensive(i,j) in this case is simply a check whether the hashes are equal. If they are, an expensive deep comparison is neccessary.", "id": 18381354, "title": "What sorting techniques can I use when comparing elements is expensive?", "traffic_rate": 1039}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["performance", "algorithm", "sorting", "comparison"]}, {"answers": [], "link": "https://stackoverflow.com/questions/5760733/perl-in-place-sort-lines-in-a-text-file", "question": {"content": "I wish to modify a text file by sorting each line based on a given key and save the old file as a backup.  The key is a numeric character contained in each line.   Is there a simple script to get this done, preferably in-place? Thanks!", "id": 5760733, "title": "perl in-place sort lines in a text file", "traffic_rate": 711}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "perl"]}, {"answers": [], "link": "https://stackoverflow.com/questions/2019951/how-do-i-sort-this-list", "question": {"content": "I have a list of lists. Now I want to sort li in such a way that higher the foo(x) for a x higher it should appear in a sorted list. What is the best way in C#/Python/any other lang to this?", "id": 2019951, "title": "How do I sort this list?", "traffic_rate": 694}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["c#", "python", "ruby", "sorting", "haskell"]}, {"answers": [{"content": "I love Tries, so just for fun, I wrote a Trie-based solution : This should work for any string of any length. Note that the result is just printed to screen, not written back in a new list. You didn't write much code, so I'll leave it as an exercise ;)", "id": 42902140, "owner_tier": 0.9, "score": -1.4285714285714286e-09}, {"content": "Here is one way to do it: Traditionally, lexicographic sort order longer strings after their otherwise identical prefixes (i.e.  'abc' goes before 'abcd'). To meet your sort expectation, we first \"fix-up\" the shorter string by adding the remaining part of the longer string plus another character to make it the longer of the two: The functools.cmp_to_key() tool then converts the comparison function to a key function. This may seem like a lot of work, but the sort expectations are very much at odds with the built-in lexicographic sorting rules. FWIW, here's another way of writing it, that might or might not be considered clearer: The logic is:", "id": 42900060, "owner_tier": 0.9, "score": 0.9999999985714286}, {"content": "My first answer was: just negate the len criterion to reverse only on that criterion. But that doesn't work, because there's a conflict between alpha sort and length. Alpha sort puts small strings first. So length criterion doesn't work. You need to merge both criteria. There's no clear priority between each other. I found a way: first compute the max length of your strings, then return the chr(127) filled (the biggest char provided you're using only ASCII) version of the string as key so smallest strings are filled with big chars in the end: they always come last. result: BTW don't call your list list for obvious reasons.", "id": 42899432, "owner_tier": 0.9, "score": 0.14285714142857142}, {"content": "One could construct the key by taking: For example:", "id": 42899718, "owner_tier": 0.5, "score": -1.4285714285714286e-09}], "link": "https://stackoverflow.com/questions/42899405/sort-a-list-with-longest-items-first", "question": {"content": "I am using a lambda to modify the behaviour of sort. Sorting a list containing the elements A1,A2,A3,A,B1,B2,B3,B, the result is A,A1,A2,A3,B,B1,B2,B3. My expected sorted list would be A1,A2,A3,A,B1,B2,B3,B. I've already tried to include the len(item) for sorting, which didn't work. How to modify the lambda so that the sort result is instead? ", "id": 42899405, "title": "Sort a list with longest items first", "traffic_rate": 1424}, "saved_time": 1721102271, "source": "stackoverflow", "tags": ["python", "python-3.x", "sorting", "lambda"]}, {"answers": [{"content": "As you've more or less stated, the `sort` package assumes that you'll be sorting items based on their values, or values that are at the ready.  You don't go into too much detail about exactly the kinds of values or objects you were sorting, and why their \"true value\", which is apparently some int value, wasn't simply precomputed and already available.  But I can believe there are good reasons for it; perhaps that value is simply never necessary after the values are in order.\n\nGoing back to this \"Schwartzian transform\", it's a nice idiom in Perl for sure.  But even in Go, the general approach is pretty sound.  All you're doing is making a new list of values easier to compare, each value corresponding to an element in the original list, and sorting by proxy.  Here's an example of a generalized way of doing that, applied to your example: http://play.golang.org/p/ph2EOXC9pd\n\nOne of the major advantages of the `sort` package's approach with its `Interface` type is that it lets one defined type act upon itself without bothering with a great deal of `interface{}` conversions.  For example, it's really cheap for the IntSlice type to compare two elements with `x.Less(i,j)`.  Must less expensive than calling `x.LessVal(x.Key(i), x.Key(j))`, even if your key values have been cached.\n\nTake that advice as you wish.  But, concretely, there are some things in your approach you should fix.  You're really not gaining anything but complexity by trying to lazily initialize your Key values, because you will need to compute all of them.  It also doesn't make much sense to use a `map[int]T` when the values of your keys are bounded, and you expect to use all values of 0..N.  In short, use a slice.  By using a slice, you can \"prime\" all your values in parallel without worrying about synchronizing access.\n", "id": "cu4gdm7", "owner_tier": 0.5, "score": 0.14285714142857142}, {"content": "\"sort\" is a really interesting case, because it shows off both the parts of generics that Go does have, and shows off the parts it does not. Contrary to popular opinion, Go does have some \"generics\", in that interfaces can provide \"generic algorithms\" ignorant of the underlying implementation, which the sort package does. What it does not have are generic types.\n\n(This isn't rhetorical slight of hand trying to make Go look good... in my considered opinion it's actually important to understand that \"generic\" is not an atomic thing on its own and consists of many parts, or you don't really understand how to use them in any language.)\n\nConsequently, one of the answers to the question \"How can you program in a language missing generics?\" is that it's only half-missing generics, and arguably has the more useful half. (And arguably does not. I mean the \"arguably\" literally.)\n\nLack of generic types has a lot of consequences here. One of them is that trying to reuse `sort` code is actually a pain because it's virtually impossible to express the 'real' types. The other one is that because of that, the real \"Go\" thing to do here is to... not. When you can't use the type system to reuse code, Go's pragmatism is that maybe you just need to duplicate a bit of code.[1]\n\nThe Keysort package should go ahead and simply provide its own sorting routines. Start by copying & pasting the sort code if you like. It's OK, it'll end up diverging enough to make it worthwhile. Then you can fix all those other problems you mention, too. It's OK. It is not part of the Go philosophy that, having provided \"sort\" in the core library, you must always and only use that sort. It's the Go philosophy that you ought to have a really good reason to use another implementation and not just reach willy-nilly for half-a-dozen sort routines because they're cool or Web Scale, but if you really do have the need, go for it. I suspect you'll find quite a few things you can do that makes this approach work better for you.\n\nAlso, regardless of the fact that it's called `sort.Interface` in the core library, I would consider calling an interface `Interface` to be a code smell, as silly as `type Struct struct`. That's just old Go code that can't be changed now ([copyright shows as 2009](https://golang.org/src/sort/sort.go)) but probably shouldn't be copied.\n\n[1]: To be honest, this is where Go's pragmatism and I have some disagreements, but I'll concede it works better than I'd expect. Still, Go is clearly not averse to some copying and pasting, and my current score for \"core libraries I've copied and started modifying for some very good reason\" is at 3. It turns out that they can provide a really nice base of working code for you to mutate if you need to... the vast bulk of them are quite nice under the hood, and well covered in unit tests, and it's been a pleasure to start with them rather than start from scratch.", "id": "cu4xiyq", "owner_tier": 0.7, "score": -1.428571419889327e-09}, {"content": "I wish this was simpler in go.\n\n    sort.SortByKey(people, func( p *Person ) { return p.Age } )\n\nGuess that needs generics though :(\n\nYour helper func doesn't define a return type to be compared.  Imposing a return type on that would break the generalized functionality provided by the `Less(i, j int) bool` function in the `sort.Interface` type.\n\nIn Python, `sort(people, key=lambda p: p.age)` works because `cmp(a.age, b.age)` is probably going to return _something_ usable.  But in Go, you need to do something which works with statically defined types.\n\nThe upshot is that with complicated structs you can define an order of precedence of attributes.  E.g.,\n\n    type sortPeople []*Person\n    \n    func (s sortPeople) Less(i, j int) bool {\n        a, b := s[i],  s[j]\n        return a.Age < b.Age || a.Name < b.Name || a.Phone < b.Phone\n    }\n    \n\nYour general idea is fine, but your Less implementation is not going to behave how you expect. You'll want something like this:\n\n    func (s sortPeople) Less(i, j int) bool {\n        a, b := s[i],  s[j]\n        if a.Age != b.Age { return a.Age < b.Age }\n        if a.Name != b.Name { return a.Name < b.Name }\n        return a.Phone < b.Phone\n    }\n\nI don't disagree with what you said, I just wish it was simpler.  I'm not an expect on the finer points of types an interfaces, but couldn't go implement an Orderable interface for types to enable this sort of thing?  I think that is how things work in something like haskell with `Ord`...\n\n\tfmt.Println(1 < 10)\n\tfmt.Println(\"bob\" < \"alice\")\n\nworks, but you can't write\n\n    func isLessThan(a, b: Orderable) {\n        return a < b\n    }\n\nIs this the generics issue again? I don't really care what it is called, just that writing\n\n    sort.SortByKey(people, func( p *Person ) { return p.Age } )\n    sort.SortByKey(people, func( p *Person ) { return p.FirstName } )\n    sort.SortByKey(people, func( p *Person ) { return p.LastName } )\n\nshouldn't be any more complicated than that.  Go knows how to sort numbers and strings, I'm giving it a function that converts from a Person to something it already knows how to sort.  Why can't it just sort it?\n\nI have a feeling I am going to get downvoted again for using the work 'generics'.. I bet that in a year or two this will be possible and people will say things like \"Remember when we had to write 10 lines of code to sort arrays by different things?\"\n\nYour `SortByProxy` is nice though, I modified it for the Person example.  It gets things down to:\n\n\tproxy := make([]int, len(needsSorting))\n\tfor i := range needsSorting {\n\t\tproxy[i] = needsSorting[i].Age\n\t}\n\tSortByProxy(needsSorting, sort.IntSlice(proxy))\n\t\n\n\tfproxy := make([]string, len(needsSorting))\n\tfor i := range needsSorting {\n\t\tfproxy[i] = needsSorting[i].FirstName\n\t}\n\tSortByProxy(needsSorting, sort.StringSlice(fproxy))\n\nThen I wrote:\n\n    func (pl People) BuildStringProxy(tf func(p *Person) string) sort.StringSlice {\n\tproxy := make([]string, len(pl))\n\tfor i, person := range pl {\n\t\tproxy[i] = tf(&person)\n\t}\n\treturn sort.StringSlice(proxy)\n    }\n\nWhich gets it down to\n\n\tSortByProxy(needsSorting, needsSorting.BuildStringProxy(func(p *Person) string { return p.FirstName }))\n\tSortByProxy(needsSorting, needsSorting.BuildStringProxy(func(p *Person) string { return p.LastName }))\n\nWhich isn't too far off from what I would like to see.\n\nYup, too hasty an example.", "id": "cu4dfc1", "owner_tier": 0.5, "score": 0.9999999985714284}], "link": "https://www.reddit.com/r/golang/comments/3h4ccr/keysort_the_schwartzian_transform_in_go/", "question": {"content": "", "id": "3h4ccr", "title": "Keysort: The Schwartzian Transform in Go", "traffic_rate": 48.87912702853945}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "Functions are objects in Python, and can be used like any other type.  Passing a function as an argument works pretty much the same was as passing a function pointer in C++.\n\nThe `key` parameter specifies a function that the sort algorithm will use to transform all the values in the sequence prior to sorting it, in effect creating a new temporary list that contains the values that will be used for comparisons.  This is how you customize sorting.  This technique is actually much more powerful than the usual technique in C++ of passing a comparison function, and it can result in much higher performance.  In essence it's a case of the [Schwartzian transform](https://en.wikipedia.org/wiki/Schwartzian_transform).\n\nThe `key` used here returns the last element in whatever is passed to it.  The first thing the sort algorithm does is call the key function on each element in the input sequence to create a temporary list of the resulting return values.  Then that temporary list is used as the keys for comparing elements in the original list.  So it's as if you're sorting `[7, 3, 5, 2]`, but each time you swap a pair of elements in the temporary list you also swap the corresponding pair in the real list.\n\nThe reason this can result in higher performance is because this transformation function is called exactly once per item in the list.  The comparison function on the other hand is called more frequently, generally O(n log n).  If the comparison function has to do anything complicated, it makes sense to \"precompute\" all the transforms once prior to starting the sort.  (This is not always necessarily a guaranteed win.)\n\nThis is a very powerful way of customizing the sort.  There are even [specialized helper functions in the `operator` module](https://docs.python.org/3/library/operator.html) meant to make this even easier (and faster, since they're implemented in C.)  This sort could be written even more compactly as:\n\n    import operator\n\n    def sort_last(items):\n        return sorted(items, key=operator.itemgetter(-1))\n\nAnd for example if you had a list of objects that each have some attribute, and you want to sort them by that attribute, you could write:\n\n    items.sort(key=operator.attrgetter('foo'))\n\n(`list.sort()` is the in-place version.)\n\nAnother example is sorting strings while ignoring case:\n\n    foo = sorted(bar, key=str.casefold)\n\n(In versions prior to 3.3 you'd have to use `str.lower` because `str.casefold` did not exist, but that does not provide true, proper case-insensitive sorting.  And hopefully you're not using 2.x.)\n\n\nHow is it that your programming language comprehension seems to be both comprehensive and godlike across many languages? You always seem to show up and answer my questions!\n\nAnyway, your awesome responses, as usual, are really helpful, and also gives me some homework to look into things a bit further.\n\nThanks!", "id": "cyl3bxd", "owner_tier": 0.7, "score": -1.6666666565375481e-09}, {"content": "This sorting function applies that function assigned to key to every item of the sequence and sort that sequence according to return values of that function. In this case it takes every tuple in tuples, returns last number in that tuple and sorts the sequence according to those last numbers.\n\nI should have clarified - I know what is happening, but the syntax is confusing to me. For example - how does last() know that it should operate on an element in the tuple, as opposed to the whole list? Is it not true that when we call [-1] from a list, or a tuple, both are equally valid syntax? Can we just assume that when we use sorted(), it knows that when we give \"key\" an instruction, that this instruction is applied to each entry in the list, and the type is derived from context?\n\nDoes sorted handle multi-dimensional objects - such as a list of tuples of tuples, i.e. \n    \n    complex_tuple = [((1,3),(2,4)),((3,4,6),(1,),(2,5,7)),((0,))]\n\n?\n\n> For example - how does last() know that it should operate on an element in the tuple, as opposed to the whole list? \n\nIt doesn't know that.  It has no idea what's going to be passed to it.  The sort algorithm calls the key function repeatedly, each time passing it one element in the input iterable that's being sorted.  In this case those elements are tuples, so the function is returning the last value in each tuple.  But it would work the same if it was a list of lists, or a list of any other sequence type that supports indexing (e.g. a deque, string, bytes, etc.)\n\nEdit: To address your addendum:\n\n> Does sorted handle multi-dimensional objects - such as a list of tuples of tuples, i.e. \n\nThe sort algorithm doesn't really care what types you pass to it or what they contain.  It's going to try to order each top-level value with `<`.  In your example, the input iterable contains three items, and it's going to simply compare each of them with each other, e.g. it will ask whether `x[0]` is less than `x[1]`, then whether the result of that is less than `x[2]`, and so on.  That's it as far as `sorted()` is concerned; the definition of less-than is not decided by `sorted()`, it's decided by the types being compared and their respective comparison operators, just as in C++ where you overload `operator<()`.\n\nIn the case of tuples, in Python they compare lexicographically, the same as `std::tuple` in C++.\n\n    >>> (1, 2) < (1, 1, 4)\n    False\n    >>> (3, 2) < (1, 2)\n    False\n\nThis applies recursively, so if the first elements of both are tuples, they are compared lexicographically as well:\n\n    >>> ((2, 1), 3) < ((2, 2), 1)\n    True\n\n\nAh okay, so when you have an iterable container, sorted() looks only at the elements in that container, not sub-elements in the case where the iterable container might contain other iterable containers, gotcha. I think I understand better now - since key=last tells \"last\" to try to do something to each element in the list container, and because the return value of tuple[-1] will give us a key value which sorted understands (in this case, numeric) it should do the right thing. Did I get that right?\n\n> because the return value of tuple[-1] will give us a key value which sorted understands (in this case, numeric)\n\nRight.  Except that `sorted()` doesn't have to understand anything really.  Comparing numeric values with less-than does the obvious thing, so `sorted()` is happy and oblivious.\n\nYou don't actually have to return a simple scalar value though; it's a general purpose transformation.  Another neat trick is that the key function can return a tuple.  That's how you specify multiple sort criteria.  Let's say you have some data with several attributes:\n\n    import collections\n\n    Person = collections.namedtuple('Person', 'name age hometown')\n\n    people = [\n        Person('Lisa', 29, 'Omaha, NE'),\n        Person('Robert', 31, 'Fresno, CA'),\n        Person('Alexanda', 31, 'Bridgeport, CT'),\n        Person('Ted', 27, 'El Paso, TX')\n    ]\n\n(The `namedtuple` is a subclass of the tuple that allows access to the fields by name as well as by index.)\n\nIf you ask for `sorted(people)`, you get them sorted by name, because that's the first field of the tuple.\n\n    >>> sorted(people)\n    [Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Robert', age=31, hometown='Fresno, CA'),\n     Person(name='Ted', age=27, hometown='El Paso, TX')]\n\nIn this version without giving a key function it's comparing tuples directly, so it checks the fields in order.  It compares names, and if there were two names that were the same, then ages would be compared, and if those are the same then hometowns would be compared.\n\nSuppose instead you want them sorted by age:\n\n    >>> sorted(people, key=operator.attrgetter('age'))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Robert', age=31, hometown='Fresno, CA'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT')]\n\nSince the key now only contains the age, and because Timsort is stable, when there's a tie there's no more fields to check, and the values retain their original ordering (Robert before Alexandra.)  If you want to sort by age and then by hometown, you can do it by having the key function return a tuple with the attributes in the desired order:\n\n    >>> sorted(people, key=operator.attrgetter('age', 'hometown'))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Robert', age=31, hometown='Fresno, CA')]\n\nThis uses a feature of `attrgetter()` that you can supply multiple attributes and it will return a tuple of the corresponding values, but to be clear you can do this yourself:\n\n    >>> sorted(people, key=lambda person: (person.age, person.hometown))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Robert', age=31, hometown='Fresno, CA')]\n\nThis lambda is a more compact version of defining a function that would look like:\n\n    def key_func(person):\n        return person.age, person.hometown\n\n    sorted(..., key=key_func)\n\nAnd thanks for the gold!\n", "id": "cyl2zvk", "owner_tier": 0.1, "score": 0.9999999983333332}, {"content": "Neat! Always interesting to see the perspective of an experienced programmer first encountering Python.\n\n>Also, the arguments one seems to be able to pass to the sorted() function seems almost magical!\n\nPython has a bunch of cool argument stuff that simply doesn't have a good analogy in C++.\n\nIn C++, if you call a function with the same name twice with different numbers of arguments, I believe that generally means you are calling different overloaded versions of the function with different distinct signatures, and that there are a finite, well-defined number of these functions.\n\nPython has a number of different concepts which largely eliminate the need for function overloading: a single function signature can be made to accommodate an arbitrary number of arguments of arbitrary type. Targets for your google searching in regards to these concepts: positional arguments, optional arguments, keyword arguments.\n\n>is there some cost associated with this?\n\nIf you are talking about performance costs, almost certainly! In Python, however, costs for a feature such as this would usually be considered negligible in the grand scheme of things, and worth the sacrifice for better flexibility, readability, simplicity, ease of use, or even elegance (depending on the feature in question). This is because the Python philosophy tends to value programmer productivity over machine performance.\n\nFinally, \n\n> I've never seen a function passed as an argument to the key argument to sorted. ... how does the sorting algorithm know that I want to access the last piece of each tuple in the list, rather than the last element of the list?\n\nFrom the docs: https://docs.python.org/3/library/functions.html#sorted\n\n>*key* specifies a function of one argument that is used to extract a comparison key from each list element\n\nThis is a bit inaccurate, since it should say a comparison key from each *iterable* element, not each `list` element. You could pass any iterable to `sorted()`.\n\n    def last(list):\n        return list[-1]\n\nSo `last(currentelement)` gets called for every element of whatever is passed to the first argument of `sorted()`. For example, if you wanted to build a list of keys that will be used by the `sorted()` function, you could do something like:\n\n    keys = []\n    for item in mylist:\n        keys.append(last(item))\n\nCalling the argument of `last()` 'list' is a bit of a misnomer, as it is definitely not guaranteed to be a `list` (and furthermore, masks the name of the important built-in function [list](https://docs.python.org/3/library/stdtypes.html#list) ).  Indeed, if you try passing an iterable containing any non-sequence type to your `sort_last` function, such as the tuple `(1, 2, 3, 4)`, you will almost certainly get a runtime error, since a non-sequence type will typically not have subscripting implemented.\n\n    >>> t = (1, 2, 3, 4)\n    None\n    >>> sort_last(t)\n    Traceback (most recent call last):\n      File \"python\", line 1, in <module>\n      File \"python\", line 4, in sort_last\n      File \"python\", line 2, in last\n    TypeError: 'int' object is not subscriptable\n\n(Note I am getting towards the edge of being too tired to post in this subreddit, so I hope not too many mistakes. I've already edited out a couple!)\n\nThanks very much for the help. The documentation for sorted, and your answer makes things more clear for me. One oddity I encountered while poking around with sorting is the difference between\n\n    container.sort()\n\nand\n\n    sorted(container,key=<condition>)\n\nI guess using sorted() seems to be the preferred method, as it preserves the original ordering of the container, and returns a new container. Its cool to have the flexibility to sort the container itself, or get a sorted copy of the container subject to some arbitrary condition.\n\nThanks again!\n\nNo problem. I think someone already discussed the in-place nature of `.sort()`. I think it should be noted that `.sort()` as a method is relatively unique to the `list`class among the built-in iterable data types. `list`s happen to have the combination of traits of being iterable, ordered, and mutable, so an in-place sorting function can kind of make sense.  This is an uncommon combination among the built-ins. For instance, `str`s, are iterable and ordered, but immutable, so no in-place `.sort()` method was implemented (theoretically to avoid confusion). `dict`s, meanwhile, are iterable and mutable, but are not ordered, so again, an in-place `.sort()` doesn't really make sense.\n\nGotcha - so in a sense, the most general thing to use would be sorted() rather than sort(), especially if there is some expectation that an iterable data-type may not be known ahead of time, for some kind of sorting application? This calls to mind use of iterators, from C++ standard template library.\n\n> the most general thing to use would be sorted() rather than sort(), especially if there is some expectation that an iterable data-type may not be known ahead of time\n\nYep, exactly.\n\n>This calls to mind use of iterators, from C++ standard template library.\n\n\nI agree. Oh man, you have no idea how happy I was when [range-based for loops](http://en.cppreference.com/w/cpp/language/range-for) and the [`auto`](http://en.cppreference.com/w/cpp/language/auto) keyword came to C++. `list`s are actually quite similar to `vector`s in their implementation, disregarding the obvious differences of typing.", "id": "cyl4cu3", "owner_tier": 0.3, "score": 0.3333333316666666}], "link": "https://www.reddit.com/r/learnpython/comments/3zcsfh/learning_python_with_google_codes_python_course/", "question": {"content": "[deleted]", "id": "3zcsfh", "title": "Learning python with Google Code's python course, lots of C++ experience, how does key-sorting with tuples work?", "traffic_rate": 153.13018518518518}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "I\u2019d say the lack of built in dependency management.  Between requirements.txt, setup.py, pipenv and now poetry, there is no consistency and it makes the dx worst.\n\nThere is consistency available now, sort of. You can use whatever dependency management system you want, and you specify *what that system is* in the `pyproject.toml`. So long as your dependencies, and your dependency manager, are both compliant with PEP 517 and 518, you shouldn't have any problems installing them.\n\nPip and venv *is* built-in dependency management. `requirements.txt` and `setup.py:setup:install_requires` have different purposes and should be used together. Granted, it's not great (especially coming from the build system nirvana that is rust's `cargo`).\n\nFor what it\u2019s worth, I see light at the end of this particular tunnel.  PEPs 517 and 518 have created a standard with pyproject,toml that solves many past problems. Poetry does a brilliant job actualizing those improvements.\n\n[deleted]\n\nReally could use some improvement or consolidation in this area!\n\nAs far as I\u2019m concerned, dependency management has been solved since ~2015.  Pip and wheels just work.\n\nThere is consistency if you only use one method...\n\nDeveloper experience\n\npip is broken. You have just been lucky.\n\nIt what way?  Just make your software work on multiple versions of dependencies and you won\u2019t have nearly as many issues.  When you create an unresolvable package, yeah, you have issues.\n\nI will admit, I wish pip came with a compiler on Windows.\n\nThe problem is that it can install incompatible versions without realising. Its dependency resolver does not look at the whole tree. It looks as it moves along. \n\nImagine you have a situation like this.\n\n1. you ask pip to install a set of packages (with requirements.txt). The first entry is foo\n2. foo requires bar < 3.0. Pip installs bar 2.4.2 which is the latest not three.\n3. pip continues to check requirements.txt and finds you want quux\n4. quux wants bar > 3.0, <4.0\n5. pip finds that you already have bar 2.4.2 installed, so it proceeds to upgrade it to 3.4.7 (latest of the 3.x series)\n6. Now your foo is broken, and nothing has warned you.\n\nNow I am not sure if the latest pip still behaves like this. It is constantly updated and I know they are working on a depsolver. but my point above is over simplified. It gets even worse when you bring in different platforms (which have different dependencies) and C extensions which have their own ABI.", "id": "g3thjeo", "owner_tier": 0.1, "score": 0.9999999998387097}, {"content": "Not so much a language design mistake, but tkinter.\n\nCan you elaborate?", "id": "g3tkafq", "owner_tier": 0.5, "score": 0.37096774177419356}, {"content": "The expressions for default arguments are evaluated at function definition time rather than call time.\n\nAnd if they're mutable and you mutate them in the function body, it can lead to very subtle bugs.\n\nThere are use cases for this though. Think of a function that behaves as a form of cache where you *want* the mutable default.\n\nYeah I just noticed that the other day when bugbear told me.. B006\nhttps://pypi.org/project/flake8-bugbear/\n\nI agree that there are use cases, but it's an implicit quirk that not many people are aware of, clearly going against python's \"zen\". If I wanted a cache then I'd turn to either decorators or the yield statement based on use-case, as it's more obviously breaking the function's re-entrancy.", "id": "g3tjguo", "owner_tier": 0.3, "score": 0.38709677403225806}, {"content": "I think the type annotation system is really yucky to work with. Having to import List from typing every time you want to declare a list input sucks. Other bolted-on type systems like Flow or TypeScript are much nicer to work with - they feel more terse and easier to compose.\n\nI think I understand why it's like that though. I think the Python maintainers want to keep the code of CPython relatively simple and easy to understand, which is the opposite of the nightmarish toolchain required to do a hello world in a modern JS. Given this, they were limited in what they could add. So perhaps not a \"design mistake\" as much a choice that I don't like.\n\nThat said I wouldn't mind using a preprocessor that strips away type annotations before running the code, if it meant the type annotation system could be better.\n\nFixed in 3.9, you'll be able to annotate types with the actual builtins. Until then, PyCharm can handle imports for you (really wish VScode python would get round to implementing that).\n\nThis one is getting fixed in Python 3.9.\n\nhttps://www.python.org/dev/peps/pep-0585/\n\nYou can just write things like list[str] without having to import a separate List from typing.\n\nI wish type-checking actually mattered at compile time.", "id": "g3tly8r", "owner_tier": 0.5, "score": 0.5483870966129032}, {"content": "Not saying only spaces for indenting, a tab should be a hard compilation error\n\nIIRC mixing tabs & spaces is now a syntax error. Also [black](https://github.com/psf/black) is solving this for you.\n\nI would go even further and raise an error any indentation which is not 4 spaces per level.\n\nEdit: \n\nI agree that this is not possible to change anymore because there is too much code with non PEP-8 compliant identation. This restriction could have only be made in the very beginning.\n\nFortunately, with PyCharm fixing the indentations is only 2 clicks away ;-)\n\nI dont care so much about that as long as it is consistent.  If we want to go down that rabbit hole he should have used s-exper like the lord intended\n\nThat would break most of the Python code at Google with IIRC uses 2 spaces", "id": "g3tgki6", "owner_tier": 0.7, "score": 0.27419354822580644}, {"content": "\nIn an otherwise nice looking language, these stick out like sore thumbs to me:\n\n    __dunders__\n    self.\n\nExplicit self is a great idea. You don't always know all of a class' member variables due to inheritance and the fact that you can add and remove them at runtime.\n\nI meant the syntax, not the concept.", "id": "g3tq6ie", "owner_tier": 0.5, "score": 0.2419354837096774}, {"content": "* `a += b` doesn\u2019t do the same thing as `a = a + b`.\n\n* `async`-`await` is OK, I understand why Python chose that approach over Lua-style coroutines where it\u2019s all implicit. However, `asyncio`, the standard library built on top of the `async`-`await` language feature, is widely disliked.\n\n> a += b doesn\u2019t do the same thing as a = a + b\n\nand it should not. One modifies in place. the other creates a new object and rebinds. That's not a design mistake. it's how programming works.\n\n> a += b doesn\u2019t do the same thing as a = a + b.\n\nexplain?\n\n> it's how programming works.\n\nIt\u2019s how *Python* works, but not all languages.\n\nI consider [C#\u2019s behaviour](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/assignment-operator#compound-assignment) to be more intuitive: \u201c`x op= y` is equivalent to `x = x op y` except that `x` is only evaluated once\u201d.\n\nA simple example:\n\n    >>> a = [1, 2]\n    >>> b = a\n    >>> a = a + [3, 4]\n    >>> print(a, b)\n    [1, 2, 3, 4] [1, 2]\n\n    >>> a = [1, 2]\n    >>> b = a\n    >>> a += [3, 4]\n    >>> print(a, b)\n    [1, 2, 3, 4] [1, 2, 3, 4]\n\nAnd a very strange example:\n\n    >>> t = ([1, 2], 5, 6)\n    >>> t[0] = t[0] + [3, 4]\n    TypeError: 'tuple' object does not support item assignment\n    >>> print(t)\n    ([1, 2], 5, 6)\n\n    >>> t = ([1, 2], 5, 6)\n    >>> t[0] += [3, 4]\n    TypeError: 'tuple' object does not support item assignment\n    >>> print(t)\n    ([1, 2, 3, 4], 5, 6)\n\nIt's two different operators. For immutable objects, the result is the same, but for the mutables, the documented behaviour is that the inplace versions mutate the LHS object in place.\n\nso now my question is why would it evaluate `x` twice in `x = x op y`? Because it's doing an assignment on `x`?\n\nthanks\n\nIf we have `int[] a = {0, 1, 2, 3}; var i = 0;`, the line `a[i++] = a[i++] + 1;` will increment `i` twice. Once on the LHS of the assignment, to work out where to write to, and then again on the RHS to work out where to read from.\n\n~~that's a completely different operator and situation. How is that related?~~\n\nok i got it. In any case, that's also the case in python. a\\[f()\\] = a\\[f()\\] + 1 evaluates f twice. a\\[f()\\] += 1 evaluates it only once.", "id": "g3tm2ip", "owner_tier": 0.3, "score": 0.4516129030645161}, {"content": "* The walrus operator, `:=`, a.k.a. assignment expressions. Almost every use of them makes code harder to read and understand.\n\n* The proposed pattern matching feature ([PEP 622](https://www.python.org/dev/peps/pep-0622/)). See [this critique](https://github.com/markshannon/pep622-critique) for examples of the issues with it.\n\nI have to disagree with your opinion on the walrus operator. I have found that is extremely useful in cases like\n\n    if k in dic:\n        v = dic.get(k)\n        # ... do stuff with v\n\ncan be rewritten as\n\n    if v := dic.get(k):\n        # ... do stuff with v\n\nIMO that's best written as:\n\n    v = dic.get(k)\n    if v:\n        # ...", "id": "g3v4h8f", "owner_tier": 0.3, "score": 0.19354838693548385}, {"content": "Python may be the worst mainstream language around for accidental mutations. On top of the standard baggage reference semantic languages like Java, C#, Kotlin, etc have, Python:\n\n - has the nasty gotcha for function default arguments\n - has the same nasty gotcha for dataclass field defaults\n - it first classes in a specific set of data structures, which are all mutable, via comprehensions. If you look at how a language like say Kotlin handles the equivalent of a list comprehension, it's almost as concise but you can use any structure, so you can use an Immutable list (and even the default is a read only list)\n - it has pretty poor support for keeping things as expressions; it tends to encourage a lot of creating things followed by in place mutation, which increases the likelihood of mutation errors. Why poor support for expressions? No real lambdas, no left to right chaining, if's are not expressions, to name a few.\n\nCould you explain what you mean by \u201cno left to right chaining\u201d?\n\nSure. Let's say you have a string. You write a function to extra a token (sub-string) from that string according to some rules. You want to do that, then turn it into an integer, and then take its log.\n\n```\nx = math.log(int(token(my_string)))\n```\n\nThis reads inside out. The data flows right to left, whereas we read left to right. You read the log first, but it's the last thing that happens. If you were to write this out with a few lines, then as we reading (from top to bottom), each step in the computation is sequenced in the same order it's carried out.\n\nIf you take kotlin for example, it has extension functions. So you would instead see something like:\n\n```\nx = my_string.token().toInt().let { log(it) }\n```\n\nSome languages (like F#, and JS experimentally) have a pipe operator:\n\n```\nx = my_string |> token |> int |> log\n```\n\nThis example is pretty simple because it involves a variable. But once you get into collections, the difference is pretty stark. Considering this piece of kotlin:\n\n```\nval x = my_list\n    .map { f1(it) }\n    .filter { p1(it) }\n    .map { f2(it) }\n    .filter { p2(it) }\n```\n\nMany languages nowadays like C#, Java, Rust, even C++, will allow you to chain operations on collections like this. In python, nobody would try to write this in a single expression:\n\n```\nx = [y2 for x2 in \n        [y1 for x1 in my_list if p1((y1 := f1(x))]\n        if p2((y2 := f2(x))]\n```\n\nI'm not even sure if that works (whether that's legal use of the walrus operator). But you can see how awful it is. In python people realistically are going to do that in several steps. That results in a lot more temporary/in-between variables lying around, all of which are mutable. Look at everything in itertools in python, it's all free functions so as you chain things together you have this reverse ordering problem. In languages like Kotlin, Rust, C#, Java, collections have member functions or extension functions for all these operations.", "id": "g3w1isu", "owner_tier": 0.5, "score": 0.1129032256451613}, {"content": "* clumsy lambda syntax\n\n* `dict.values()[0]` doesn't work (mostly for REPL use)\n\n* gotta do `itertools.chain(i1,i2)`. Why not `i1+i2`?\n\n* logging module. 'nuff said\n\n* defining the same function twice doesn't throw an error. Like, when is it *not* a bug?\n\n> defining the same function twice doesn't throw an error. Like, when is it not a bug?\n\nOne specific case I can think of is when you want to overwrite an inherited class method.\n\n> clumsy lambda syntax \n\nwhat do you mean?\n\n> gotta do itertools.chain(i1,i2). Why not i1+i2?\n\nYou can do that, for iterables where that makes sense, like lists. But you don't want all iterables to behave like that (see numpy arrays). It makes sense to have a function which performs that explicit task for any iterable.\n\n+1 on logging module, actually the most obtuse and least pythonic module\n\nThat's a special case that can be easily treated differently.\n\nSame function, different language:\n\nJavascript:\n\n    (x) => x + 1\n\nJulia:\n\n    x -> x + 1\n\n\nPython:\n\n    lambda x: x + 1\n\nIt's a small difference, but it's noticeable.\n\nI meant for actual iterators, like `iter(a) + iter(b)`\n\nJulia's lambdas are so beautiful. Probably my favorite language.\n\nI've been (unfortunately) writing a ton of JS at work lately, and this is one of the only things I miss when I come back to Python.\n\nYou have inspired me to add this to my iterator convenience library [f_it](https://pypi.org/project/f-it/).", "id": "g3tj3nn", "owner_tier": 0.5, "score": 0.4999999998387097}, {"content": "GIL and intentionally gimping lambdas. Actually, any instances where a feature is intentionally gimped for ideological reasons is a flaw.\n\nHow are Python's lambdas 'gimped'? Not disagreeing, just not sure what you mean.\n\nPython\u2019s lambdas are not full anonymous functions - they can only contain a single expression.\n\nPersonally I\u2018m starting to think this is a good thing, because it makes it impossible to write complex Python code like [this JavaScript \u201crun\u201d function](https://github.com/getify/You-Dont-Know-JS/blob/1st-ed/async%20%26%20performance/ch4.md#promise-aware-generator-runner).\n\nYou will never ever prevent people from writing bad code by limiting the language.", "id": "g3toxm7", "owner_tier": 0.7, "score": 0.1451612901612903}, {"content": "Not having good ABCs/ interfaces for shared functionality, and therefore needing free functions cluttering the global namespace which may or may not work; then when you want to implement your own class with that interface, you have to write your own methods on the class anyway, just without any guidance on what the right dunder methods are, and then force users to guess that they can call the free function on it because it doesn't show up in the API.\n\nCan this be treated like a file? Who knows!\n\nPost-hoc ABCs for e.g. sequences are being added but it's a long way behind the curve.", "id": "g3tsx97", "owner_tier": 0.9, "score": 0.03225806435483871}, {"content": "The ability to use a variable which is stated at the bottom of the source code tobuse it at the top of the source code", "id": "g3vb4c6", "owner_tier": 0.1, "score": 0.016129032096774194}, {"content": "I am perennially annoyed by the decision to have the lamda argument to .sort() take one object and return a scalar or a tuple of scalars. Why this lambda cannot be written to take two objects and return a boolean indicating whether the first should be sorted before than the second (i.e. as comparators in C++ and Java work) is beyond my understanding.\n\nYou can define a cmp function and use `functools.cmp_to_key`.\n\nYou can also create a class and implement `__lt__`, `__gt__` methods.\n\nThis was supported in Python 2 (`sort` accepted a `cmp` parameter), but removed in Python 3. I\u2019m not sure exactly why.\n\nThere are a couple things wrong here:\n\n- `list.sort()` doesn't take lambda argument, it takes a `key=` argument which can be any callables, lambda is just one type of callable\n\n- the `key=` function doesn't have to return scalars or tuple of scalars, it can return any comparable objects\n\n> Why this lambda cannot be written to take two objects and return a boolean indicating whether the first should be sorted before than the second\n\nPython 2's `.sort()` used to take a comparator `cmp=`, but it was removed in Python 3. The `key=` argument implements a technique that used to be very common, called Decorate-Sort-Undecorate or Schwartzian Transform.\n\nThe comparator mechanism was removed to remove the conflict between comparisons using `__cmp__` and how it relates to other rich arithmetic comparators like `__eq__`, `__lt__`, `__le__`,  `__rt__`, `__re__`.\n\nIt is a lot easier to write a `key=` function correctly than to write a `cmp=` function, and it is a lot easier to reuse existing functions as `key=` compared to writing `cmp=` which almost always will have to be custom written. For example, to do case insensitive comparison, you can simply do `.sort(key=str.lower)`.\n\nIt's also quite easy to accidentally write a cmp method that doesn't satisfy [total ordering](https://en.m.wikipedia.org/wiki/Total_order), which is usually what you want 95% of the time you sort something. \n\nAnd since you can easily convert a `cmp` method with `cmp_to_key()`, there's really no more actual need to support `cmp=`. Personally, in the last ten years or so of coding nearly everyday, I never really felt the need for the old `cmp=`, it may be different to you, but I'd never seen a use case where sorting with comparator will make a better code.\n\nI knew about __lt__ and __gt__, kind of cool to know that functools has a cmpt_to_key function, I'll have to explore how that works.\n\nThanks!", "id": "g3wdjub", "owner_tier": 0.3, "score": 0.1290322579032258}, {"content": "I think that the lacks of proper type checking is what always makes it difficult to scale with more hands on the code base", "id": "g4mr6i9", "owner_tier": 0.1, "score": 0.016129032096774194}, {"content": "significant whitespace\n\n*ducks and hides*\n\n``from __future__ import braces``", "id": "g3tse0l", "owner_tier": 0.5, "score": -1.6129032258064515e-10}], "link": "https://www.reddit.com/r/Python/comments/iloezk/what_do_you_think_are_pythons_design_mistakes/", "question": {"content": "I'm curious to know what do you think are Python's design mistakes?\n\nI'm not looking to trash Python - I've been using it for 25 years almost on a daily basis and love it. I also know that Python started way back when the computation landscape was different, and that hindsight is 20/20. But humor me, if you can change something in Python - what will it be and why?\n\nI'll start with implicit variable declaration (lack of var/let), which leads to:\n- Bugs that are hard to catch automatically: Is that a typo or did you mean to create a new variable here?\n- Lack of proper lexical scoping and oddities such as \"global\" and \"nolocal\"\n\nI'd love to hear your thoughts.\n(What triggered this question is [this tweet](https://twitter.com/dabeaz/status/1300949834514530304) by David Beazley)", "id": "iloezk", "title": "What do you think are Python's design mistakes?", "traffic_rate": 207.942496260595}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "    random.shuffle(array)\r\n\r\nOk, ok, seriously, yeah. You use Knuth's shuffle:\r\n\r\n    for i in range(len(array)):\r\n        s = random.randint(i, len(array) - 1)\r\n        array[i], array[s] = array[s], array[i]\r\n\r\nThis is a well-studied and well documented problem, and it's not as easy as it looks to get a well-shuffled result.\r\n\r\nMoral of the story: Studied algorithms are almost always better than yours. ;)\n\n> This is a well-studied and well documented problem, and it's not as easy as it looks to get a well-shuffled result.\n\nrandom.shuffle was added to Python's standard library after we noticed that the following scenarios occurred regularly on comp.lang.python:\n\n> Q. How do I shuffle a list?\n\n> A. Here's how to do it: <broken solution>\n\nor\n\n> Q. How do I shuffle a list?\n\n> A1. Here's how to do it: <correct solution>\n\n> A2. No, that one's biased, here's the right way: <broken solution>\n\n>   array = random.shuffle(array)\n>\n> Ok, ok, seriously, yeah.\n\nNo, that's the correct answer -- you use a library function.\n\nIf a library function doesn't exist to do what you need, *then* you get out your copy of TAOCP, look up a suitable algorithm, write the code, and test it.\n\n\nTwo names to trust in the shuffle business: Fisher-Yates!  Think of Fisher-Price.  What's price about?  Lots of money.  And who has lots of money?  Bill Gates.  And what sounds like Gates?  Yates!  Fisher-Yates.\n\nOr you be boring and go for [Knuth](http://en.wikipedia.org/wiki/Knuth_shuffle).  It's the same thing, but it doesn't sound as cool.\n\nIt wasn't very clear in the article but that's the same as the \"correct\" solution given.\n\n> array = random.shuffle(array)\n\nrandom.shuffle modifies the list in place, it returns None :-)\n\nI've said it before, and I'll say it again.  [Don Knuth is my homeboy.](http://geekz.co.uk/shop/store/show/knuth-tshirt)\n\nSadly, this is exactly what is wrong with the practice of programming. If one has the mental ability to write a program that's worth something, then he/she most certainly has the ability to learn to use elementary formal methods to check their work.\n\nWhy don't we then? Because too many people cry \"boohoo, this is too hard.. I can't do it\" when they really mean \"I can do this, but I'm just lazy\"\n\nNote, I'm not blaming the lazy folks (I'm one of them). I'm blaming the faulty education practices of making too little demands.\n\nThat's pretty funny; I'd have thought there'd be someone who'd just give a wikipedia link to Knuth's shuffle and be done with it. It's really not difficult. (I did it from memory, and I don't think I'd ever actually implemented it before. The actual implementation is trivial.)\n\nWell, right. And then the interviewer goes \"pretend that random.shuffle doesn't exist\", and then you say, \"I'd use a Fisher-Yates shuffle (a Knuth shuffle)\", and write it out. (Or, if you don't know it offhand, you say \"I'd pull out my copy of TAOCP for the actual implementation.\")\n\nStill, it's always nice to understand how a library works!", "id": "c05ji5v", "owner_tier": 0.5, "score": 0.9999999999350648}, {"content": "sure, just toss the array in the air, let it fall on the floor, and pick it up the elements randomly.\n\nWell, that used to work, but ever since memory shrunk to the point where you could fit more than about 64KB on a stick, it's gotten really hard to see where the elements land since they're so small. Tossing memory elements from a modern 2GB stick is like tossing dust in the air, and the expense of getting illegal immigrants to cut it apart in the first place is pretty significant, you know; it's a lot of cutting! That's a mi$take you'll only make once.\n\nThat's why I like to keep some 16KB sticks around, it makes this algorithm a lot easier, and you'll never need an array that takes more than 16KB shuffled anyhow.\n\nThat's basically what I do. This whole shuffling in place thing is silly.", "id": "c05jm5l", "owner_tier": 0.3, "score": 0.20129870123376623}, {"content": "I generate an array of 52 random numbers alongside my deck, then I jointly sort them. (I resolve conflicts by generating another pair of numbers to determine which of the pair is higher).\n\nDecorate sort undecorate, basically.\n\nThe problem there is that sorting is (at best) O(n log(n)), whereas the Fisher-Yates shuffle is O(n).\n\nYour parlance doesn't seem universal to me. Could you rephrase that in more standard terminology?\n\nno, it's O(52). Are we talking about arbitrary sized decks of cards? I've never had to deal with them for any actual application..\n\nAside from that, the rand() function is so massively expensive compared to anything else done in those algorithms that we'd need decks in the millions of cards range before rand would stop dominating the profile.\n\nThe bigger problem is that without the conflict resolution specifics, the shuffle is biased.\n\nThese specifics alone make the algo more complex and error-prone to implement than Fisher-Yates, imo.\n\n\n> Decorate-sort-undecorate, a programming design pattern, also known as the Schwartzian transform\n\nhttp://en.wikipedia.org/wiki/Schwartzian_transform\n\n> The Schwartzian Transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items.\n\nEdit: It's also common in the Python community\n\nRTFA... hell, even just the title. It's not about shuffling a deck of cards it's about shuffling arrays in general.  Deck of cards is just the canonical example.\n\nI don't know why so many agree with you.  I'm with phil_g on this. It's O(n log(n)).\n\nIf you're just doing a card game fine; the difference is negligible in that specific case.\n\nNo. That's not how Big O Notation works:\n\nhttp://en.wikipedia.org/wiki/Big_o_notation\n\nBig O Notation is the study of the dominant terms in the time equation t(n) where n is some measure of the size of the input. Your implementation for shuffling an array is, as mentioned above, O(n*log(n)), which is the general order of sorts. \n\nIf you are attempting to say that your sort would complete in under 52 units of time in the given case, you are also wrong because, as mentioned the worst case time for sorting an array is n*log(n), which means your algorithm will finish in under 89 units of time, compared to 52 units of time for the O(n) Fisher-Yates algorithm.\n\nAh, thank-you. It confused me because I couldn't see the relationship to python's decorators :-)\n\nRTFC. My comment was specifically about a deck of cards. (And the array size has to get into the billions before the sort time gets close to the rand() time)", "id": "c05jmo1", "owner_tier": 0.5, "score": 0.17532467525974024}, {"content": "This was one of the first homework assignments in my Intro CS class, and yes, the professor did go into why the obvious solution doesn't work.", "id": "c05jw9v", "owner_tier": 0.7, "score": 0.07792207785714285}, {"content": "Huh. I've been making an array of random ints and sorting the array with those random ints as keys. Thorough, but much less efficient. I didn't realize there was a simple and much more efficient solution.\n\n[deleted]\n\nMy old method was correct. It gives a fairly even spread of orders.\n\n[deleted]\n\nYou're so full of shit. I just tested it. The distribution is even, within a fraction of a percent. It's just inefficient, which is bad but not as bad as being incorrect.\n\nC# test code I used:\n\n    using System;\n    using System.Collections.Generic;\n    using System.Text;\n\n    namespace shuffletest\n    {\n        class Program\n        {\n            static Random r = new Random();\n            const int MAX = 1000000;\n            private static bool ArrayEqual(int[] a, int[] b)\n            {\n                if (a.Length != b.Length)\n                {\n                    return false;\n                }\n                else\n                {\n                    for (int i = 0; i < a.Length; i++)\n                    {\n                        if (a[i] != b[i])\n                        {\n                            return false;\n                        }\n                    }\n                }\n                return true;\n            }\n            private static void Shuffle(int[] a)\n            {\n                int[] b = new int[a.Length];\n                for (int i = 0; i < b.Length; i++)\n                {\n                    b[i] = r.Next();\n                }\n                Array.Sort(b, a);\n            }\n            static void Main(string[] args)\n            {\n                int[][] basic = new int[][] { new int[] { 0, 1, 2 },\n                                              new int[] { 0, 2, 1 },\n                                              new int[] { 1, 0, 2 },\n                                              new int[] { 1, 2, 0 },\n                                              new int[] { 2, 0, 1 },\n                                              new int[] { 2, 1, 0 }};\n                int[] counts = new int[] { 0, 0, 0, 0, 0, 0 };\n                int[] shuffle;\n                for (int i = 0; i < MAX; i++)\n                {\n                    shuffle = basic[0].Clone() as int[];\n                    Shuffle(shuffle);\n                    for (int k = 0; k < counts.Length; k++)\n                    {\n                        if (ArrayEqual(shuffle, basic[k]))\n                        {\n                            counts[k]++;\n                        }\n                    }\n                }\n                foreach (int c in counts)\n                {\n                    Console.WriteLine(c);\n                }\n                Console.ReadLine();\n            }\n        }\n    }\n\n\n[deleted]\n\nOK OK I just read Oleg's paper.\n\nYeah my system is wrong and only works in practice for arrays that have less than 4 billion members.\n\nDouchebag.\n\nYour algorithm is computationally inefficient; it will *never* produce unbiased results; and it completely breaks down with only **13** items.\n\nYou betray a misunderstanding of the intricacies of these algorithms.  That's okay.  Stubbornly defending your code when presented with your ignorance, however, is not.\n\nTake a step back and actually learn how Fisher-Yates works.  Understand its nuances.  You'll be better off for it.\n\nMy ignorance? My misunderstanding? What a douchebag.\n\nI just did some calculations, and it turns out that you are also full of shit. In fact, I have no idea how you come to these conclusions.\n\nAt 13 items, the probability of having no duplicate keys is over 99.99999%. This is not a noticeable problem with an algorithm that is supposed to give a random ordering. If you want to know the actual way to calculate that, look up the birthday paradox. At 6000 items, the odds of having a duplicate key are still under 0.5%. Such a bias, if it occurs, will be **unpredictable** in where occurs.\n\nWhat's cute is that I think the Knuth shuffle is neat and I intend to use it from now on. I freely admit that it is better. There is no doubt that it is a lot more efficient. I said so from the start. But you are seriously a complete dumbass if you think bias with this other algorithm would be a problem.\n\n[deleted]", "id": "c05jpls", "owner_tier": 0.5, "score": 0.05844155837662337}, {"content": "Why bother trying to shuffle an array in place?\r\n\r\nJust copy the array into a mutable list, then refill the original array with items randomly selected from said list.\r\n\r\nIt trivial to show this is random (well, as random as your random number generator), hard to screw up, and easy to generalize for any collection.\n\nhow do you plan to \"randomly select\" items from the copied list?\n\nJust like grabbing marbles from a sack.\r\n\r\n    while oldList.length > 0\r\n        index = Random(0, oldList.length-1)\r\n        newList.Add oldList(index)\r\n        oldList.RemoveAt(index)\n\nThat's exactly what the in-place shuffle does. The newList is the elements from 0 to k-1, and the oldList is the elements from k onward.\n\nOnce you see that it's completely conceptually identical, you sort it in place because it's easier, and it's faster.\n\nThe time complexity of this would be O(n**2), no? If oldList is an array, oldList.RemoveAt(index) would take O(n) time. If it is a linked list oldList(index) would take O(n) time.\n\nCompare to in place algorithm which is O(n) time.\n\nLets see. N to copy the array + n to select each element + n * (n..1) to find and remove elements from a single-linked list. So I think that ends up being roughly O(n Log n), with n^2 as the worse case. Of course you can speed this up by using a doubly-linked list and seeking both directions rather than always starting at 0.\r\n\r\nBut so what?\r\n\r\nWe are talking about randomizing values. Generally speaking you are not going to have a lot of values in your list, nor is speed going to be very important under most circumstances.\r\n\r\nMost programs that randomize values actually have added delays in order to give the user the impression that they are actually being shuffled.\n\nFYI, O(n!) is not \"roughly\" O(n log n), it's O(n^2).\n\nIt would only be n! if you used a singly-linked list and your random number generator always returned the highest possible value.\r\n\r\nAnd again, so what? It isn't like you are going to randomize 10,000 items.\n\nConstant factors don't affect big-O complexity. So for example using a doubly-linked list halves the amount of work, but it's still O(n!). And though the random number generator doesn't always pick the last element, if the RNG is fair the total of the numbers chosen will converge on (n/2)!, which is still O(n!).\n\nI'm not saying it's a bad algorithm -- it has the nice benefit of being purely functional -- but it's still O(n^2). Someone already linked this, but in case you missed it, http://okmij.org/ftp/Haskell/perfect-shuffle.txt shows how to make this algorithm O(n log n) using a binary tree instead of a list.", "id": "c05jv0u", "owner_tier": 0.9, "score": 0.18831168824675323}, {"content": "I am probably way out of my depth here, but I never shuffle the array...\n\no.o I try to generate a random index for the (already in order) array, then pick a new random index out of the remainder.", "id": "c05jx16", "owner_tier": 0.7, "score": 0.07142857136363635}, {"content": "I'm lazy, I tend to to do a random schwartzian transform and sort it.  Shuffle in O(n log n)\n\nThe advantage to this algorithm is that it is functional. The disadvantage is that it is actually slightly biased.\n\nSee oleg for an explanation and a proper perfect functional shuffle (not that I don't tend to use the above-described shuffle for \"good enough\" applications anyway,\"):\n\nhttp://okmij.org/ftp/Haskell/perfect-shuffle.txt\n\nYou call yourself \"Lazy\" for using an O(n lg n) algorithm when an O(n) algorithm would do?  For hand-writing code that exists in a library already?  When did either of these actions become \"lazy\"?\n\nLazy means he does the least work for himself, not the computer.  That means he doesn't care about complexity and won't look something up if he can code it out quickly enough.\n\nDepending on the problem, O(n) usually takes *more* work for the programmer than O(n log n)\n\nJust because it exists in a library doesn't mean it exists in a library I have readily available to me.\n\nThe last time I needed a shuffle, I didn't have a library to do it.  It took thirteen characters to do what I described above.\n\nThat's true, but this isn't one of those problems.\r\n\r\nIt takes [three lines of Python](http://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/c05ji5v).\n\nYou missed the point. It takes three lines of Python because you know Knuth's shuffle and the results behind it.\n\nThe actual *design and analysis* of an algorithm (not just typing it) takes more work..\n\nSure it is.  Ruby implementation (where I most recently did what I described above):\n\n   a.sort_by { rand }\n\n(except I added that to the Array class as \"shuffle\" -- if it somehow performs poorly ever, I'll replace my implementation with a new one)\n\nNote that this is really easy to understand as well.  If you trust your random number generator to be random enough (which the Knuth one also requires) and you trust your sort, then it's pretty obvious what will happen there.\n\nHmm.\r\n\r\nI see your point, but I never had to design or analize this problem. I've got other things to do; that's why I trust Knuth to do this kind of thing for me.\r\n\r\nAnd it's not like this is a gorram \"best path through a weighted cyclic graph including negative weights with other special constraints\" problem. It's a shuffle. You shouldn't be designing your own anything that's this common -- you should be just using your languages implementation, and if your language doesn't have one, then [checking wikipedia](http://en.wikipedia.org/wiki/Shuffling#Implementation_of_shuffling_algorithms).\n\nI must say then when I first designed a shuffle algorithm, the Knuth method came to me immediately as the natural solution.\n", "id": "c05ji8g", "owner_tier": 0.5, "score": 0.27922077915584415}, {"content": "Just checked my [poker server](http://github.com/philluminati/texas_holdem/tree/master/card.c) and I don't think I fell for it. This is my implementation and it is the correct one yeah?\n\n    for (i = 0; i < SHUFFLE_INTENSITY; i++)\n    {\n       a = (rand() % 52);\n       b = (rand() % 52);\n \n       debug (5, \"i=%i,a=%i,b=%i\\n\",i,a,b);\n \n       tmp = deckArray[a];\n       deckArray[a] = deckArray[b];\n       deckArray[b] = tmp;\n    }\n\n\n I don't loop through A and swap with B, I choose two cards evenly and swap them. This is correct isn't it?\n\nNo, that is another *slightly* wrong one.  Take a look at \"Knuth's Shuffle\", linked to in the other comments.\n\nIt is more inefficient than wrong, though.  With Knuth's Shuffle, you swap *n*-1 times where *n* is the size of the list.  With your algorithm, you need to set SHUFFLE_INTENSITY significantly higher than *n* to get a decent shuffle.\n\nTo illustrate, these are the occurences of each permutation after 100000 shuffles on a list of 3 elements:\n\n    PhilShuffle with SHUFFLE_INTESTITY=10 (10 swap operations):\n    [0, 1, 2] : 16524\n    [2, 0, 1] : 16608\n    [1, 2, 0] : 16763\n    [0, 2, 1] : 16657\n    [2, 1, 0] : 16598\n    [1, 0, 2] : 16850\n    PhilShuffle with SHUFFLE_INTESTITY=3 (3 swap operations):\n    [0, 1, 2] : 18618\n    [2, 0, 1] : 14669\n    [1, 2, 0] : 14969\n    [0, 2, 1] : 17231\n    [2, 1, 0] : 17374\n    [1, 0, 2] : 17139\n    Knuth's Shuffle (2 swap operations):\n    [0, 1, 2] : 16604\n    [2, 0, 1] : 16571\n    [1, 2, 0] : 16570\n    [0, 2, 1] : 16782\n    [2, 1, 0] : 16891\n    [1, 0, 2] : 16582\n\n\nIn addition to what Deestan mentioned about the algorithm, there are two other quirks as well.\n\nFirst, using rand() is going to limit the number of distinct permutations to the period of the random number generator.  You should use an algorithm with a period that is significantly higher than 52!.\n\nSecond, most PRNG's return a value in a domain of [0..2^n).  Blindly taking that value modulo 52 can introduce bias.  Consider the simple scenario of [0..63]:\n\n[0..51] : 1/64 for each card\n\n[52..63] : 1/64 for the first 12 cards\n\nI don't think it's correct. If your SHUFFLE_INTENSITY isn't high enough, there's a high probability that many cards just won't move.\n\nI think you'll get a better result if you set SHUFFLE_INTENSITY to 52, and set a to i, so you'll swap every card in the deck with some card (possibly itself) at least once.\n\nThanks, I think I understand. My technique is unbiased but capable of picking up the same card, or swapping the same two cards multiple times, thus requiring a shuffle intensity well above 52. (In the code I have it set to 3000. *well, it's C ;-)*) Thus it is simply inefficient since Knuth can do the same job in 52 operations exactly.\n\nSounds like someone didn't read the article.\n\nNope, still biased.\n\nA card is more likely to end up in the position in started in than any other position.\n\nYou can do the same combinatorical proof that it's biased.\n\nFirst of all your shuffle is biased - it does not return each possible shuffled deck with the same probability.\n\nSecondly, the use of a poor shuffling algorithm [has already been used to attack a poker server](http://www.cigital.com/papers/download/developer_gambling.php). Ironically, in that case the developers published their shuffling algorithm to demonstrate that it was fair. The attack consisted of two parts: first that the shuffle was not fair, and secondly that they used an off-the-shelf RNG.\n\nI have realised it is bias, when I looked back at Deestan's post.\n\n> PhilShuffle with SHUFFLE_INTESTITY=3 (3 swap operations):\n\n> [0, 1, 2] : 18618 <- Too many.\n\n> [2, 0, 1] : 14669\n\n> [1, 2, 0] : 14969\n\nI've made the change now to Knuth (Fisher-Yates) algorithm. That is an interesting link, thank you. It really isn't a trivial matter.\n\nI'm curious how you didn't end up with any [0, 2, 1]s and friends.\n\nThat seems odd.\n\nLol, that is a snippet from [Deestan's comment](http://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/c05jldm).\n\n", "id": "c05jlbq", "owner_tier": 0.7, "score": 0.28571428564935064}, {"content": "My naive fix would have been to create an empty tree-map of index to item, iterate the array, assign a random index until I found one not yet defined in the map, insert the item into the map, and then iterate the map ordered by index to re-fill the array.\n\nNowhere near as efficient, but it would have been correct.\n\nEdit: a cute improvement would be to use much wider indexes in the map. Then collisions would be unlikely. After all, we only care about the *order*. The iterator implies the real index.", "id": "c05jnas", "owner_tier": 0.5, "score": 0.05844155837662337}, {"content": "An important note, since he mentioned Poker Servers:  For a serious poker server, you should *not* use computer random number generators.\n\n(Edit: substitute \"computer random number generators\" for \"standard library pseudorandom number generators indiscriminately\" in the previous sentence.  Thanks, theeth.)\n\nThis is because the number of possible permutation results are capped by the number of possible random seeds.  I.e. a standard 32-bit integer has 4294967296 possible values, while a deck of 52 cards has 52! = 80658175170943878571660636856403766975289505440883277824000000000000 possible permutations.  \n\nThis means that *no matter how clever your shuffling algorithm is*, there are umzillions of permutations it will never generate.\n\nAs an example, I tried making a Python script to shuffle a list of 52 cards via the standard Python shuffle(), draw 4 cards, and try to guess the 5th.  With a perfect shuffle, it should have a success rate of 1/48.  My script had a success rate somewhere around 1/20.\n\n> For a serious poker server, you should not use computer random number generators.\n\nThat's ludicrous.\n\nThis problem with seeding only appears because you're limiting yourself (or rather, the library is) to setting an infinitely small portion of the initial state of the algorithm.\n\nFor example, to fully seed Mersenne Twister (the algo used in Python), you'd have to provide 624 different 32bit ints, letting you position yourself anywhere along the 2^19937 \u2212 1 (more than 10^6001) period.\n\nIf you really want to push further, you could go for George Marsaglia relatively simple multiple with carry algorithm (period of 10^39460, seed of 131072 bits).\n\nNo matter how clever you think you are, if you don't understand how the algorithms you're using works, you're bound to make silly assumptions.\n\nI wonder what's the current state of software random number generation. While studying the Montecarlo method, I remember reading about specialized hardware cards with radioactive isotopes that guarantees a true random number always.\n\n[deleted]\n\n> That's ludicrous.\n\nAch, indeed it is.  Thanks for the explanation.  I had the [GNU C RNG](http://www.delorie.com/gnu/docs/glibc/libc_396.html) in mind when I tested, Python was just used for convenience.\n\n[Random.org](http://www.random.org/integers/) creates numbers from atmospheric noise (you can subscribe to them and get a sort of 'random number service'), and I've also heard of creating numbers from the background radiation of the universe. \n\nInteresting stuff, in my opinion. \n\nIt was just a throwaway script I lost ages ago, but here's a *pseudocode* version showing the possible candidates for 5th card:\n\n    import random\n    given_hand = [47, 34, 27, 3] # seed(1) - next is 7\n    candidates = {}\n    init_list = range(52)\n    for i in xrange(2**32):\n        random.seed(i)\n        test_list = init_list[:]\n        random.shuffle(test_list)\n        if test_list[:4] == given_hand:\n            candidates[test_list[5]] = 1\n    print candidates.keys()\n\nNote:\n\n * It is horribly inefficient and runs stupid slow.\n * The Python random module seed is not really limited to 32 bits.  This was only written to give actual data of how the probability would be biased with a 32 bit seed.\n\nWebcam + Lava Lamp.\n\n[deleted]\n\nYes, it's not really feasible to attack anything brute-force like this.\n\nThough with some clever coding and a more heuristic approach, you could slightly improve your poker odds in real-time even with a normal desktop computer.", "id": "c05jlyd", "owner_tier": 0.7, "score": 0.3506493505844156}, {"content": "For a class practical, I used the first method outlined in the article. The 2nd way never occurred to me. Still shuffled the array rather well.\n\nJust to illustrate the bias (I *had* to test this with code), both shuffle techniques performed 100000 times on the initial list [1, 2, 3].\n\n    <function stupidShuffle at 0x0232E570> occurences:\n    [0, 1, 2] : 22302\n    [2, 0, 1] : 11198\n    [1, 2, 0] : 21875\n    [0, 2, 1] : 10988\n    [2, 1, 0] : 11282\n    [1, 0, 2] : 22355\n    ----------------------------------------\n    <function cleverShuffle at 0x0232E530> occurences:\n    [0, 1, 2] : 16665\n    [2, 0, 1] : 16754\n    [1, 2, 0] : 16728\n    [0, 2, 1] : 16542\n    [2, 1, 0] : 16707\n    [1, 0, 2] : 16604\n\n\nNo it didn't. I'm with you, I also used the wrong solution before I knew better, but the harsh truth is simply that it doesn't shuffle correctly. (Note that the correct solution is not the slightest bit more complicated, so a \"good enough\" argument doesn't hold up either.)\n\nThe simple way of seeing that it will be biased is to think of what happens when you shuffle an array of size 3.\n\nYou make 3 calls to the random function, each returning one of 3 possible values, so you have 9 different outcomes, each one is equally valid.\n\nHowever, when there are only 3 items, there are 3 * 2 * 1 = 6 different ways of shuffling them.\n\nThis means that of the 9 equally valid outcomes from that shuffling algorithm, some must be more likely than others.\n\nIt is funny how we are often more convinced with a sample of 100000, rather than just doing an exact brute force \"every possible shuffle\" method (where in this case, the number of different shuffles is quite small).\n\nSame thing with the Monty Hall problem - to many people it is more convincing to see the results of lots of runs, even though the number of different combinations is tiny.\n\nHmmm, very interesting. I'll have to read this article a bit more closely. I only had a cursory glance at the \"clever shuffle\" algorithm when I initially followed the link. Thanks for the trouble you went to. It's appreciated.\n\n> You make 3 calls to the random function, each returning one of 3 possible values, so you have 9 different outcomes\n\nThat would be 18 different outcomes.\n\nGah, actually, it would be 3 * 3 * 3 = 27 possible outcomes. 6 doesn't divide 27 though, so the argument still stands. (if it was 18 different outcomes, then the shuffle could be unbiased)\n\nOh, of course. I'll second that \"gah\"...", "id": "c05jjci", "owner_tier": 0.9, "score": 0.3246753246103896}, {"content": "I bet I could shuffle 100 arrays.", "id": "c05jkn0", "owner_tier": 0.5, "score": -6.493506454042397e-11}], "link": "https://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/", "question": {"content": "", "id": "72yhu", "title": "Do you know how to shuffle an array?", "traffic_rate": 935.2064958283671}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "There is also a video of the talk, recorded at ICFP 2009 in Edinburgh: http://www.vimeo.com/6624203\n\nThanks; this gets a little hard to follow in the middle with just the slides!", "id": "c0kbseo", "owner_tier": 0.1, "score": 0.23728813550847458}, {"content": "I want to downvote on principle for using \"considered harmful\", but the content is actually pretty good.\n\nGuy Steele is a master, and the masters know when they can get away with breaking the rules.\n\n''considered harmful' considered harmful' considered harmful\n\n_Slightly_ harmful! That's a different thing!\n\nGuy Steele can slam a revolving door.\n\nGeneralising, \n\n    let rec considered_harmful n =\n      if n = 1 then\n        \"considered harmful\"\n      else\n        Printf.sprintf \"'%s' considered harmful\" (considered_harmful (n - 1))\n\nI considered an infinite lazy version, but unfortunately it is left recursive.\n\nMostly harmless?\n\nMy take on it is that foldr/foldl are inheritently bad, but rather the problem is in linked lists. \n\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''...\n\n;-)\n\n    let considered_harmful n =\n      List.fold_left\n        (fun s _ -> sprintf \"'%s' considered harmful\" s)\n        \"considered harmful\"\n        (List.range n)\n\nYou meant this one.  It's tail-recursive *and* ironic. :)\n\nAnd for the nit-pickers yes List.range isn't part of the standard library so suppose this was at the top:\n\n    module List = struct\n      include List\n      let range n =                                     \n        let rec loop accum = function\n          | 0 -> List.rev accum\n          | i -> (loop ((n-i)::accum) (i-1))        \n        in\n        loop [] n\n    end\n\n\nNo, the problem is that foldr and foldl inherently promote a sequential order, which is bad (for parallelisation). Whether you do it on a linked list, an array or a tree the problem is the same: There's a linear data dependency. reduce avoids this problem by assuming associativity and thus giving you much more freedom.\n\n(Of course, linked lists are also generally a problem for similar reasons)", "id": "c0kbinl", "owner_tier": 0.5, "score": 0.9067796609322034}, {"content": "Thanks for using Google Docs!\n\nWhy is Google Docs considered good?  I was actually thinking the opposite.  Google Docs gave up downloading the images for me about halfway through\u2026\n\nBetter than fucking scribd\n\nThere's a PDF [download link](http://research.sun.com/projects/plrg/Publications/ICFPAugust2009Steele.pdf) right at the top, if you like.\n\nI like the fact that you don't need any additional software installed to read a PDF, and those who are using Adobe Acrobat are protected from file using an unpatched security exploit.\n\nGoogle docs considered.... harmful? \u0ca0_\u0ca0\n\nand it's blocked by our proxy as online file storage.\n\n> Why is Google Docs considered good? \n\nI second this question. A simple S5 presentation would have worked perfectly. A PDF would have been ok with a [pdf] tag. This thing good isn't.\n\nThere are extensions for pretty much every browser that needs it allowing users to view PDF files in Google Docs without visiting the original document.  There are no such things for allowing users to view shitty Google Docs-converted PDFs in their original form without having to go to Google.\n\nTherefore, linking to shitty, low-quality, poorly-anti-aliased Google Docs conversions should **never** become common practice.  It only makes those of us who have our shit together (using a decent PDF viewer, or OS X, which has these features built in) have to suffer for the idiots who can\u2019t click an \u201cInstall Extension\u201d button.\n\nI'm pretty sure this was sarcastic.\n\nThat's like saying being shanked is better than being burned alive. It might be true, but...", "id": "c0kbg4b", "owner_tier": 0.9, "score": 0.9999999999152543}, {"content": "I particularly like slide 33 - I've never seen Mergesort and Quicksort compared quite that way.\n\nThere are more analogies like that. \n\nFor example you can generate permutations by selection, where you select arbitrary element from list and cons it to permuted rest or take head and insert it into permuted tail. In prolog insert and select is the same predicate, just with different modes:\n\n    % For insert you call as (+,+,-), for select call as (-,-,+). \n    select(X,T,[X|T]).\n    select(X, [H|T], [H|S]) :- select(X, T, S).\n\nAnd now:\n    % (+,-)\n    permute_select([],[]).\n    permute_select(List, [Head|PermutedTail]) :-\n        select(Head, Tail, List).\n        permute_select(Tail, PermutedTail).\n\n    % (-,+)\n    permute_insert([],[]).\n    permute_insert(Permuted, [Head|Tail]) :- \n        permute_insert(PermutedTail, Tail);\n        select(Head, PermutedTail, Permuted).\n\nIt's pretty easy to take it further and prove that insert sort and selection sort are actually the same.\n\nYou can find similar analogy with graph traversal. You can write DFS and BFS as the same algoritm with the only difference in the accumulator data structure. For DFS it's FIFO and for BFS it's LIFO queue.\n\nI like 72\n\n> The programming idioms that have become second nature to us as everyday tools DON\u2019T WORK.\n\n> You can find similar analogy with graph traversal. You can write DFS \n> and BFS as the same algoritm with the only difference in the \n> accumulator data structure. For DFS it's FIFO and for BFS it's LIFO \n> queue.\n\nHere's an implementation of that idea for binary trees (not general graphs) in Java, with a module hierarchy diagram and code set side-by-side (it's a little wide: sorry about that):\n\n[BFS and DFS traversal code](http://www.willamette.edu/~fruehr/241/samples/traversal/traversalFrames.html)\n\nThe implementation is a bit baroque; this was done for a beginning data structures class which was taught in Java. (Also note that the diagram may not be anything like standard UML or other such notations, except by perhaps accident.)", "id": "c0kbqsu", "owner_tier": 0.5, "score": 0.1186440677118644}, {"content": "This is true. I once used `foldr` and almost lost a thumb. \n\nNow how many thumbs do you have?\n\nHe lost a lot of blood, but luckily, he found most of it.\n\nalmost less than 2\n\nMore than the average person.\n\n(2>1.999)\n\nOne less than he did.\n\nI know the average person, and he has two thumbs like most everyone else\n\nAlmost.\n\nThe average person has around 1.99 thumbs, 0.98 testicle and 0.49 penis.\n\n[deleted]\n\nWrong, that's the median or the modal person.  The average person does not exist, because they would need to have 1.99 thumbs.", "id": "c0kbifx", "owner_tier": 0.9, "score": 0.4745762711016949}, {"content": "Brilliant, as expected from Guy Steele.", "id": "c0kbka9", "owner_tier": 0.9, "score": 0.04237288127118644}, {"content": "I read this topic title, and had no idea what it meant.\r\n\r\nI clicked this link, to figure out the topic title, but had no idea what I was looking at.\r\n\r\nI came to these comments, to figure out what I had been looking at, but still have no idea.\r\n\r\n*sob*\n\nI can't explain the whole presentation, but `foldl` and `foldr` are functions that take a list and produce a single value out of it, by combining all the elements together in a particular way.\n\nThey do this by repeatedly applying a function or operator that can combine only two elements together.  By repeatedly applying the two-argument function, they can handle all the items in the list.\n\nThey usually define a value that would correspond to an empty list.  (For example, the sum of the empty list is zero.  Or the product of the empty list is 1.)\n\nFor example, say you want to find the sum of these integers:\n\n    1 2 3 4 5\n\nFolding would involve choosing an addition function (I'll call it `add(a,b)`) to be the function used to combine pairs together, starting at one end and moving toward the other.  The steps might resemble this:\n\n    1 2 3 4 5\n    -> add(1, 2) 3 4 5\n    -> 3 3 4 5\n    -> add(3, 3) 4 5\n    -> 6 4 5\n    -> add(6, 4) 5\n    -> 10 5\n    -> add(10, 5)\n    -> 15\n\nOr, if you want to work from the other end, like this:\n\n    1 2 3 4 5\n    -> 1 2 3 add(4, 5)\n    -> 1 2 3 9\n    -> 1 2 add(3, 9)\n    -> 1 2 12\n    -> 1 add(2, 12)\n    -> 1 14\n    -> add(1, 14)\n    -> 15\n\nThe `foldl` (\"fold left\") and `foldr` (\"fold right\") functions work from different ends of the list.  They are usually defined recursively, and the examples above don't exactly mirror which operations go on in what order and which recursive calls are made.  They're just meant to demonstrate that you're working from one end of the list only, which makes your processing very serial.\n\nFor what it's worth, `foldl` and `foldr` come in very handy.  They aren't just used for summing lists of integers.  If you have a whole bunch of sets and you want the intersection of all of them (say each set represents the results of a single search criterion, and you want the result of searching for all the criteria together), and if you had a function which takes the intersection of two sets, you could use a fold function to make it trivial work to generalize it to into taking the intersection of N sets.\n\nAnyway, the relevance of all this is that folding is extremely convenient (it can turn any two-argument, associative operator into an N-argument operator), but it doesn't parallelize well at all, because it just attacking the list from one end, doing one combination at once.  So even though it was a very successful programming idea, something more is needed for parallel programming.\n\nI thought I was a fairly good programmer, and I have no clue what this means. I share your pain", "id": "c0kc9vq", "owner_tier": 0.5, "score": 0.18644067788135593}, {"content": "> Just derive a weak right inverse function and then apply the\nThird Homomorphism Theorem. See\u2014it\u2019s easy!\n\n...\n\nA link to the paper mentioned on this slide is [here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.5695) if anyone is interested. Another paper proving the Third Homomorphism Theorem is [here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.2247). It's not really very difficult--Neighborhood of Infinity posts are usually more complex.", "id": "c0kbkj1", "owner_tier": 0.7, "score": 0.07627118635593219}, {"content": "Nice presentation.\n\nWhat he calls conjugate transforms is also known as tupling in the program calculation literature.\n\nA useful example everyone likely knows is the bombastically named Schwartzian transform. You want to sort a list by some feature `f x` of each datum `x` without constantly recomputing `f x`. A way to do it is `map fst . sortBy snd . map (\\\\x -> (x, f x))`. If `f :: A -> B` then in Steele's terminology `T = [A]` is the original data type and `U = [(A, B)]` is the enriched data type used in the intermediate steps.\n\nAnother example is computing Fibonacci numbers in linear time. Then `T = [Integer]` and `U = [(Integer, Integer)]`. Using `U` we may compute the sequence by iterating `\\\\(x, y) -> (y, x+y)` starting with `(0, 1)` and finally projecting away the second component of each list element, same as we did in the first example.\n\nI thought I'd also draw attention to this foot note:\n\n> It would be nice to have a simple facility to cache any monoid in a tree. It\u2019s straightforward to cache more than one monoid, because the\ncross-product of two monoids is a monoid. Deforestation of monoid-cached trees may turn out to be an important optimization.\n\n", "id": "c0kd3d9", "owner_tier": 0.5, "score": 0.033898305000000004}, {"content": "For Perl programmers, I'd just like to point out that the Conjugate Transforms idea on slide 41 (if you look at the numbers on the slides themselves; otherwise, slide 42) is essentially the same idea as the [Schwartzian Transform](http://en.wikipedia.org/wiki/Schwartzian_transform).", "id": "c0kclmy", "owner_tier": 0.9, "score": 0.01694915245762712}, {"content": "Wow, nice article. Seeing when it was presented I am also happy that I could read it freely and not on a publisher page like science direct :D", "id": "c0kbotc", "owner_tier": 0.1, "score": 0.008474576186440678}, {"content": "\"If you can construct two sequential versions of a function that\nis a homomorphism on lists, one that operates left-to-right and\none right-to-left, then there is a technique for constructing a\nparallel version automatically.\n\n...\n\nJust derive a weak right inverse function and then apply the\nThird Homomorphism Theorem. See\u2014it\u2019s easy!\"\n\nI'll get right on that...", "id": "c0kc1id", "owner_tier": 0.5, "score": 0.008474576186440678}, {"content": "What did I ever do to be considered harmful?\n\nImpostor!\n\nshhhhhh, this is probably the best chance I'll get at having one of those cleverly relevant names that everyone else seems to get.", "id": "c0kco9s", "owner_tier": 0.5, "score": 0.02542372872881356}, {"content": "\"going forward\" considered harmful. ", "id": "c0kbvfx", "owner_tier": 0.5, "score": -8.47457621968245e-11}, {"content": "Isn't this what nested data parallelism is mostly about?\n\nUsing a fold over an associative operation should actually use mconcat - associativity can then be exploited in interesting parallel ways. Though it might be nice to generalize mconcat to non-lists so that it can work on arrays/et-al.", "id": "c0kc61h", "owner_tier": 0.7, "score": -8.47457621968245e-11}, {"content": "What is the difference between 'delay' and 'work'? \n\nI know that total work = number of steps = time complexity, but I haven't seen 'delay' before. What does it measure?\n\nIn a parallel computation, I'm guessing it is the minimum duration of the computation, given unlimited parallel resources. I.e., how many time units you have to wait for the answer.\n\nTotal work could be a measure of duration, except for the fact that many \"workers\" can complete the same amount of work in less time.\n\nSo, let's say that you have two methods for making a chair.  In one of them one guy spends two units of time making the seat, and then he passes it to another guy, who spends two units of time making the back to fit the seat.  In the other method, the first guy still spends two units of time making the seat, but the other guy is making the back simultaneously; it takes him three units of time to do that, because he doesn't have the seat to match against.\r\n\r\nIn the first method, the total work is four units (first guy for two, second for two), and the delay is also four because it's four units of time from start to finish.\r\n\r\nIn the second method, the total work is five units (first guy for two, second for three), but the delay is only three units because they're working at the same time for most of that.\r\n", "id": "c0kcjuq", "owner_tier": 0.3, "score": 0.05084745754237288}], "link": "https://www.reddit.com/r/programming/comments/b0eck/or_foldl_and_foldr_considered_slightly_harmful/", "question": {"content": "", "id": "b0eck", "title": "or, foldl and foldr considered slightly harmful", "traffic_rate": 935.2064958283671}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "Python, #19/10. Glad to see they're getting tougher. Video of me solving at https://www.youtube.com/watch?v=YQjPXSlSelc. Not reading everything up front probably cost me time here. This code solves part 2 (part 1 requires a minor modification).\n\n     from collections import defaultdict\n     lines = open('4.in').read().split('\\n')\n     lines.sort()\n\n     def parseTime(line):\n         words = line.split()\n         date, time = words[0][1:], words[1][:-1]\n         return int(time.split(':')[1])\n\n     C = defaultdict(int)\n     CM = defaultdict(int)\n     guard = None\n     asleep = None\n     for line in lines:\n         if line:\n             time = parseTime(line)\n             if 'begins shift' in line:\n                 guard = int(line.split()[3][1:])\n                 asleep = None\n             elif 'falls asleep' in line:\n                 asleep = time\n             elif 'wakes up' in line:\n                 for t in range(asleep, time):\n                     CM[(guard, t)] += 1\n                     C[guard] += 1\n\n     def argmax(d):\n         best = None\n         for k,v in d.items():\n             if best is None or v > d[best]:\n                 best = k\n         return best\n\n     best_guard, best_min = argmax(CM)\n     print best_guard, best_min\n\n     print best_guard * best_min\n\nNot reading everything is what cost me this time, too. After 30 minutes I gave up and went to bed. Woke up this morning and completed it to find that I spent the previous night solving for part #2 without even knowing what it was.\n\nThank god I never delete the unused code.\n\n>argmax\n\nargmax also works with this \"implementation\":   \ud83d\ude42\n\n    def argmax(d):\n        k = max(d, key=d.get)\n        return key, d[k]\n\nWhat do you think about this suggestion:\n\n    best_guard = max(d, key=d.get)\n    best_min = d[best_guard]\n\n\ud83d\udc4d\n\nI love the videos! Keep it up!\n\nI love how you probably have countless fancy programs and text editors installed but you still use Vim like a real pro! Cool video, and congrats for the result :)\n\nPS: lol @ those 35k unread emails\n\nObviously you're a pro and you know better but I have a feeling that if you used re or other parsing library you would get much quicker. Also have \"from collections import defaultdict\" in a boilerplate file.\n\n> lines = open('4.in').read().split('\\n')\n\nIs that different than:\n\n    lines = open('4.in').readlines()\n\n\n[deleted]\n\nYour code appears to be wrong, at least the very end : you have to first find which guard sleeps the most, in total. That would use your C defaultdict, yet you never use it. Only after finding said guard, you have to find HIS personal favorite minute. How did this code work for you ? \n\nSame here!\n\nThough, it was really easy to change code back to solve part two then xD\n\nNice! Much cleaner", "id": "eb1wb5a", "owner_tier": 0.3, "score": 0.5679012344444444}, {"content": "**AWK**\n\n4.1\n\n    sort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}/falls/{s=$3}/wakes/{for(t=s;t<$3;++t){++zg[g];++zgt[g\",\"t]}}END{for(g in zg)if(zg[g]>zg[og])og=g;for(t=0;t<60;++t)if(zgt[og\",\"t]>zgt[og\",\"ot])ot=t;print og*ot}'\n\n4.2\n\n    sort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}/falls/{s=$3}/wakes/{for(t=s;t<$3;++t)++zgt[g\",\"t]}END{for(gt in zgt)if(zgt[gt]>zgt[ogt])ogt=gt;split(ogt,oa,\",\");print oa[1]*oa[2]}'\n\n\nHorray for awk :)\n\nI tried to indent it a bit nicer so its easier to read:\n\n```awk\nsort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}\n                                    /falls/{s=$3}\n                                    /wakes/{for(t=s;t<$3;++t){\n                                              ++zg[g];++zgt[g\",\"t]\n                                           }\n                                    }\n                                    END{for(g in zg)\n                                          if(zg[g]>zg[og])\n                                            og=g;\n                                        for(t=0;t<60;++t)\n                                          if(zgt[og\",\"t]>zgt[og\",\"ot])\n                                            ot=t;\n                                        print og*ot\n                                    }'\n\n```\n\n```awk\nsort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}\n                                    /falls/{s=$3}\n                                    /wakes/{for(t=s;t<$3;++t)\n                                              ++zgt[g\",\"t]\n                                    }\n                                    END{for(gt in zgt)\n                                          if(zgt[gt]>zgt[ogt])\n                                            ogt=gt;\n                                        split(ogt,oa,\",\");\n                                        print oa[1]*oa[2]\n                                    }'\n\n```", "id": "eb24n3b", "owner_tier": 0.3, "score": 0.19753086407407405}, {"content": "Taken from my **[Day 4 Reflections Post](https://github.com/mstksg/advent-of-code-2018/blob/master/reflections.md#day-4)**:\n\n[Haskell] Day 4 was fun because it's something that, on the surface, sounds like it\nrequires a state machine to run through a stateful log and accumulate a bunch\nof time sheets.\n\nHowever, if we think of the log as just a stream of tokens, we can look at at\nit as *parsing* this stream of tokens into time sheets -- no state or mutation\nrequired.\n\nFirst, the types at play:\n\n\ttype Minute = Finite 60\n\n\ttype TimeCard = Map Minute Int\n\n\tdata Time = T { _tYear   :: Integer\n\t              , _tMonth  :: Integer\n\t              , _tDay    :: Integer\n\t              , _tHour   :: Finite 24\n\t              , _tMinute :: Minute\n\t              }\n\t  deriving (Eq, Ord)\n\n\tnewtype Guard = G { _gId :: Int }\n\t  deriving (Eq, Ord)\n\n\tdata Action = AShift Guard\n\t            | ASleep\n\t            | AWake\n\nNote that we have a bunch of \"integer-like\" quantities going on: the\nyear/month/day/hour/minute, the guard ID, and the \"frequency\" in the `TimeCard`\nfrequency map.  Just to help us accidentally not mix things up (like I\npersonally did many times), we'll make them all different types.  A `Minute` is\na `Finite 60` (`Finite 60`, from the *finite-typelits* library, is a type that\nis basically the integers limited from 0 to 59).  Our hours are `Finite 24`.\nOur Guard ID will be a newtype `Guard`, just so we don't accidentally mix it up\nwith other types.\n\nNow, after parsing our input, we have a `Map Time Action`: a map of times to\nactions committed at that time.  The fact that we store it in a `Map` ensures\nthat the log items are ordered and unique.\n\nWe now essentially want to parse a stream of `(Time, Action)` pairs into a `Map\nGuard TimeCard`: A map of `TimeCard`s indexed by the guard that has that time\ncard.\n\nTo do that, we'll use the *parsec* library, which lets us parse over streams of\narbitrary token type.  Our parser type will take a `(Time, Action)` stream:\n\n\timport qualified Text.Parsec as P\n\n\ttype Parser = P.Parsec [(Time, Action)] ()\n\nA `Parser Blah` will be a parser that, given a stream of `(Time, Action)`\npairs, will aggregate them into a value of type `Blah`.\n\nTurning our stream into a `Map Guard TimeCard` is now your standard\nrun-of-the-mill parser combinator program.\n\n\t\n\t-- | We define a nap as an `ASleep` action followed by an `AWake` action.  The\n\t-- result is a list of minutes slept.\n\tnap :: Parser [Minute]\n\tnap = do\n\t    (T _ _ _ _ m0, ASleep) <- P.anyToken\n\t    (T _ _ _ _ m1, AWake ) <- P.anyToken\n\t    pure [m0 .. m1 - 1]     -- we can do this because m0 < m1 always in the\n\t                            --   input data.\n\n\t-- | We define a a guard's shift as a `AShift g` action, followed by\n\t-- \"many\" naps.  The result is a list of minutes slept along with the ID of the\n\t-- guard that slept them.\n\tguardShift :: Parser (Guard, [Minute])\n\tguardShift = do\n\t    (_, AShift g) <- P.anyToken\n\t    napMinutes    <- concat <$> many (P.try nap)\n\t    pure (g, napMinutes)\n\n\t-- | A log stream is many guard shifts. The result is the accumulation of all\n\t-- of those shifts into a massive `Map Guard [Minute]` map, but turning all of\n\t-- those [Minutes] into a frequency map instead by using `fmap freqs`.\n\tbuildTimeCards :: Parser (Map Guard TimeCard)\n\tbuildTimeCards = do\n\t    shifts <- M.fromListWith (++) <$> many guardShift\n\t    pure (fmap freqs shifts)\n\n\nWe re-use the handy `freqs :: Ord a => [a] -> Map a Int` function, to build a\nfrequency map, from [Day 2](https://github.com/mstksg/advent-of-code-2018/blob/master/reflections.md#day-2).\n\nWe can run a parser on our `[(Time, Action)]` stream by using `P.parse ::\nParser a -> [(Time, Action)] -> SourceName -> Either ParseError a`.\n\nThe rest of the challenge involves \"the X with the biggest Y\" situations, which\nall boil down to \"The key-value pair with the biggest *some property of\nvalue*\".\n\nWe can abstract over this by writing a function that will find the key-value\npair with the biggest *some property of value*:\n\n\timport qualified Data.List.NonEmpty as NE\n\n\tmaximumValBy\n\t    :: (a -> a -> Ordring)  -- ^ function to compare values\n\t    -> Map k a\n\t    -> Maybe (k, a)         -- ^ biggest key-value pair, using comparator function\n\tmaximumValBy c = fmap (maximumBy (c `on` snd)) . NE.nonEmpty . M.toList\n\n\t-- | Get the key-value pair with highest value\n\tmaximumVal :: Ord a => Map k a -> Maybe (k, a)\n\tmaximumVal = maximumValBy compare\n\nWe use `fmap (maximumBy ...) . NE.nonEmpty` as basically a \"safe maximum\",\nallowing us to return `Nothing` in the case that the map was empty. This works\nbecause `NE.nonEmpty` will return `Nothing` if the list was empty, and `Just`\notherwise...meaning that `maximumBy` is safe since it is never given to a\nnon-empty list.\n\nThe rest of the challenge is just querying this `Map Guard TimeCard` using some\nrather finicky applications of the predicates specified by the challenge.\nLuckily we have our safe types to keep us from mixing up different concepts by\naccident.\n\n\teitherToMaybe :: Either e a -> Maybe a\n\teitherToMaybe = either (const Nothing) Just\n\n\tday04a :: Map Time Action -> Maybe Int\n\tday04a logs = do\n\t    -- build time cards\n\t    timeCards               <- eitherToMaybe $ P.parse buildTimeCards \"\" (M.toList logs)\n\t    -- get the worst guard/time card pair, by finding the pair with the\n\t    --   highest total minutes slept\n\t    (worstGuard , timeCard) <- maximumValBy (comparing sum) timeCards\n\t    -- get the minute in the time card with the highest frequency\n\t    (worstMinute, _       ) <- maximumVal timeCard\n\t    -- checksum\n\t    pure $ _gId worstGuard * fromIntegral worstMinute\n\n\tday04b :: Map Time Action -> Maybe Int\n\tday04b logs = do\n\t    -- build time cards\n\t    timeCards                      <- eitherToMaybe $ P.parse buildTimeCards \"\" (M.toList logs)\n\t    -- build a map of guards to their most slept minutes\n\t    let worstMinutes :: Map Guard (Minute, Int)\n\t        worstMinutes = M.mapMaybe maximumVal timeCards\n\t    -- find the guard with the highest most-slept-minute\n\t    (worstGuard, (worstMinute, _)) <- maximumValBy (comparing snd) worstMinutes\n\t    -- checksum\n\t    pure $ _gId worstGuard * fromIntegral worstMinute\n\nLike I said, these are just some complicated queries, but they are a direct\ntranslation of the problem prompt.  The real interesting part is the building\nof the time cards, I think!  And not necessarily the querying part.\n\nParsing, again, can be done by stripping the lines of spaces and using\n`words` and `readMaybe`s.  We can use `packFinite :: Integer -> Maybe (Finite\nn)` to get our hours and minutes into the `Finite` type that `T` expects.\n\n\tparseLine :: String -> Maybe (Time, Action)\n\tparseLine str = do\n\t    [y,mo,d,h,mi] <- traverse readMaybe timeStamp\n\t    t             <- T y mo d <$> packFinite h <*> packFinite mi\n\t    a             <- case rest of\n\t      \"falls\":\"asleep\":_ -> Just ASleep\n\t      \"wakes\":\"up\":_     -> Just AWake\n\t      \"Guard\":n:_        -> AShift . G <$> readMaybe n\n\t      _                  -> Nothing\n\t    pure (t, a)\n\t  where\n\t    (timeStamp, rest) = splitAt 5\n\t                      . words\n\t                      . clearOut (not . isAlphaNum)\n\t                      $ str\n\n\nLook, you need to write a Blog. This is too good to be forgotten after 24 hours. I always appreciate a good post explaining how someone solved a problem, especially in haskell.\n\nComing from a duck-typing language I could really appreciate types in this problem, if nothing else to not have to do some pre-coding scanning to see if guards slept until after 01:00, whether 2 guards overlapped etc. As it was I eyeballed the data and accounted for some obvious things (like guards who never sleep on one shift) and hacked together something that worked good enough.\n\nI created a new Record data type with three constructors matching the three line types:\n\n    data Record = BeginShift  Timestamp BadgeID\n                | FallsAsleep Timestamp\n                | WakesUp     Timestamp\n                deriving (Show)\n    \n    parseRecord :: Parser Record\n    parseRecord = begins <|> fallsAsleep <|> wakesUp\n    \n    -- [1518-11-01 00:00] Guard #10 begins shift\n    -- [1518-11-01 00:05] falls asleep\n    -- [1518-11-01 00:25] wakes up\n    begins, fallsAsleep, wakesUp :: Parser Record\n    begins      = BeginShift  <$> timestamp <* string \" Guard #\" <*> number <* string \" begins shift\"\n    fallsAsleep = FallsAsleep <$> timestamp <* string \" falls asleep\"\n    wakesUp     = WakesUp     <$> timestamp <* string \" wakes up\"\n\nI built up the list of guard/minutes asleep using a bit of recursion and pattern matching:\n\n    -- Analyze a (sorted) list of records to determine which minutes\n    -- of the night were slept through by which guards\n    analyze :: [Record] -> [(BadgeID, Minute)]\n    analyze rs = go 0 rs\n    \n      where go :: BadgeID -> [Record] -> [(BadgeID, Minute)]\n    \n            -- Switch the guard we're analyzing\n            go _ (BeginShift _ badge : rs) = go badge rs\n    \n            -- A FallsAsleep record is always followed by a WakesUp record, so pattern match on this pair\n            go badge (FallsAsleep a : WakesUp w : rs) \n                = minutes badge a w ++ go badge rs\n    \n            go _ [] = []\n    \n            -- In our puzzle input, guards don't fall asleep before midnight, which simplifies this\n            minutes b a w = [ (b, minute a + i) | i <- [0 .. minute w - minute a - 1] ]", "id": "eb2576e", "owner_tier": 0.5, "score": 0.2962962961728395}, {"content": "Mods, first off, love your work, you're the best! \n\nSmall request, can the time the thread is unlocked be added to the message? \n\nI.e.\n\n> edit: Leaderboard capped, thread unlocked __at 00:45__!\n\nCan do.\n\nYou may already know this, but you can find that data on the daily leaderboard for each puzzle, [e.g. for day 5](https://adventofcode.com/2018/leaderboard/day/5) it was 00:10.\n\nAs an aside, oh how I wish the links for a personal and private leaderboards were right there in the header. \n\nAh, I actually didn't know that! \n\n", "id": "eb278k4", "owner_tier": 0.7, "score": 0.19753086407407405}, {"content": "[**APL**](https://github.com/jayfoad/aoc2018apl/blob/master/p4.dyalog) \\#89/71\n\nSorting the input is trivial in Dyalog 17.0 thanks to the [Total Array Ordering](https://www.youtube.com/watch?v=8cbPLRAcC7M). Parsing the input lines is a little messy, using global assignments from a dfn to update global state. Once we've got an array `a` telling us how many times each guard is asleep for each minute of the witching hour, it becomes much more elegant:\n\n    f\u2190{\u2283\u2378\u2375=\u2308/,\u2375} \u235d coordinates of maximum value\n    {\u2375\u00d7f \u2375\u2337a}f+/a \u235d part 1\n    \u00d7/f a \u235d part 2\n\n&#x200B;\n\nAre you a human?\n\nwhat.. wizardry is this?!? \n\nFantastic.\n\nBleep bloop, I mean yes. That's why I like writing short programs. Some of these solutions, even ones that made the leaderboard, take 5 lines of code to find the maximum value (or its location) in a list. I can do it with `\u2308/vec` (or `vec\u2373\u2308/vec`, the \"vec-index of the max reduce of vec\"). What's not to like? Especially when you're coding against the clock.\n\nIf you want to learn, you could check out [The APL Orchard](https://chat.stackexchange.com/rooms/52405/the-apl-orchard).\n\nArray wizardry!\n\nIt looks like the notes I took in 400 level math classes(set theory etc). I\u2019m gonna dig into this language later for shits and giggles. \n\nWhat do you... do.. with it? \n\nAnything you want. It's general purpose and you can write [web servers](https://github.com/Dyalog/MiServer) with it if you want. It excels at wrangling arrays of data, in a way that is naturally data-parallel, but you'll find that many problems lend themselves to array-based solutions once you learn to look at them right.\n\ncool shit dude! my background is solely numerical dataset manipulation (giant array output from simulations) so I'm using AoC to force myself to learn better string manipulation and other crap.\n\nkeep on doing insane stuff!\n\nAPL can do insane stuff with numerical datasets! As for strings, the approach is that you just use a 1-d array of characters. But arrays can be arbitrarily nested, so of course you can have arrays of strings, and it works just like any other array of arrays.\n\nThanks for the information.  I did a lot of wolfram/mathematica, and I've been slowly forcing myself back to python because it's free.  R is probably a closer corollary to wolfram but python is just more extensible I think.\n\nat any rate, thanks again for the info! so cool to see all these different approaches.", "id": "eb2btjl", "owner_tier": 0.1, "score": 0.41975308629629626}, {"content": "So did anyone else lose time because they had a correct solution but returned just the minute they chose, forgetting to multiply it with the guard ID? I lost so much time catching that error that it made the difference between spot 60ish and 160 on Part 1.\n\nI try to make the highlighted part of the last paragraph of each puzzle a summary of that puzzle.  Today, it was:\n\n>*What is the ID of the guard you chose multiplied by the minute you chose?*\n\nMy problem was I kept multiplying by the amount of times they slept on the minute, instead of the minute itself. Made that mistake for both parts.\n\nI lost time because I skim read \"*Strategy 1: Find the guard that has the most minutes asleep*\" and stopped reading and tried to put the guard ID.\n\nI got stumped because I did not see that a guard could fall asleep more than once in a shift. I got the first question right, so I assumed that I parsed the input correctly. So I kept trying out new ways to do part 2, but it always gave the same answer. It was not untill I manually sorted the file that I saw that a guard could fall asleep several times!\n\nYou were *totally* clear. I even read that sentence. It's just that my dumb ass got excited about finding a solution and forgot what result was requested.\n\nIn your examples you seem to count as asleep, the minute when the guard fell asleep and NOT the minute when the guard wakes up.\nIs this the right way to identify the second factor?\n\nThe good news is that this comment made me realize why I wasn't getting the right answer for part 1, so thanks!\n\nI'm an idiot\n\nAh, that's rough.\n\nYup, that's what I did as well.", "id": "eb1woib", "owner_tier": 0.7, "score": 0.9999999998765433}, {"content": "Python 3, #2/#2. Reached for dateutil in the heat of the moment although turns out you only need the minute!\n\n\n    guards = collections.defaultdict(list)\n    times = collections.defaultdict(int)\n\n    for line in sorted(inp(4).splitlines()):\n        time, action = line.split('] ')\n\n        time = dateutil.parser.parse(time[1:])\n\n        if action.startswith('Guard'):\n            guard = int(action.split()[1][1:])\n        elif action == 'falls asleep':\n            start = time\n        elif action == 'wakes up':\n            end = time\n            guards[guard].append((start.minute, end.minute))\n            times[guard] += (end - start).seconds\n\n    (guard, time) = max(times.items(), key=lambda i: i[1])\n    (minute, count) = max([\n        (minute, sum(1 for start, end in guards[guard] if start <= minute < end))\n    for minute in range(60)], key=lambda i: i[1])\n\n    print('part 1:', guard * minute)\n\n    (guard, minute, count) = max([\n        (guard, minute, sum(1 for start, end in guards[guard] if start <= minute < end))\n    for minute in range(60) for guard in guards], key=lambda i: i[2])\n\n    print('part 2:', guard * minute)\n\n\nhaha, the dates set me off as well, didn't realize they were there just for sorting.\n\nAnd since they use the ISO standard, you can just sort them as strings and don't need to worry about it haha.\n\n[deleted]\n\nI took advantage of this!\n\n[](/facehoof) And here I went and made an entire linked-list sorting algorithm when I could have just used 'sort'... *sigh*.\n\nI didn't figure that out, even though it's the main reason I advocate for ISO standard datetime. *sigh*.\n\nBut Powershell will just `get-date 1518-01-02` without any problem, so I didn't really need to, but it would still have been quicker. D'oh.\n\nThe date is pre-Gregorian, did you account for Old-Style vs New-Style dates? ;)\n\nPtsch, I'm a high level scripting pleb, and high level languages are the real world version of Douglas Adam's [somebody else's problem field](https://www.goodreads.com/quotes/691174-the-somebody-else-s-problem-field-is-much-simpler-and-more) (from Hitchhiker's Guide to the Galaxy).\n\nI'm sure the .Net elves will have understood and taken care of that hard stuff, so I don't have to ;P", "id": "eb1w7xy", "owner_tier": 0.1, "score": 0.32098765419753084}, {"content": "Finally got top 100 :) 62/43 with Gawk (used multidimensional arrays so I guess it's not awk anymore)\n\nThis is exact code I submitted, no ifs ands or buts. \n\nRun `sort -V` on the file before inputting to Awk\n\nPart 1:\n\n    BEGIN {\n    #   RS\n      FPAT = \"#?[0-9]+\"\n      guard=99999999\n    }\n    /#[0-9]+/{\n      guard=$6\n    }\n    /wake/{\n      minute=$5\n      for(i=fell_asleep[guard];i<minute;i++) {\n        asleep_minutes[guard][i]++\n      }\n      sleep_time[guard] += (minute - fell_asleep[guard])\n    }\n    /falls/ {\n      minute=$5\n      fell_asleep[guard] = minute\n    }\n    END {\n      max_sleep_time=0\n      max_sleep_guard=999999999\n      for(guard in sleep_time) {\n        if (sleep_time[guard] > max_sleep_time) {\n          max_sleep_guard = guard\n          max_sleep_time = sleep_time[guard]\n        }\n      }\n      max_minute=9999999\n      max_minute_val=0\n      for(minute in asleep_minutes[max_sleep_guard]) {\n        if (asleep_minutes[max_sleep_guard][minute] > max_minute_val) {\n          max_minute= minute\n          max_minute_val=asleep_minutes[max_sleep_guard][minute]\n        }\n      }\n      print max_sleep_guard\n      print max_minute\n    }\n\nPart 2:\n\n    BEGIN {\n    #   RS\n      FPAT = \"#?[0-9]+\"\n      guard=99999999\n    }\n    /#[0-9]+/{\n      guard=$6\n    }\n    /wake/{\n      minute=$5\n      for(i=fell_asleep[guard];i<minute;i++) {\n        asleep_minutes[guard][i]++\n      }\n      sleep_time[guard] += (minute - fell_asleep[guard])\n    }\n    /falls/ {\n      minute=$5\n      fell_asleep[guard] = minute\n    }\n    END {\n    #   max_sleep_time=0\n    #   max_sleep_guard=999999999\n    #   for(guard in sleep_time) {\n    #     if (sleep_time[guard] > max_sleep_time) {\n    #       max_sleep_guard = guard\n    #       max_sleep_time = sleep_time[guard]\n    #     }\n    #   }\n      max_minute=9999999\n      max_minute_val=0\n      max_guard=99999999999\n      for(guard in asleep_minutes) {\n        for(minute in asleep_minutes[guard]) {\n          if (asleep_minutes[guard][minute] > max_minute_val) {\n            max_minute= minute\n            max_guard=guard\n            max_minute_val=asleep_minutes[guard][minute]\n          }\n        }\n      }\n      print max_guard\n      print max_minute\n    }\n\n\nWhat am I looking at?\n\nI've been loving your AWK solutions. You have a Github?\n\nNice use of pattern selectors, I'm just getting familiar with the language atm and that's awesome.\n\n\nIt's the language that Larry Wall thought was too readable, so he invented Perl.\n\n[Awk](https://en.wikipedia.org/wiki/AWK)\n\nThanks :)\n\nhttps://github.com/markasoftware\n\nI don't make anything OSS with Awk. I have used it only for other speed-coding stuff and some golf, as well as that one time I needed to process 300GB of CSV files */me shudders*.", "id": "eb1w92n", "owner_tier": 0.5, "score": 0.2222222220987654}, {"content": "**FORTRAN**\n\nWill edit with cleaned up code later. Got held up a looong time by having copy pasted a line into another loop without then updating which loop index it was using. \n\n>While this example listed the entries in chronological order, your  entries are in the order you found them. You'll need to organize them  before they can be analyzed.\n\nNo.\n\n    PROGRAM DAY4\n      IMPLICIT NONE\n      INTEGER :: I,J,K,L,HOUR,MINUTE,DAY\n      INTEGER :: IERR\n      CHARACTER(LEN=50),ALLOCATABLE :: LINES(:)\n      CHARACTER(LEN=50) :: LINE\n      INTEGER, ALLOCATABLE :: GUARDS(:)\n      LOGICAL, ALLOCATABLE :: ASLEEP(:,:)\n      INTEGER, ALLOCATABLE :: DATES(:)\n      CHARACTER(LEN=8) :: DATE\n      INTEGER :: GUARD,BESTGUARD,SLEEP,BESTSLEEP,SLEEPMINUTE(0:59),BESTMINUTE,BESTMINUTEAMMOUNT\n      OPEN(1,FILE='input.txt')\n      I=0\n      DO\n         READ(1,*,IOSTAT=IERR)\n         IF(IERR.NE.0)EXIT\n         I=I+1\n      END DO\n      ALLOCATE(LINES(I))\n      REWIND(1)\n      READ(1,'(A)')LINES\n      CLOSE(1)\n    \n      J=1\n      DO K=2,I\n         IF(ANY(LINES(1:K-1)(1:12).EQ.LINES(K)(1:12)))CYCLE\n         J=J+1\n      END DO\n    \n      ALLOCATE(GUARDS(J),ASLEEP(J,0:59),DATES(J))\n      DATES=99999999\n      ASLEEP=.FALSE.\n      GUARDS=0\n      WRITE(DATE,'(A)') LINES(1)(2:5)//LINES(1)(7:8)//LINES(1)(10:11)\n      READ(DATE,*) DAY\n      DATES(1)=DAY\n      L=2\n      DO K=2,I\n         WRITE(DATE,'(A)') LINES(K)(2:5)//LINES(K)(7:8)//LINES(K)(10:11)\n         READ(DATE,*) DAY\n         IF(ANY(DATES(1:L-1).EQ.DAY))CYCLE\n         DATES(L)=DAY\n         L=L+1\n      END DO\n    \n      DO J=1,I\n         WRITE(DATE,'(A)') LINES(J)(2:5)//LINES(J)(7:8)//LINES(J)(10:11)\n         READ(DATE,*) DAY\n         READ(LINES(J)(13:14),*)HOUR\n         READ(LINES(J)(16:17),*)MINUTE\n         IF(HOUR>0)DAY=MINVAL(DATES,MASK=DATES>DAY)\n         SELECT CASE (LINES(J)(20:24))\n         CASE ('Guard')\n            READ(LINES(J)(SCAN(LINES(J),'#')+1:SCAN(LINES(J),'b')-1),*) GUARDS(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1))\n         CASE('wakes')\n            ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)=.NOT.ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)\n         CASE('falls')\n            ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)=.NOT.ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)\n         END SELECT\n      END DO\n    \n      !Part 1\n      BESTGUARD=0\n      BESTSLEEP=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         SLEEP=0\n         GUARD=GUARDS(I)\n         DO J=1,SIZE(GUARDS,DIM=1)\n            IF (GUARDS(J).EQ.GUARD) SLEEP=SLEEP+COUNT(ASLEEP(J,:))\n         END DO\n         IF(SLEEP>BESTSLEEP)THEN\n            BESTSLEEP=SLEEP\n            BESTGUARD=GUARD\n         END IF\n      END DO\n      SLEEPMINUTE=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         IF(GUARDS(I).NE.BESTGUARD)CYCLE\n         DO J=0,59\n            IF(ASLEEP(I,J)) SLEEPMINUTE(J)=SLEEPMINUTE(J)+1\n         END DO\n      END DO\n      WRITE(*,'(A,I0)') 'Part 1: ',(MAXLOC(SLEEPMINUTE,DIM=1)-1)*BESTGUARD\n    \n      !PART 2\n      BESTMINUTE=0\n      BESTMINUTEAMMOUNT=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         SLEEPMINUTE=0\n         GUARD=GUARDS(I)\n         DO J=1,SIZE(GUARDS,DIM=1)\n            IF (GUARDS(J).NE.GUARD)CYCLE\n            DO K=0,59\n               IF(ASLEEP(J,K)) SLEEPMINUTE(K)=SLEEPMINUTE(K)+1\n            END DO\n         END DO\n         IF(MAXVAL(SLEEPMINUTE)>BESTMINUTEAMMOUNT)THEN\n            BESTMINUTE=MAXLOC(SLEEPMINUTE,DIM=1)-1\n            BESTGUARD=GUARD\n            BESTMINUTEAMMOUNT=MAXVAL(SLEEPMINUTE)\n         END IF\n      END DO\n      WRITE(*,'(A,I0)') 'Part 2: ',BESTGUARD*BESTMINUTE\n    \n      DEALLOCATE(LINES,DATES,GUARDS,ASLEEP)\n    END PROGRAM DAY4", "id": "eb20muy", "owner_tier": 0.5, "score": 0.04938271592592593}, {"content": "#Perl 6\n\nLooks like I'm posting the first Perl 6 solution of the day.\n\nThis is post-cleanup... but more or less what I wrote, except all the functions were inline. My cleanup was just moving them all into (anonymous) functions so the implementation reads better.\n\n    # Helper functions\n    my &guard              = { .substr(26).words.head         }\n    my &mins               = { .substr(15, 2).Int             }\n    my &total-sleep-length = { .value.total                   }\n    my &most-slept-minute  = { .value.max(*.value).value      }\n    my &magic-value        = { .key \u00d7 .value.max(*.value).key }\n\n    # State variables\n    my (%sleep, $guard, $falls);\n\n    # Data collection\n    for 'input'.IO.lines.sort {\n        when *.contains('Guard') { $guard = .&guard }\n        when *.contains('falls') { $falls = .&mins  }\n        when *.contains('wakes') { %sleep{$guard} \u228e= $falls ..^ .&mins }\n    }\n\n    # Output solution\n    for &total-sleep-length, &most-slept-minute -> &func {\n        say %sleep.max(&func).&magic-value\n    }\n\nThe `\u228e` operator is the Multiset Union operator (ASCII version: `(+)`). Perl 6 has a `Bag` type (aka. [Multiset](https://oeis.org/wiki/Multisets)). I create a range of the minutes slept and add those to the Bag of values for that guard.\n\n# Python 3\n\nRe-implementation of the above in Python, just because.\n\nI laugh in the face of PEP8!\n\n    from collections import Counter, defaultdict\n\n    # Helper functions\n    get_guard          = lambda x: int(x[26:].split()[0])\n    get_mins           = lambda x: int(x[15:17])\n    total_sleep_length = lambda x: sum(x[1].values())\n    most_slept_minute  = lambda x: max(x[1].values())\n    magic_value        = lambda x: x[0] * max(x[1].items(), key=lambda y: y[1])[0]\n\n    # State variables\n    sleep = defaultdict(Counter)\n\n    # Data collection\n    for line in sorted(open('input').readlines()):\n        if   'Guard' in line: guard = get_guard(line)\n        elif 'falls' in line: falls = get_mins(line)\n        elif 'wakes' in line: sleep[guard].update(range(falls, get_mins(line)))\n\n    # Output solution\n    for func in (total_sleep_length, most_slept_minute):\n        print(magic_value(max(sleep.items(), key=func)))", "id": "eb26m6l", "owner_tier": 0.3, "score": 0.03703703691358025}, {"content": "Day 4...IN EXCEL??!?!?\n\n     =COUNTBYCELLCOLOR(M4:BT4,$I$1)\n\nhttps://github.com/thatlegoguy/AoC2018", "id": "eb385eh", "owner_tier": 0.3, "score": 0.03703703691358025}, {"content": "Python 2, just missed the leaderboard for the second star. I completely forgot about 'in' and spent too long indexing the strings, as well as not initially sorting the data by time *and* sorting the list of guards the wrong way, which I figured out after I tried with the test code.\n(The code has been edited.)\n\n\tl = sorted(lines)\n\tguards = defaultdict(lambda:[0 for x in range(60)])\n\tfor s in l:\n\t\tif s[25]==\"#\":\n\t\t\tg=s.split()[3]\n\t\telif s[25]==\"a\":\n\t\t\tst=int(s[15:17])\n\t\telse: # wake up\n\t\t\tt=int(s[15:17])\n\t\t\tfor x in range(st,t):\n\t\t\t\tguards[g][x]+=1\n\n\t# part 1\n\tg1 = sorted(guards.keys(), key=lambda g:-sum(guards[g]))[0]\n\t# part 2\n\tg2 = sorted(guards.keys(), key=lambda g:-max(guards[g]))[0]\n\n\tfor g in [g1,g2]:\n\t\tgh = guards[g]\n\t\tminute = gh.index(max(gh))\n\t\tprint int(g[1:])*minute\n\n\nNice strategy!", "id": "eb1wu1a", "owner_tier": 0.1, "score": 0.02469135790123457}, {"content": "**Python 3** 51/63\n\n    from collections import Counter, defaultdict\n    from datetime import datetime\n\n    guards = defaultdict(Counter)\n    for t, m in [l.split('] ') for l in sorted(s.splitlines()) if l]:\n        t = datetime.strptime(t, '[%Y-%m-%d %H:%M')\n        if '#' in m:     g = int(m.split('#')[1].split()[0])\n        if 'falls' in m: start = t\n        if 'wakes' in m:\n            minutes = int((t - start).total_seconds() // 60)\n            guards[g].update(Counter((start.minute+i)%60 for i in range(minutes)))\n\n    _, id = max((sum(c.values()), id) for id, c in guards.items())\n    part1 = id * guards[id].most_common()[0][0]\n\n    (_, minute), id = max((c.most_common()[0][::-1], id) for id, c in guards.items())\n    part2 = id * minute\n\n\nAt which point do you read the input? Looks like the variable `s` is undefined, so presumably that's the input?\n\n[deleted]\n\n> so presumably that's the input\n\nThat's right. I just copy and paste it manually to `s` variable in IPython.\n\n> Counter\n\nIt looks like Counter is a multiset, is that right?\n\nhow did you copy paste the .txt to s?\n\n\nI am not the guy you asked, but I assume they just copy and paste the text using their computer's clipboard or similar.\n\n[deleted]\n\nPersonally I just read the input files using Python and then process them as necessary.  \nSomething like this works fine:\n\n    with open(\"input.txt\") as f:\n        data = f.readlines()\n        # do other things with the data here", "id": "eb1x7yq", "owner_tier": 0.1, "score": 0.16049382703703702}, {"content": "I thought I was wayyy out of the running on that one, but 19 minutes and first time into leaderboard part 2!  Thanks PowerShell date handling!  #37 / #61\n\n[Card] today's puzzle would have been a lot easier if my language supported: Python's enumerate().\n\n#####PowerShell\n\nCode:\n\n    # hashtable of Guard ID -> (0,0,0,0 ... array of 60 minutes for that guard.\n    $guards = @{}\n\n    Get-Content .\\data.txt  | sort-object {\n\n        [void]($_ -match '\\[(.*)\\] (.*)')\n        Get-Date $matches[1]\n    \n    } | foreach { \n\n        [void]($_ -match '\\[(.*)\\] (.*)')\n        $date = get-date $matches[1]\n        $message = $matches[2]\n\n        switch -regex ($message)    # switch state machine\n        {\n            # For a Guard identifier line, get the ID, set them up with 60 blank minutes.\n            'Guard #(\\d+)' { \n                $script:guard = $matches[1]\n                if (-not $guards.ContainsKey($script:guard)){ $guards[$script:guard] = @(0) * 60 }\n            }\n\n            # If they fell asleep, store the date for use when they wake.\n            'sleep' { $script:sleep = $date }\n        \n            # If they wake, loop over the minutes from sleep..wake and increment their array\n            'wakes' {\n                $script:sleep.Minute..($date.Minute-1)| foreach-object {\n                    $guards[$script:guard][$_]++\n                }\n            }\n        }\n    }\n\n    # Part 1, most minutes asleep, which minute is highest\n    $mostSleepy = $guards.GetEnumerator() | sort-object { $_.Value | measure -sum | % sum } | select -Last 1\n    $minute = $mostSleepy.Value.IndexOf(($mostSleepy.Value | sort)[-1])\n    \"Part 1: Guard $($mostSleepy.Name), minute $minute\"\n    $minute * $mostSleepy.Name\n\n    # Part 2, guard with most same-minute asleep\n    $mostSame = $guards.GetEnumerator() | sort-object { ($_.Value | sort)[-1] } | select -Last 1\n    $minute = $mostSame.Value.IndexOf(($mostSame.Value | sort)[-1])\n    \"Part 2: Guard $($mostSame.Name), minute: $minute\"\n     $minute * $mostSame.Name\n\n(I actually made the hashtable and eyeballed the largest value and counted the minutes by hand, because it was quicker than writing code to do it, but I've added that code here)\n\n\n\n\nNo date handling for me, the input format meant a simple alphabetic sort put them in the correct chronological order.\n\nSame sort of process for checking which line type we're on, set the current guard and start minute on the respective line types, and add to tables during wake up lines.\n\nThen just like you I did the relevant sorting to find the sleepiest guard, and found his sleepy minute.\n\nI'm quite pleased with my part 2 solution.  I stored the current Guard and current Minute concatenated as a string as the key to a hash table.  Each time that Guard/Minute was hit, we only need to do a simple increment.  At first pass I was going to parse this string out at the end, but instead I switched my 'delimiter' between Guard and Minute to be `*`.  The multiplication to get the answer is then an Invoke-Expression on the key of the highest value in the hash table.\n\n    $in = gc .\\04-input.txt | sort\n    \n    $gHash = @{}\n    $mHash = @{}\n    \n    for($i = 0;$i -lt $in.count;$i++){\n    \t$m = @(([regex]\"\\d+\").matches($in[$i]).value)\n    \t\n    \t\n    \tif($m[5]){\n    \t\t$curGuard = +$m[5]\n    \t}elseif($in[$i] -like \"*falls asleep*\"){\n    \t\t$sleepStart = +$m[4]\n    \t}else{\n    \t\t$sleepEnd = +$m[4]-1\n    \t\t\n    \t\t$sleepStart..$sleepEnd | % {\n    \t\t\t$gHash[$curGuard]+=@($_)\n    \t\t\t$mHash[\"$curGuard * $_\"]++\n    \t\t}\n    \t}\n    \t\n    }\n    \n    $sleepyGuard = ($gHash.getEnumerator() | sort {$_.value.count} -descending)[0].name\n    $sleepyMinute = ($gHash[$sleepyGuard] | group | sort count -descending)[0].name\n    \n    $sleepyGuard*$sleepyMinute\n    \n    ($mHash.getEnumerator() | sort value -descending)[0].name | iex  \n\n>\\[void\\]($\\_ -match '\\\\\\[(.\\*)\\\\\\] (.\\*)')\n\nHi ka-splam, been folliowing (4 days behind at the moment!) your PS solutions.\n\nWhat does the above do/mean please?\n\nYou said you visually did calculation - which i did also - because I could nt get the minute part to work in Part 1. Evaluates to -1.\n\nDid you test it?\n\nHi \ud83d\udc4b,\n\nThe `-match` operator takes a string on the left and a pattern on the right, and tries to match the string with the pattern. `$_` is my input line each time. So this pattern is `sort-object { $_ ...}`  and sort-object gets the input, runs the scriptblock on it, and then sorts on the result of that. So I pull the date out of the lines, make it a PowerShell DateTime so it will sort properly.\n\n(I later learned that most of this is unnecessary, and if I'd just put `get-content .\\data.txt | sort-object | foreach {}` it would have sorted properly anyway. Oops.).\n\n`-match` returns true/false, which I don\u2019t care about here because I made sure it will always match on my input data. So I take the result and cast it to \u2018[void]` which makes it vanish. Instead I could have used `$result = $_ -match '..pattern..'` but that leaves a variable hanging around that is not going to be used, matter of choice if that bothers you.\n\nWhen `-match` can match the pattern to the text, it sets the automatic variable `$matches` with some information. So the next line works with that as a way to use the result of this line. `$matches[1]` is the first group `()` in the pattern.\n\nI think it\u2019s pulling the date out of the square brackets and then the text after the square brackets, is it? (The pattern is a regex).\n\n(Was the hashtable explanation any help at all btw?)\n\nDid I test what?  This code works on my input, yes.\n\nWhen I first did it by hand, I poked my finger at the screen and counted 59, 58, 57, 56 .. ok minute 45 has the biggest for guard 2663 so 2663*45 and put that into the site, no double-check then, too much hurry.\n\nI am sure it is me and my code. Thanks.", "id": "eb1wuff", "owner_tier": 0.7, "score": 0.1111111109876543}, {"content": "Yeah!!! I have no idea what I'm doing anymore!!! Ruby\n\n[Card] [Image](https://i.imgur.com/U8iWZRt.png)\n\n    h={}\n    ll=0\n    sl={}\n    p (a=$<.sort).map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    sl[ll]=b\n    elsif $3\n    u,i,l,n=sl[ll]\n    kk=h[ll] ? h[ll] : h[ll]=0\n    kk +=  (o*60+q)- (l*60+n)\n    h[ll] = kk\n    end\n    }\n    guard = h.max_by{|x,y|y}.first\n    p guard\n    \n    # part 1\n    \n    sl=0\n    qqq=Hash.new 0\n    a.map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    (sl=q) if ll == guard\n    elsif $3\n    (n=sl\n    (qqq[n]+=1\n    \n    n+=1\n    n=0 if n==60)while n!=q) if ll == guard\n    end\n    }\n    p qqq.max_by{|x,y|y}.first*guard\n    \n    # part 2\n    qqqt=Hash.new{Hash.new 0}\n    qsl={}\n    a.map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    (qsl[ll]=q) #if ll == guard\n    elsif $3\n    (n=qsl[ll]\n    qqq=qqqt.has_key?(ll) ? qqqt[ll] : (qqqt[ll]=qqqt[ll])\n    (qqq[n]+=1\n    \n    n+=1\n    n=0 if n==60)while n!=q) #if ll == guard\n    end\n    }\n    p qqqt\n    a,b= qqqt.max_by{|x,y|y.max_by{|x,y|y}.last}\n    p a*b.max_by{|x,y|y}.first\n    p b.max_by{|x,y|y}.last\n    \n\nEdit: Yes, just like all the previous days, I didn't read the question...\n\nEdit: Changed the layout of the code so you can run it without any hassle and it will just work. If you're reading this, bless you", "id": "eb1xcir", "owner_tier": 0.3, "score": 0.02469135790123457}, {"content": "Today's puzzle would have been a lot easier if my language supported looser types.\n\nI also need to learn how to read the instructions closer to make my life simpler!\n\nAnother day, another Rust:\n\n    use aoc2018::*;\n    use chrono::{Timelike, Duration};\n    \n    fn main() -> Result<(), Error> {\n        let mut records = Vec::new();\n    \n        for line in BufReader::new(input!(\"day4.txt\")).lines() {\n            let line = line?;\n            let (date, rest) = line.split_at(18);\n            let date = chrono::NaiveDateTime::parse_from_str(date, \"[%Y-%m-%d %H:%M]\")?;\n            records.push((date, rest.trim().to_string()));\n        }\n    \n        records.sort_by(|a, b| a.0.cmp(&b.0));\n    \n        let mut current = 0u32;\n        let mut asleep = None;\n    \n        let mut minutes_asleep = HashMap::<_, u32>::new();\n        let mut guard_asleep_at = HashMap::<_, HashMap<u32, u32>>::new();\n        let mut minute_asleep = HashMap::<_, HashMap<u32, u32>>::new();\n    \n        for (date, rest) in records {\n            if rest.ends_with(\"begins shift\") {\n                current = str::parse(&rest.split(\" \").nth(1).unwrap()[1..])?;\n                asleep = None;\n                continue;\n            }\n    \n            match rest.as_str() {\n                \"falls asleep\" => {\n                    asleep = Some(date);\n                }\n                \"wakes up\" => {\n                    let asleep = asleep.as_ref().expect(\"guard did not fall asleep\");\n                    let count = date.signed_duration_since(asleep.clone()).num_minutes() as u32;\n    \n                    *minutes_asleep.entry(current).or_default() += count;\n    \n                    let mut m = asleep.minute();\n    \n                    for i in 0..count {\n                        *guard_asleep_at\n                            .entry(current)\n                            .or_default()\n                            .entry(m)\n                            .or_default() += 1;\n    \n                        *minute_asleep\n                            .entry(m)\n                            .or_default()\n                            .entry(current)\n                            .or_default() += 1;\n    \n                        m += 1;\n                    }\n                }\n                other => {\n                    panic!(\"other: {}\", other);\n                }\n            }\n        }\n    \n        let max = minutes_asleep\n            .into_iter()\n            .max_by(|a, b| a.1.cmp(&b.1))\n            .expect(\"no max asleep\");\n    \n        let pair = guard_asleep_at\n            .remove(&max.0)\n            .unwrap_or_default()\n            .into_iter()\n            .max_by(|a, b| a.1.cmp(&b.1))\n            .expect(\"no max guard asleep\");\n    \n        let mut sleep_min = None;\n    \n        for (ts, guards) in minute_asleep {\n            if let Some((guard, times)) = guards.into_iter().max_by(|a, b| a.1.cmp(&b.1)) {\n                sleep_min = match sleep_min {\n                    Some((ts, guard, max)) if max > times => Some((ts, guard, max)),\n                    Some(_) | None => Some((ts, guard, times)),\n                };\n            }\n        }\n    \n        let sleep_min = sleep_min.expect(\"no result found\");\n    \n        assert_eq!(pair.0 * max.0, 19830);\n        assert_eq!(sleep_min.0 * sleep_min.1, 43695);\n        Ok(())\n    }\n\nIn case you didn't know, you can use the include_str macro instead of requiring a BufReader with the input macro, e.g.\n\n    for line in include_str!(\"day4.txt\").lines() {\n        ...\n    }\n\nSaves a few keystrokes!", "id": "eb1y2im", "owner_tier": 0.5, "score": 0.01234567888888889}, {"content": "##Powershell 5.1\n[card] Python's sum()\n\nBoth halves used this basic layout:\n\n    $data = Get-Content $inputPath | Sort-Object\n    \n    $timer = New-Object System.Diagnostics.Stopwatch\n    $timer.Start()\n    \n    $guards = @{}\n    $curDate = ''\n    $curGuard = -1\n    [int]$sleepMinute = -1\n    foreach ($line in $data) {\n        $tokens = $line.Replace('[', '').Replace(']', '').Replace(':', ' ').Replace('#', '').Split(' ')\n        if ($tokens[0] -ne $curDate -or [int]$tokens[1] -eq 23) {\n            $curDate = $tokens[0]\n            if ($tokens[4] -ne 'asleep') {\n                [int]$curGuard = $tokens[4]\n            }\n            if ($null -eq $guards[$curGuard]) {\n                $guards[$curGuard] = New-Object int[] 60\n            }\n        }\n    \n        if ($tokens[3] -eq 'falls') {\n            [int]$sleepMinute = $tokens[2]\n        }\n        elseif ($tokens[3] -eq 'wakes') {\n            for ($i = $sleepMinute; $i -lt [int]$tokens[2]; $i++) {\n                $guards[$curGuard][$i]++\n            }\n        }\n    \n    }\n\nBasically enumerate everything a hashtable, with the key as the guard's ID and a 60 int array for the minutes as the value, and the amount of time they were asleep. then hook the appropriate ending on the bottom from below\n\n###Part 1\n\n    $curBestGuard = -1\n    $curBestGuardTime = -1\n    $curBestMinute = -1\n    \n    foreach ($g in $guards.Keys) {\n        $sum = 0\n        $guards[$g] | ForEach-Object {$sum += $_}\n        if ($sum -gt $curBestGuardTime) {\n            $curBestGuard = $g\n            $curBestGuardTime = $sum\n            $maximum = ($guards[$g] | Measure-Object -Max).Maximum\n            [int]$curBestMinute = [Array]::IndexOf($guards[$g], [int]$maximum) \n            \n        }\n    }\n    \n    $check = [int]$curBestGuard * [int]$curBestMinute\n    Write-Host $check\n    $timer.Stop()\n    Write-Host $timer.Elapsed\n\nAverage runtime: 0.058s\n\n###Part 2\n\n    $curBestGuard = -1\n    $curBestGuardTime = -1\n    $curBestMinute = -1\n    \n    foreach ($g in $guards.Keys) {\n        $maximum = ($guards[$g] | Measure-Object -Max).Maximum\n        if ($maximum -gt $curBestMinute) {\n            [int]$curBestGuardTime = [Array]::IndexOf($guards[$g], [int]$maximum)\n            $curBestGuard = $g\n            $curBestMinute = $maximum\n        }\n            \n    }\n    \n    \n    $check = [int]$curBestGuard * [int]$curBestGuardTime\n    Write-Host $check\n    \n    $timer.Stop()\n    Write-Host $timer.Elapsed\n\n\nAverage runtime 0.053s \n\n", "id": "eb1ycuy", "owner_tier": 0.9, "score": -1.2345678937315173e-10}, {"content": "##**C++**\n\n    int main(int argc, char* argv[]) {\n        std::ifstream ifs(argv[1]); std::vector<std::string> lines;\n        for (std::string l; std::getline(ifs, l);) { lines.push_back(l); }\n        std::sort(lines.begin(), lines.end());\n\n        std::map<int, std::array<int, 60>> guards;\n        int s = -1, id = -1, *g = nullptr; std::smatch m;\n        for (const auto& l : lines)\n            if (std::regex_match(l, m, std::regex(R\"(.*Guard #(\\d+) begins shift)\")))\n                g = guards[std::stoi(m[1])].data();\n            else if (std::regex_match(l, m, std::regex(R\"(.*:(\\d+)\\] wakes up)\")))\n                std::for_each(g+s, g+std::stoi(m[1]), [](auto& x){x++;});\n            else if (std::regex_match(l, m, std::regex(R\"(.*:(\\d+)\\] falls asleep)\")))\n                s = std::stoi(m[1]);\n        \n        auto maxMinute = [](const auto& gi) { return[&gi](auto m) -> std::pair<int,int>{\n            return {std::distance(gi.begin(), m), *m};}(std::max_element(gi.begin(), gi.end()));};\n\n        const auto& [id1,guardInfo1] = (*std::max_element(guards.begin(), guards.end(),\n            [](auto& l, auto& r) { return std::reduce(l.second.begin(), l.second.end()) <\n                std::reduce(r.second.begin(), r.second.end()); }));\n        const auto& minute1 = maxMinute(guardInfo1).first;\n\n        const auto& [id2,guardInfo2] = *std::max_element(guards.begin(), guards.end(),\n            [&](auto& l, auto& r) { return maxMinute(l.second).second < maxMinute(r.second).second; });\n        const auto& minute2 = maxMinute(guardInfo2).first;\n\n        std::cout << \"1: \" << minute1 * id1 << \"\\n\" << \"2: \" << minute2 * id2 << \"\\n\";\n    }\n\nI think Part 2 should be the following, since maxMinute returns an index, and not the value there!\n\n    const auto &[id2, guardInfo2] =\n            *std::max_element(guards.begin(), guards.end(), [&](auto &l, auto &r) {\n                return l.second[maxMinute(l.second)] <\n                       r.second[maxMinute(r.second)];\n            });\n\n\n\nWow, my solution was very similar to yours.\n\n    #include <unordered_map>\n    #include <vector>\n    #include <range/v3/algorithm.hpp>\n    #include <range/v3/getlines.hpp>\n    #include <range/v3/numeric.hpp>\n    #include <range/v3/view/slice.hpp>\n    \n    namespace view = ranges::view;\n    \n    template <>\n    template <bool part2>\n    void\n    Day<4>::solve(std::istream& is, std::ostream& os) {\n      std::vector<std::string> events(ranges::getlines(is));\n      ranges::sort(events);\n      std::unordered_map<int, std::array<int, 60>> guards;\n      {\n        int id {0}, start {0}, stop {0};\n        char c;\n        for (std::string const& s : events) {\n          if (sscanf(s.c_str() + 15, \"%d] %c\", &stop, &c) == 2 && c == 'w') {\n            for (auto& t : guards[id] | view::slice(start, stop)) {\n              ++t;\n            }\n          } else if (sscanf(s.c_str() + 15, \"%d] %c\", &start, &c) == 2 && c == 'f') {\n          } else if (sscanf(s.c_str() + 19, \"Guard #%d\", &id) == 1) {\n          }\n        }\n      }\n      auto max = [](const auto& l) {\n        return ranges::distance(ranges::begin(l), ranges::max_element(l));\n      };\n      auto map = [&](auto const& pair) {\n        if constexpr (auto& [_, l] = pair; part2) {\n          return ranges::max(l);\n        } else {\n          return ranges::accumulate(l, 0);\n        }\n      };\n      const auto& [id, data] = *ranges::max_element(guards, std::less<>{}, map);\n      const auto& minute = max(data);\n      os << id * minute << std::endl;\n    }\n\nI'm going to have to study this later. It's short but not hackey. Chaining algorithms ftw!\n\nWell spotted.  I think I screwed that up when trying to condense it, although weirdly I thought I checked and the answer was still correct :/\n\nI'll fix it when I get home from work.\n\nFixed now.  Interestingly, it still gave the right answer with the wrong code.  I guess in my input, the minute with the maximum sleep time just happened to always be a higher value than the minute it was compared to, ha!\n\nNice!  I've gotta start playing with ranges - they're going to change everything...\n\nI use AOC as a testbed so I can learn how to use them effectively", "id": "eb1zdih", "owner_tier": 0.3, "score": 0.12345678999999998}], "link": "https://www.reddit.com/r/adventofcode/comments/a2xef8/2018_day_4_solutions/", "question": {"content": "#--- Day 4: Repose Record ---\n\n***\n\nPost your solution as a comment or, for longer solutions, consider linking to your repo (e.g. GitHub/gists/Pastebin/blag or whatever).\n\nNote: The Solution Megathreads are for *solutions* only. If you have questions, please post your own thread and make sure to flair it with `Help`.\n\n***\n\n### Advent of Code: The Party Game!\n\n[Click here for rules](/r/adventofcode/w/aoctpg)\n\nPlease prefix your card submission with something like [Card] to make scanning the megathread easier.  THANK YOU!\n\n#### [Card prompt: Day 4](https://i.imgur.com/ZmeXVnnm.jpg)\n\nTranscript:\n> Today\u2019s puzzle would have been a lot easier if my language supported ___.\n\n***\n\n###~~This thread will be unlocked when there are a significant number of people on the leaderboard with gold stars for today's puzzle.~~\n###*edit:* Leaderboard capped, thread unlocked!", "id": "a2xef8", "title": "-\ud83c\udf84- 2018 Day 4 Solutions -\ud83c\udf84-", "traffic_rate": 38.337674714104196}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "I wrote about [the history of the Schwartzian Transform](https://www.perl.com/article/the-history-of-the-schwartzian-transform/), including its origins in Lisp.", "id": "ffd6f5w", "owner_tier": 0.3, "score": 0.6363636354545454}, {"content": "I'm not surprised. I mean programming languages borrow and steal stuff from each other since the beginning of time, but I love it when the languages that are \"cool\" right now start adding things that Perl has had for ages, even though these same people *usually* shit on Perl.\n\nI think ES6 added a few that I knew Perl had for a while. String interpolation and optional chaining just off the top of my head. Just waiting for autovivification to inevitably arrive and be touted for how awesome and novel it is.\n\n> but I love it when the languages that are \"cool\" right now start adding things that Perl has had for ages\n\nOr things that aren't so much \"cool\" as \"needed for security\".\n\nThe one that seemed to hit a number of languages a few years ago (with papers and [presentations](https://fahrplan.events.ccc.de/congress/2011/Fahrplan/attachments/2007_28C3_Effective_DoS_on_web_application_platforms.pdf)) was denial-of-service through hash collisions. Perl has never guaranteed hash ordering, which means the hash implementation can be changed without breaking (correctly written) code. Perl 5.8.1, back in September 2003, introduced hash randomization to thwart this attack, and various other countermeasures have been added to its hash implementation since then.\n\nRuby 2.8.7 got hash randomization in 2008. PHP 5.3.9, Python 2.6.8 and Python 3.3 only got it in 2012!\n\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode\n\nStrange, hash randomization is mentioned in the [5.8.1 perldelta](https://perldoc.pl/perl581delta#Hash-Randomisation), but the breaking change actually happened in [5.18](https://perldoc.pl/perl5180delta#Hash-randomization) in 2013. I can't determine what actually changed in 5.8.1.\n\nThe change in 5.18 was the introduction of \"Hash Traversal Randomization\" (and the `PERL_PERTURB_KEYS` environment variable), which randomizes each hash's order independently. I think the order can also change more dramatically as entries get added and removed.\n\nThe initial implementation back in 5.8.1 (with `PERL_HASH_SEED`) was a simple global random seed for all hashes.\n\nSee `perlsec` for details.", "id": "ffd4izv", "owner_tier": 0.7, "score": 0.999999999090909}, {"content": "Many years ago, O'Reilly sent me a free copy of the *Python Cookbook* along with a note thanking me for my contribution. Puzzled, I dug a bit further and found out it was because I had pointed out the derivation of the DSU sort on mailing list and that information had now been included in the book.\n\nOf course, it's originally a Lisp trick (which is how Randal knew it).", "id": "ffer36x", "owner_tier": 0.5, "score": -9.090909035659355e-10}, {"content": "Probably how List::UtilsBy::sort_by works!\n\nIn fact it is!", "id": "ffera1b", "owner_tier": 0.7, "score": 0.09090909000000001}], "link": "https://www.reddit.com/r/perl/comments/esyvc5/giving_credit_where_credit_is_due/", "question": {"content": "From the Python Wiki [https://wiki.python.org/moin/HowTo/Sorting#Sorting\\_Basics](https://wiki.python.org/moin/HowTo/Sorting#Sorting_Basics), note the last line ...  \n\n\n## The Old Way Using Decorate-Sort-Undecorate\n\nThis idiom is called Decorate-Sort-Undecorate after its three steps: \n\n* First, the initial list is decorated with new values that control the sort order. \n* Second, the decorated list is sorted. \n* Finally, the decorations are removed, creating a list that contains only the initial values in the new order.   \n...  \n\n\nAnother name for this idiom is [Schwartzian transform](http://en.wikipedia.org/wiki/Schwartzian_transform), after Randal L. Schwartz, who popularized it among Perl programmers.", "id": "esyvc5", "title": "Giving credit where credit is due.", "traffic_rate": 2.8680186170212765}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "    smaller = filter (\\(_, x, _) (_, y, _) -> x < y) xs\n\nchecks the value of the second element of a three-tuple.\n\nThe title asks a different question though:\n\n    setEl2of3tuple (x, y, z) p = (x, p, z)", "id": "gtwmok0", "owner_tier": 0.3, "score": 0.9999999966666667}], "link": "https://www.reddit.com/r/haskell/comments/mn5dmj/how_to_change_a_specific_element_in_a_tuple/", "question": {"content": " \n\nso, I have a list of trios (\\[Char\\], int, int), and I want to sort it from smallest to largest, but according to the second item of each trio. So far I have this:\n\nsort :: \\[(\\[Char\\], Int, Int)\\] -> \\[(\\[Char\\], Int, Int)\\]\n\nsort \\[\\] = \\[\\]\n\nsort (x:xs) = sort smaller ++ \\[x\\] ++ sort larger\n\nwhere\n\nsmaller = filter (<= x) xs\n\nlarger = filter (> x) xs\n\nBut this function sorts the list according to the first item, and I don't know how to change it?\n\n this type of tuple operation is being very difficult for me, I\u2019ve tried it in many ways and I can\u2019t get an answer", "id": "mn5dmj", "title": "how to change a specific element in a tuple", "traffic_rate": 13.1328125}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "The three new sorting algorithms are for sorting lists of size 3, 4, and 5, respectively. This is interesting, but if you're reading this thinking that AI techniques have found replacements for Merge Sort or Introsort or something like that, then you're going to be disappointed.\n\nYes, but they are primitives used to implement those larger sort algorithms. \n\n>The sort 3, sort 4 and sort 5 algorithms in the LLVM libc++ standard sorting library are called many times by larger sorting algorithms and are therefore fundamental components of the library. \n\n>We reverse engineered the low-level assembly sorting algorithms discovered by AlphaDev for sort 3, sort 4 and sort 5 to C++ and discovered that our sort implementations led to improvements of up to 70% for sequences of a length of five and roughly 1.7% for sequences exceeding 250,000 elements.\n\n>These algorithms were sent for review and have officially been included in the libc++ standard sorting library3. It is the first change to these sub-routines in over a decade\n\nYeah but I mean when you have lists of size 3,4 or 5, \n\nI\u2019ll see you in the dust loser\n\n>Introsort \n\nTIL about [introsort](https://en.wikipedia.org/wiki/Introsort), thanks!\n\n1.7% sounds huge! Btw, isn't 4, 5 elements a small enough list that people should have already brute forced all the combinations of comparisons and sequences and just know the optimal steps from that?\n\n> Yes, but they are primitives used to implement those larger sort algorithms.\n\nThey are. And this makes them significantly faster, but it does not make them conceptually different. I suspect that a random reader of the post title will think the latter is true.\n\n> Yes, but they are primitives used to implement those larger sort algorithms. \n\nOnly if your input is a primitive.\n\nFor all other cases they're disabled by templates.\n\nGlobal minimization techniques (GAs, Boltzmann machines) have been applied to finding best-possible small sorts since the '80s. This just changes the optimization method and the cost function.\n\n[deleted]\n\n> that people should have already brute forced all the combinations of comparisons and sequences and just know the optimal steps from that?\n\nThe orders of elements are easy to brute force, but the number of operations and the orders in which you can call them is very large. What they're doing isn't just optimizing the order of comparisons and moves to minimize each, but also optimizing the order of the different calls inside each of those to reduce those as well as combining sorting functions to reduce the cost of sorting larger sets.\n\nThey show some of the examples they found in their paper, so it's worth a read. A couple examples though:\n\n* They were able to remove a move operation from sort3 by changing the order in which they compared/swapped things. \n* They were able to optimize the results of some variable sort functions (functions that sort a variable number of elements 0-N) by using different fixed sized sort functions and using those as inputs into optimized versions of larger fixed sized sort functions.\n\nYes. There are optimal sorting networks at those sizes that are proven to be optimal", "id": "jn9ur0b", "owner_tier": 0.7, "score": 0.9999999999970336}, {"content": "The paper represents algorithms as CPU instructions in assembly. But then they are submitted to LLVM as C++ code which is then translated back to machine code. I'm curious whether that machine code is as efficient as the original CPU instructions. Perhaps there are a few more percent to be gained by generating exactly the right assembly for each CPU make and model. In general, I wonder how much power there is in our computers that will be unlocked by AI-guided code optimization.\n\nThere was a cool paper I read a while back where a team used evolutionary techniques to program FPGAs to, IIRC, determine the frequency of an analog audio input tone. Then they examined the resulting programs and found a very unexpected result. Some of the programs contained what should have been no-ops - logic blocks not connected to input or output. But when they removed the no-ops, the program's performance was degraded! The programs had evolved to solve the problem efficiently *on those individual chips*, taking advantage of tiny flaws and idiosyncrasies - logic block A leaks a tiny bit of current to logic block B on this chip, that kind of thing.\n\n[deleted]\n\nThe benefit depends on how different the target system is. Those improvements assume the architecture and relative cost of operations stays the same, which is not guaranteed in the coming decades.\n\nI worked on a system where large matrix dot product was a trivial operation, but abs() of a float was a massive performance hit. I also worked on a system where 2d Fourier transformation was near instant, but adding two integers was a challenge to be avoided.\n\nWe are likely going to need efficient sorting algorithms for those platforms in the coming decades, but there is little research in the area.\n\n[deleted]\n\nI haven't found the specific patches yet, but the code in their repo here ~~would suggest that that they're just using the assembly verbatim~~ https://github.com/deepmind/alphadev/blob/main/sort_functions_test.cc\n\nEdit: nevermind, found the patch https://reviews.llvm.org/D118029\n\nIn general, \"AI code optimization for arbitrary code\" is an extremely challenging topic. Currently, I doubt there's really even an idea about how someone would even approach designing that system.\n\nFor the algorithmic improvements that Google has discovered recently (matrix inversion, and now array sorting) they have used a variation of the same Monte Carlo Tree Search algorithm that they've been using for something like 15 years. In *extreme* brevity, this algorithm requires that the designers clearly and exhaustively define a model of the problem that the AI is meant to solve. Then, the AI learns to solve that *exact* problem. It may or may not result in some new emergent solution - but extrapolating what an AI learns from one problem to another is still an extremely undeveloped field of active research.\n\nThis is all to say, adapting AlphaZero to a clearly defined and heavily studied and deeply important abstract math problem such as sorting an array or inverting a matrix makes sense as a use case for this type of AI. Optimizing arbitrary code is still *waaay* off in the future - the possibility space of a problem that vague is so *insanely* large.\n\nSame thing happens when I remove logging statements in my code. \ud83d\ude02\n\nhttps://www.damninteresting.com/on-the-origin-of-circuits/\n\nThat sounds incredibly interesting, do you have a link to that paper by chance?\n\nYou are asking the wrong question.\n\n50 years or more ago they started asking whether AI could do hand writing recognition as well as humans. They would spend tens of thousands of dollars to show that it could recognize a few thousand digits.\n\n\u201cWhat a waste! A human could have done that in a single hour.\u201d\n\nYeah, and since then they have replaced tens of thousands of human letter sorters with computers.\n\nThis project was not designed to find a new sorting algorithm. That was a side effect. The project was intended to discover if algorithms could discover useful new algorithms.\n\nThe answer was yes. Which implies that they are just getting started.\n\nThe software they built is not called \u201calphasortalgorithm.\u201d\n\nIt\u2019s called AlphaDev. It is a general purpose engine for searching for new algorithms. And it is the VAX/VMS of such engines. Wait until we get to the M2 MacBook version.", "id": "jn9xreb", "owner_tier": 0.9, "score": 0.21655295164342925}, {"content": "Interesting and novel work. I don't see much application beyond primitives tho. \n\nMy reason for stating this is it invokes the comparitor function more often then the previous version of these algorithms [citation](https://reviews.llvm.org/D118029#3359924). Given the trivial cost of machine-primitive (float, int, etc.) comparisons this is a reasonable trade-off. Nevertheless when your comparitor function is operating on higher level objects/structs... this trade-off maybe less than desirable.\n\nThis fact is backed up in the review comments where they in no uncertain terms state the complex predicates are slower in these algorithms -> [cite-1](https://reviews.llvm.org/D118029#3344301) & [cite-2](https://reviews.llvm.org/D118029#3359924). These algorithsm are (only) invoked behind template meta-programming specialization to ensure they're not ran on non-primitives. \n\n---\n\nI believe the paper hyping this as having massive impacts **THE WORLD OVER** on sorting. As often programmers aren't just sorting lists of integers/floats. \n\nHopefully this approach can be adapted and applied to other difficult problems. Ensuring it could handle SIMD in the future maybe really interesting, specially for stuff like matrix multiplication & FFTs, where just handling raw ints & floats is really all you need.\n\n>Interesting and novel work. I don't see much application beyond primitives tho.\n\nThe work wasn't to find a sorting algorithm. And it also didn't just find a useful sorting algorithm. It also found a hashing algorithm.\n\nThe work was to automate the search for better algorithms.\n\nOne could presumably change the inputs to the meta-algorithm and say \"find me a better algorithm for string sorts\" and it might be able to do that too. If not today, then after the next paper. Or the paper after that.\n\nJudging it based on the breadth of applicability of these particular algorithms is like criticizing a handwriting recognition experiment that only handled postal codes and not also phone numbers. The search for algorithms is the point, not the details of the specific problem statement they offered to the \"search engine\" to show that it works.\n\nAgreed. I hate that we still benchmark algorithms based on primitive types. In part because of the overlap between \u201cworking with primitives\u201d and \u201conly need a partial sort\u201d.\n\nBut what\u2019s been a shadow in my mind for decades now, came to verbalization a good while ago, getting louder every day, is that O(1) access time is a dangerous myth that\u2019s slowing progress of the industry.\n\nPrimitives get closest to maintaining this fiction, so they aren\u2019t simply best case examples, they\u2019re actively harmful to the dialogs we need to be having post-Dennard. There are no saviors coming to let us continue our fictions a little longer.\n\nSerious question: In the scenario of sorting search results, it actually would just be sorting floats, right? Or whatever type their ranking values have. So in this case it would be applicable?\n\n> I believe the paper hyping this as having massive impacts THE WORLD OVER on sorting. As often programmers aren't just sorting lists of integers/floats. \n\niirc aren't there hard limits on sorting?  And we're pretty close to them I thought.\n\nIs schwartzian transform addressed as an approach to this issue?\n\na *lot* of work goes into small, crazy compiler optimizations. things like optimizing the speed of sorting very small lists are more important than you might think. this sort of thing could potentially be used when the compiler is able to infer that a loop should be unrolled into individual statements, for example.\n\nDitto. It seems a lot of people misunderstood the primary purpose of the paper/AlphaDev. Once we have a model, we can chain different ML models together and they can do general or special algorithm optimization.\n\nthe \"hashing algorithm\" is kind of.. not impressive? it looks like they just took the simplest thing that didn't obviously fail any tests and threw it in absl so they could point to it as \"hey look it's doing real work!\"\n\nhttps://news.ycombinator.com/item?id=36229099\n\n> Distribution on real workloads was good, it **almost passed smhasher** and we decided it was good enough to try out in prod. We did not rollback as you can see from abseil :)\n\nyeah the \"hashing algorithm\" definitely doesn't look like it was taken seriously by anyone other than pr\n\nFYI I editted out the part of memory obliviousness. \n\nAfter re-reading the paper I lowered my expectations. This approach doesn't generalize to higher abstraction optimization problems like that. \n\nAt best it'd able to understand SIMD, so maybe faster FFT or matrix multiplication algorithms, _hopefully_.", "id": "jna0gx1", "owner_tier": 0.9, "score": 0.10145357460397508}, {"content": "Great, I can\u2019t wait to be asked to implement it in interviews", "id": "jna3en2", "owner_tier": 0.9, "score": 0.04805695638979531}, {"content": "I have a genuine question: I remeber many *many* years ago from my algorithm analysis lecture that mathematically sort algorithms cannot get better than O(n.logn). What am I missing?\n\nThis is still true, a comparison-based sort will always need at least `n log n` comparisons to sort any list. But the actual implementation in code can still be faster or slower, especially when we consider how complex modern CPUs are with branch prediction and etc.\n\nO(n.logn) is the best possible *asymptotic* complexity of sorting in the *general case*.\n\nSo, 5n.logn and 2n.logn are both O(n.logn), but the latter one is clearly faster.\n\nAdditionally, if you have additional constraints on the data you can apply faster algorithms that are tailore for the specific case. For example, sorting n keys of length w lexicographically can be done in O(w.n) with radix sort. Which is faster than O(n.logn) for w < logn\n\nThey can - see radix sort\n\nIf you compare elements, then asymptotically it can't be better, but there are linear sorts like counting or radix, and you can also have different constants.\n\nWhen you say O(whatever), you're describing how the speed *changes* as the size of the list changes, for sufficiently large lists.\n\nSo if going from sorting 100 elements to 200 elements takes 4 times as long, it's O(n^2).\n\nSo let's say sorting 100 elements takes 1 second, and 200 takes 4 seconds. I come along and I present my new sorting algorithm. It takes 0.01 seconds to sort 100 elements, and 0.04 seconds to sort 200 elements. Still grows by the square of the number of inputs, so it's still O(n^2). But mine is 100x faster.\n\nSaying something is O(n log n) is a useful shortcut to say \"it's probably pretty alright\", but it's just one metric that doesn't come close to fully describing the speed of an algorithm.\n\nthese are sort3 sort4 and sort5\n\nyes..\n\n 3 elements, 4 elements and 5.   thats it.  the ability to do these well is not O(n) notation based.\n\nYou can get O(1) performance with the intelligent design sort: the list has already been perfectly sorted by a divine Sorter according to criteria beyond our mortal comprehension, and reordering the list would be an affront against nature.\n\nIt's worth adding that asymptotic complexity tells you what happens when n tends to infinite, it's meaningless for very low numbers. For example if the exact number of operations is given by the function *5+n\\^2*  this means it's assymtotically worse than *10+10\\*n* but it isn't worse for all n, for n=<10 it's still better.\n\nThis isn\u2019t breaking any Big O barrier, it\u2019s squeezing out better real-world performance through low-level optimization of small-n subproblems.\n\nU can still optimise for constant factor. nlgn might be 1000000nlgn, decreasing that large number would yield speed ups", "id": "jnahj67", "owner_tier": 0.3, "score": 0.07030554731237021}, {"content": "More recently, Google has also merged [Bitsetsort](https://github.com/minjaehwang/bitsetsort) to [LLVM's std::sort() implementation](https://github.com/llvm/llvm-project/commit/4eddbf9f10a6d1881c93d84f4363d6d881daf848), which actually has a more significant performance improvement in general cases (but was developed by hand on top of other optimized sorts, not with machine learning).\n\nOne of the reasons it's so fast is that parts of it are carefully written so LLVM generates SIMD instructions.\n\nThis is one of the things I found most interesting. Cause it's trying to optimize against the real hardware. Current hardware is so complex we may be missing some tricks.", "id": "jnb36nz", "owner_tier": 0.5, "score": 0.008602788487095817}, {"content": "This seems minor but hyped to be huge.\n\nReddit is big and pretty stupid. I see here only 3-4 competent commentators besides me. This cannot be avoided with their approach to communities and commenters.\n\nI don't understand why you are blaming Google here. They have nothing to do with it.", "id": "jnb3ot1", "owner_tier": 0.3, "score": 0.001483239391871848}, {"content": "Now they can sort ads into my search results even faster", "id": "jncympi", "owner_tier": 0.5, "score": 0.0011865915129041827}, {"content": "Coolest research paper I've read this year or possibly ever.", "id": "jnaij8r", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "Applying deep learning to fundamental algorithms is so fucking cool. Deepmind also found a more efficient matrix multiplication algorithm last year, which is obviously HUGE for machine learning.", "id": "jnc4rwn", "owner_tier": 0.1, "score": 0.0008899436339365173}, {"content": "Could someone explain the psuedocode for the original sort3? I am having a hard time following, the way I read it the original value B can never be placed in Memory\\[2\\], and as such will not correctly sort when B is the largest value. I must be misinterpreting something and would really appreciate some clarification, so I can then try and understand the AlphaDev solution.\n\n&#x200B;\n\nPosted a question about this on reddit [here](https://www.reddit.com/r/compsci/comments/143wh6u/understanding_alpha_dev_sorting_algorithm/).\n\nHave the same question, it seems like they are focusing on a specific set of comparisons and leaving off the step comparing B and C, since presumably AlphaDev uses the same approach for this portion.\n\nAlso if you compare the assembly (b) and (c), then you see that they only removed an instruction, nothing else changed except for the comments.", "id": "jncl69d", "owner_tier": 0.1, "score": 0.001483239391871848}, {"content": "Now if only Google would make Google search great again ...\n\nThe whole AI transition comes with a massive reduction in service quality. It has been hugely disappointing so far. Quality goes down, but tons of promo articles babble about the epic future. It's bizarre.", "id": "jnd43nd", "owner_tier": 0.7, "score": 0.000593295754968852}, {"content": "Over a year ago, committed 8.April 2022", "id": "jnddsm1", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "I\u2019m planning to apply it to sort billions of rows using only 32GB of memory. Previously I had completed algorithm for billion-row Distinct, GroupBy, Filter and JoinTable. Sorting is the last one I cannot solve at the moment.", "id": "jne55rc", "owner_tier": 0.1, "score": 0.000593295754968852}, {"content": "https://news.ycombinator.com/item?id=36231147 debunks the hype here.", "id": "jned149", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "Imagine spending time optimizing the sorting of lists sized n < 10 in 2023.\n\nThis is coming from a recovering micro-optimization fiend.", "id": "jnd6fsi", "owner_tier": 0.3, "score": 0.00029664787600118655}, {"content": "And so it begins. AI is now enhancing portions of itself.", "id": "jnckj5u", "owner_tier": 0.9, "score": -2.9664787896766537e-12}, {"content": ">It is the first change to these sub-routines in over a decade.\n\nThis shouldn't say much, since C++'s std is still missing a half-century (or century-old) algorithm.", "id": "jndmudh", "owner_tier": 0.1, "score": 0.00029664787600118655}], "link": "https://www.reddit.com/r/programming/comments/143gskm/google_finds_faster_sorting_algorithm_using_deep/", "question": {"content": "", "id": "143gskm", "title": "Google finds faster sorting algorithm using deep reinforcement learning", "traffic_rate": 935.2067938021454}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "Got to be hash slices.\n\n    my @cols = qw[col1 col1 col3];\n\n    my @data;\n    while (<$some_file_handle>) {\n      chomp;\n      my %row;\n      @row{@cols} = split /\\t/;\n      push @data, \\%row;\n    }\n\nI use this all the time. And it's always a big \"wow\" moment when I introduce it on my intermediate level course.\n\nWould \n\n@rows = ('a','b','c');\n\n@hash{@rows} = (1, 2, 3);\n\nMake\n\n$hash{a} == 1\n\n\n?\n\nWorks especially well IMO for stuff like CSV with header rows.\n\n    #!/usr/bin/env perl\n\n    use strict;\n    use warnings;\n \n    use Data::Dumper;\n\n    #read header row. \n    chomp ( my @header = split /,/, <DATA> ); \n\n    my @rows; \n \n    #iterate other rows\n    while ( <DATA> ) {  \n       chomp;\n       my %row;\n       #use hash slice to insert into named values. \n       @row{@header} = split /,/; \n       #push this row into the larger data structure. \n       push @rows, \\%row; \n    }\n\n    print Dumper \\@rows; \n\n    __DATA__\n    field1,field2,field3\n    value,anothervalue,someothervalues\n    fish,bird,carrot\n\n\n\nI'm assuming the opened file is delimited by tabs? \n\nExactly.\n\nOf course, the usual disclaimer applies; if you hand-roll your CSV parsing, one day you'll choke on something you didn't expect, e.g. quoted values or multi-line rows - for instance:\n\n    foo,\"Hi, this is one field\",42\n\nIf you use Text::CSV, (or Text::CSV::XS) then it's a battle-tested solution that won't fall over little gotchas like that, like your code would have.  (If you control the generation as well as the consumer of the CSV then you can be more confident hand-rolling if you really need to, but even then I'd still tend to just use Text::CSV personally.)\n\nIt can also handle the \"give me the data as a hashref\" cleanly for you, e.g. :\n\n    open my $fh, \"<\", $filename or die \"Failed to open $filename - $!\";\n    my $csv = Text::CSV->new;\n    $csv->column_names($csv->getline($fh));\n    while (my $row = $csv->getline_hr($fh)) {\n        # $row is a hashref whose keys are the column names\n    }\n\n\nIn this example, yes. But the concept is easy to use for other file types.\n\nwell that is a massive time saver, thanks for that gem\n\nGranted. But most of the time, I find Text::CSV to be altogether too heavyweight for _most_ usage scenarios - sure, when you're handling quotes and whatnot it's absolutely my go-to. (Although sometimes that's `Text::Parsewords`).\n\nBut _most_ times CSV is not that complicated. \n\nSo this tool is used to create 2D indexing of text files or CSV  files? ", "id": "dnnxv46", "owner_tier": 0.5, "score": 0.9782608693478261}, {"content": "Using `my` in a for^list loop doesn't actually declare a new variable - it just renames `$_` - which means if you alter it you actually alter the value in the list itself.\n\nTry this:\n\n    use strict;\n\n    my @cars = qw( BMW Audi Volvo );\n    for my $car ( @cars ) {\n        $car .= \" drivers suck\";\n    }\n    print( join( \", \", @cars ) );\n\nThis outputs:\n\n    BMW drivers suck, Audi drivers suck, Volvo drivers suck\n\nWhich is not what you'd expect. This bit me several times until I learned about this quirk.\n\nThis has nothing to do with `$_`. This is because `for/foreach` aliases each value, rather than copying it, like subroutines alias their arguments to the elements of `@_`.\n\nIn fact, this is one reason why using `$_` in a for loop can be dangerous even though it localizes the variable; if you call code inside your for loop that modifies `$_` for its own purposes, that will modify whatever the for loop is iterating through!\n\nI've been using Perl for 3 years and you opened my sir!) Thank you.\n\nSorry, I can't see why that is surprising. That's what I thought would happen.\n\nOne of my favourite uses of this is trimming leading and trailing whitespace:\n\n    my @foo = ('     foo  ', ' bar    ');\n    s/^ +//, s/ +$// for @foo;\n\nYeah, 'my' is scope. Same as 'our'.\n\nWhat does the following code do?\n\n    use strict;\n\n    my @cars = qw( BMW Audi Volvo );\n    for ( my $i = 0; $i < scalar @cars; $i++ ) {\n        my $car = $cars[ $i ];\n        $car .= \" drivers rule\";\n    }\n    print( join( \", \", @cars ) );\n\n\nIt's very useful in general for quickly making the same modification to every element of an array.\n\n    my @foo = ('     foo  ', ' bar    ');\n    $_ = join(' ', split(' ', $_)) for @foo;\n\nAlthough it's kind of convoluted, this can be a faster way if you can tolerate (or actually want to) knock all whitespace down to a single space. The ' ' argument to split is a special case condition which automatically trims and flattens whitespace.\n\n    use strict; \n    my @cars = qw( BMW Audi Volvo ); \n     for ( my $i = 0; $i < scalar @cars; $i++ ) { \n             my $car = $cars[ $i ]; \n             $car .= \" drivers rule\"; \n     } \n     print( join( \", \", @cars ) );\n\n\nbut that is completely different. Without trying it I would say that it prints out\n\n     \"BMW, Audi, Volvo\"\n\nNow, I tried it, and that's what it prints. There is no aliasing of the member of @cars, it is copied out in a sub-scope, and nothing changes at the outer scope.\n\n\n\n\nYet in both cases you use `my` to declare a new variable and you only alter this new variable.\n\nBut in the first case even altering this new variable actually changes the list.", "id": "dnnwesd", "owner_tier": 0.5, "score": 0.9999999997826087}, {"content": "I find `//` to be particularly useful. It's not comment, it's a 'defined' test, that works like `||`. \n\nSo you can do:\n\n     print $value // 'default',\"\\n\";\n\nAnd if `$value` is undefined, then you get a default. But _unlike_ `||` it handles empty string and zero differently (which are boolean false, but defined). \n\nYou can also use it for setting defaults with `//=`:\n\n    my $thing //= 'default value here'; \n\n", "id": "dno693s", "owner_tier": 0.9, "score": 0.23913043456521738}, {"content": "I've learned some of the coolest Perl stuff by reading the \"perldelta\" manpages. They contain all the new features, and a list of things that will be obsoleted in the new version of Perl.", "id": "dnnxtww", "owner_tier": 0.7, "score": 0.15217391282608697}, {"content": "I will second /u/davorg about Hash slicing. Particulalry nice when you have a function that takes a lot of arguments, so you use named args... but you also use them a lot so you want to assign them to scalars up front.\n\nThis is a trivially short example of a higher-order function, but you get the idea\n\n    sub process( %args ) {\n        my ( $data, $pred, $proc ) = @args{qw( data pred proc )};\n        return map { $proc->($_) } grep { $pred->($_) } $data->@*\n    }\n\nI'm also loving using signatures and post-deref as you can see.\n\nBut for more classic fave tip, I can't go past `split(' ')`. The `split` function typically takes a regex. Even if you give it a \"string\" it's treated as a regex pattern.\n\nThe exception to this is the ~~single-quoted~~ single space `' '`. This is special syntax that splits a string by any arbitrary whitespace, ingnoring leading and trailing whitespace.\n\n    my $line = \"    oddly    spaced     words  \";\n\n    my @words = split( ' ', $line );\n    # @words = ('oddly', 'spaced', 'words')\n\nI do a lot of text processing and I use this almost every time.\n\nNote that `/ /` does not do the same thing, only `' '`.  \n\nAlso, a word of warning... If you like your regex patterns with insignificant whitespace so much you `use re '/x'`, then this trick doesn't work. The most obvious solution is to do it inside a function that lexically disables  the `re` pragma\n\n    use re '/x';\n\n    sub words( $str, $n = 0 ) {\n        no re;\n        return split( ' ', $str, $n );\n    }\n\n    my $line = \"    oddly    spaced     words  \";\n\n    $line =~ / s p a c e d / && say join( ', ', words($line) );\n\nThe `$n` retains the ability to control the number of times to split.\n\nEdit: Updated to reflect whitespace split works with `split(\" \")`\n\nAgreed on these, just one very minor point: \" \" or a scalar variable containing a single space will have the same effect - i.e.\n\n    perl -e'split \" \"'\n\nworks the same way as your split ' ' example.\n\nUnlike Python, we don't need a (named) function to make a lexical scope. Any block will do.\n\n    $line =~ / s p a c e d / && do {\n        no re;\n        say join ', ', split ' ', $line;\n    }\n\n    if ($line =~ / s p a c e d /) {\n        no re;\n        say join ', ', split ' ', $line;\n    }\n\n\nAhh, you are correct, sir. I could have sworn that double-quoted string behaved differently. Damn false memories.\n\nTrue, but I've used my `words()` sub about 8 times in my current project. I don't wanna repeat myself, and creating lexical block everywhere is not always convenient. For example, I'm using inside a conditional which makes for some nicely readable code.\n\n    if ( words($input) == 1 ) { ... }\n", "id": "dnnzrdw", "owner_tier": 0.3, "score": 0.17391304326086957}, {"content": "dispatch tables \n\n    my $dtable = { \n        something         => \\&BLA::Today::do_something,\n        something_else => \\&BLA::Today::do_something_else,\n    };\n\nsymbol table manipulation : read the source for Class::Data::Inheritable,  saw the light ", "id": "dnofair", "owner_tier": 0.1, "score": 0.10869565195652174}, {"content": "`map` is an awesome function, and you can do a lot of cool things with it. \n\nA pretty standard:\n\n\n    my @things = qw ( fish carrot banana potato );\n    my %is_thing = map { $_ => 1 } @things; \n\n    print \"'fish' is a thing\\n\" if $is_thing{'fish'};\n\nYou can select 'paired' values straight into a hash. \n\nSo given some text like:\n\n    test=woilla\n    test2=gronk\n    fleeg=flirble\n\nYou can hashify this:\n\n    my %value_of = $str =~ m/(\\w+)=(\\w+)/g; \n\n\n\nBut why not just use a for loop and make it easier for whomever is coming behind you to read. I have been writing perl since perl v2 and I can't find a really good reason to use map other than to be perish. I love perl, probably too much, but I hate perl code that is hard to read just because someone wanted to be perlish. I am sure that I have never done that \ud83d\ude1c ... Err not that I would admit to in Reddit \n\nUsing map for this *is* the easy, idiomatic way. \n\nIt's perlish? This **is** Perl. That's like complaining something written in Java is javaish.\n\nIMO, I think that you say that because you're more use to for loops. I prefer map personally.\n\nI didn't grok `map` until I realised it's a list comprehension in the functional programming sense. \n\nBeing able to manipulate whole lists is surprisingly useful. \n\nYes it is idiomatic and there are a lot of Perl idioms that I do like; however, I do not believe in sacrificing readability solely for the sake of being perlish. ", "id": "dno5x4w", "owner_tier": 0.9, "score": 0.17391304326086957}, {"content": "My tip is use [Mojo::DOM](https://metacpan.org/pod/Mojo::DOM) for HTML parsing.\n\nIn the past, I used to use `HTML::TokeParser` then `HTML::TokeParser::Simple` and people recommended all sorts of XLTS and Tree parsers and they all were awkward to use.\n\nSo as far as blowing minds goes, `Mojo::DOM` did that when I first found it. I already knew CSS well at the time, so perhaps that helped the ease of use, but compared to `HTML::TokeParser`, `Mojo::DOM` plays in a whole 'nother league.\n\n-----\n\nBut if you meant core-Perl tip, then I'd say it's that the `/.../` in list context returns the captures and `/.../g` returns the matched substrings if there are no captures in the regex:\n\n    say join \", \", \"foo bar ber\" =~ /\\w+ (\\w+) (\\w+)/; # OUTPUT: bar, ber\n    say join \", \", \"foo bar ber\" =~ /\\w+/g;            # OUTPUT: foo, bar, ber\nNice and concise and no `$1, $2` to deal with.\n    \n\ndo you know what the logic is for /g having that functionality, as opposed to, say, /r ?\n\nOtherwise, wow, TIL. \n\nI ask because /g with capture groups 'carries on going for the whole string'\n\n     my $x = \"cat dog house\";\n     while ($x =~ /(\\w+)/g) {\n\t \tprint \"Word is $1, ends at position \", pos $x, \"\\n\";\n\t }\n\nprints\n\n    Word is cat, ends at position 3\n    Word is dog, ends at position 7\n    Word is house, ends at position 13\n\nand /r is the \"return the result of the match\". \n\n> do you know what the logic is for /g having that functionality, as opposed to, say, /r ?\n\nNope, don't know. I don't think `/r` existed that far back yet.\n\n`/r` applies to `s///`, not `m//`, which has different return values to begin with. There's no `/r` modifier for `m//`.\n\nYour example is running `m//g` in scalar context (whereas Zoffix's example is in list context), which changes both its behavior and return value, something that can be surprising to newcomers but was probably intended to be \"intuitive\".\n\nWell, one thing is completely clear: you *definitely* know what you're talking about. ", "id": "dnog2s7", "owner_tier": 0.3, "score": 0.13043478239130435}, {"content": "The [Schwartzian transform](https://en.wikipedia.org/wiki/Schwartzian_transform). I don't often have a need for it, but it's a very cool way to sort an array by an additional attribute. There's no need for a secondary data structure that keeps using memory after the sort is done, and the attribute is only found/computed once for each element. \n", "id": "dnoi9cn", "owner_tier": 0.7, "score": 0.06521739108695652}, {"content": "    eval { may_die(); 1 } or print \"boom\";\n\nInstead of\n\n    eval { may_die() };\n    print \"boom\" if $@;\n\n\nTo give some more context, this is an important construct for anyone using plain eval instead of Try::Tiny or Syntax::Keyword::Try. Never rely on the value of `$@` to determine if an error occurred; but if an error occurred, eval will always return undef, and you can rely on that.", "id": "dnpe04u", "owner_tier": 0.1, "score": 0.08695652152173913}, {"content": "`say $x` is prettier than `print $x . \"\\n\"`\n\nEmbrace post-dereferencing.\n\n    $a = { a => 1 } ;\n    $b = $a ;\n    $b->{ a } = 2 ;\n    say $a->{a} ; # 2, because references\n    my $c = { $a->%* } ;\n    $c->{ a } = 4 ;\n    say $a->{a} ; # 2, because values\n    say $c->{a} ; # 4, because values\n\nPlus, the Schwartzian Transform is a bit mindbending, but once you understand it, it's **very** powerful.\n\nSurely you'd never type `print $x . \"\\n\"` rather than `print \"$x\\n\"`?", "id": "dno7sac", "owner_tier": 0.7, "score": 0.08695652152173913}, {"content": "Not trick per se, but the first time I ran into lexical post-if:\n\n    my $var = some_stuff() if whatever_expression();\n\nIf you don't know it: it's a horrible bug waiting to happen, because the lexical is only created if the condition is true, but in the scope of the surrounding code, because post-if doesn't create its own scope. So if you have a variable with the same name further up the calling chain, that value will end up being used.\n\nMy tip: Use a linter or a PPI based test to catch them in your code.\n\nPeople deliberately use this to give them persistent variables in subroutines;\n\n    #!/usr/bin/perl\n\n    use strict;\n    use warnings;\n\n    sub foo {\n      my $foo if 0;\n\n      return ++$foo;\n    }\n\n    print foo(), \"\\n\" for 1 .. 4;\n\nThis now gives a deprecation warning and can be replaced by `state`.\n\n    #!/usr/bin/perl\n\n    use strict;\n    use warnings;\n    use feature 'state';\n\n    sub foo {\n      state $foo;\n\n      return ++$foo;\n    }\n\n    print foo(), \"\\n\" for 1 .. 4;\n\n\n\n\nlike the acceptable : \n      my $var = 32 if 0;\n\n\nFWIW there is a [core perlcritic policy](https://metacpan.org/pod/Perl::Critic::Policy::Variables::ProhibitConditionalDeclarations) for this, and I've subclassed it into the [freenode theme](https://metacpan.org/pod/Perl::Critic::Policy::Freenode::ConditionalDeclarations).\n\nAlong with being [deprecated](https://metacpan.org/pod/perldeprecation#Using-my(\\)-in-false-conditional.) for a while now as you said, this functionality seems to have been accidental due to how `my` was implemented with both compile-time and run-time behavior. It's certainly not documented or tested anywhere.", "id": "dnogxpi", "owner_tier": 0.7, "score": 0.26086956499999997}, {"content": "Autovivification, which is such a cool Perl trick that [the wikipedia page](https://en.wikipedia.org/wiki/Autovivification) starts out \"In the Perl programming language...\"\n\nGiven a bunch of lines that have devices and genres (this was from an iTunes example that I just tried -- full example further down)...\n\n    my ($device, $genre) = split(/\\t/, $_); \n    $index->{$device}->{$genre}++; \n \nWe get so used to this, that it's easy to take for granted.  But in most languages you can't do this sort of thing so easily.\n\nYou can go as many levels deep as you want, and you can change up the grouping by just moving around where your variables are in the hash.  \n\nIt's very powerful and is a handy party trick (well okay, a work trick).\n\nHere's the full example, which assumes you have `curl` and `jq`:\n\n    curl -s \"https://itunes.apple.com/search?term=football&country=us&entity=software\"  | jq -r '.results[] | .genres as $genres | .supportedDevices as $devices | $genres[] | . as $genre | $devices[] | [.,$genre] | join(\"\\t\")' |  perl -MData::Dumper -lne '($device, $genre) = split(/\\t/); $index->{$device}->{$genre}++; print Dumper($index) if eof'\n\nAnd, if you don't have `jq` or prefer the pure-Perl version...\n\n    curl -s \"https://itunes.apple.com/search?term=football&country=us&entity=software\"  | perl -MJSON -MData::Dumper -lne 'push(@out, $_); if (eof) {  my $index = {}; my $data = from_json(join(\"\", @out)); for my $result(@{$data->{results}}) { my $genres = $result->{genres}; my $devices = $result->{supportedDevices}; for my $genre(@{$genres}) { for my $device(@{$devices}) { $index->{$device}->{$genre}++ } } } print Dumper($index) } '\n\n**Autovivification**\n\nIn the Perl programming language, autovivification is the automatic creation of new arrays and hashes as required every time an undefined value is dereferenced. Perl autovivification allows a programmer to refer to a structured variable, and arbitrary sub-elements of that structured variable, without expressly declaring the existence of the variable and its complete structure beforehand.\n\nIn contrast, other programming languages either: 1) require a programmer to expressly declare an entire variable structure before using or referring to any part of it; or 2) require a programmer to declare a part of a variable structure before referring to any part of it; or 3) create an assignment to a part of a variable before referring, assigning to or composing an expression that refers to any part of it.\n\nPerl autovivification can be contrasted against languages such as Python, PHP, Ruby, and many of the C style languages, where dereferencing null or undefined values is not generally permitted.\n\n***\n\n^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/perl/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot)   ^]\n^Downvote ^to ^remove ^| ^v0.27\n\nGood bot", "id": "dnp15uf", "owner_tier": 0.7, "score": 0.10869565195652174}, {"content": "In my case, and back when I was learning, the whole array slicing syntax deal was just a delight to learn and use.\n\n\nBut an actual tip would be using -e to check for file existence.\n\n\n\n    if -e './whatevs.txt' ...;\n\nAnd to be sure its a file\n\n-f", "id": "dnntcco", "owner_tier": 0.9, "score": 0.08695652152173913}, {"content": "What about hashref slices?\n\n    my $hr = { qw(one 1 two 2 three 3 four 4) };\n    my @odd = @{$hr}{qw(one three)};\n\nInterpolating *anything* into strings, not only scalars and arrays:\n\n    \"localtime @{[ join '-', localtime ]}\"\n    \"2 + 2 = @{[ 2 + 2 ]}\"\n\nWriting `sort` like functions with `$a` `$b`:\n\n    sub pairgrep(&@) {\n        my $func = shift;\n        my $caller = caller;\n        my @res;\n        while (@_) {\n            my $key = shift;\n            my $val = shift;\n            no strict 'refs';\n            local *{\"$caller\\::a\"} = \\$key;\n            local *{\"$caller\\::b\"} = \\$val;\n            push @res, $key, $val if $func->();\n        }\n        @res;\n    }\n\nAnd then\n\n    my %h = qw(one 1 two 2 three 3 four 4);\n    my %even = pairgrep { $a ~~ [qw(two four)] } %h;\n\nI know I know, it's core in  `List::Util` from Perl 5.20, but I badly wanted it in older versions.\n\nAnd something I learned yesterday. What happens with a hash in scalar context?\n\n    %h = qw(one 1 two 2 three 3 four 4);\n    print scalar %h, \"\\n\";\n    print %h . \"\\n\";\n    3/8\n    3/8\n\nWAT? Number of used/all hash buckets!\n\n> What about hashref slices?\n>\n>     my $hr = { qw(one 1 two 2 three 3 four 4) };\n>     my @odd = @{$hr}{qw(one three)};\n\nAnd now with postfix dereferencing:\n\n    my @odd = $hr->@{qw(one three)};\n\n\nHashes in scalar context now (since 5.26) returns the number of keys, since the hash bucket details are slower to calculate, implementation dependent, and really not relevant to the programmer. If you really needed the implementation details for some reason, you can use [Hash::Util](https://metacpan.org/pod/Hash::Util).\n\n> And now\n\nFrom Perl 5.20 upwards, with\n\n    use feature qw(postderef);\n\nand it's experimental up to 5.22.\n\nI see. So an unintuitive solution was chosen instead of the original unintuitive solution. I expected it to be the number of keys plus values, due to the analogy below:\n\n    %h = qw(one 1 two 2);\n    @a = %h;\n    $s = @a; #4\n\nfeature.pm isn't needed to enable it once it became no longer experimental (5.24+), since it doesn't conflict with existing syntax.\n\nAlso reasonable, but another perspective is that the logical \"size\" of a hash is the number of keys it stores. I am just glad it's something useful (and faster) now.\n\nagreed.", "id": "dno63dq", "owner_tier": 0.1, "score": 0.39130434760869565}, {"content": "references - before their existence, I used any number of old, lame methods to encode some form of data structure into a string, list or associative array. references have me freedom to go wild and shape the structure on the fly the way that I wanted. Wow!!!!\n\nWhile I am not a computer scientist and am ignorant in many if not most languages, I don't know of any language with similar capability.\n\n- edited to correct typos\n\nMost scripting languages have this capability, or the capability of multi-dimensional data structures, in some form; it would be very difficult to do anything complex without it.\n\nFun fact: since perl 4 didn't have references, it had a shortcut to generate fake multidimensional arrays, which still exists in perl 5 today, but you should probably never use it, and it looks confusingly like a hash slice: https://metacpan.org/pod/perldata#Multi-dimensional-array-emulation\n\nYep, used or in the olden days", "id": "dnoxjbf", "owner_tier": 0.1, "score": 0.06521739108695652}, {"content": "The `dbmopen` command which ties a hash to a local on-disk database. Years before sqlite, I believe, and doesn't require a module.\n\nIt's deprecated and there are much better ways to do it, but sooner or later you need some kind of simple, *ad hoc* database and `dbmopen` is the quickest way to do it.\n\nFWIW, this was superceded aaages ago by the core module [DB_File](https://metacpan.org/pod/DB_File). Another one for similar usecases is [DBM::Deep](https://metacpan.org/pod/DBM::Deep). But I tend to prefer SQLite anyway, though I might be [biased](https://metacpan.org/pod/Mojo::SQLite); it's generally going to be much more efficient in the long run if you're doing anything other than reading and writing the entire structure at once.\n\nI'm totally aware it's been superseded, but it's been superseded by more complex things. the dbm thing is instant, and there's practically no syntax to remember. You use one command, and now you have a hash which is saved to disk.", "id": "dnp2jqy", "owner_tier": 0.7, "score": 0.08695652152173913}, {"content": "What I forget again and again and am amazed when seeing in a code is using `values` where keys aren't important, especially as lvalues.\n\n\nE.g.\n\n    $_++ for values %count;\n\nor\n\n    say sum(map $hash{$_}{count}, keys %hash);\n    # versus\n    say sum(map $_->{count}, values %hash);\n", "id": "dnq2myd", "owner_tier": 0.1, "score": 0.021739130217391305}, {"content": "[P](https://stackoverflow.com/a/6163129/1039320)[e](http://world.std.com/~swmcd/steven/perl/pm/xs/intro/index.html)[r](http://www.masteringperl.org/2015/05/computing-excellent-numbers/)[l](http://www.linuxcareer.com/perl-as-a-career-option) [c](http://irclog.perlgeek.de/perl6/2013-10-17#i_7723873)[o](http://www.perl.com/pub/2005/07/14/bestpractices.html)[m](http://www.wall.org/~larry/pm.html)[m](http://interviews.slashdot.org/story/02/09/06/1343222/larry-wall-on-perl-religion-and)[u](http://www.perlmonks.org/bare/?node=Erudil)[n](http://rjbs.manxome.org/rubric/entry/1959)[i](http://blogs.perl.org/users/damian_conway/2012/03/why-i-love-my-job.html)[t](http://perlbuzz.com/2010/11/29/think_for_perls_sake/)[y](http://www.perlmonks.org/?node_id=1180698)...\n\n^ generated via this script:\n\n    #!/usr/bin/perl\n    use strict;\n    use warnings;\n    \n    chomp( my @links = reverse <DATA> );\n    \n    print map /\\s+/    # don't turn whitespace into a link\n      ? $_\n      : \"[$_](\" . pop(@links) . \")\",\n      split //, 'Perl community';\n    \n    print \"...\\n\\n^ generated via this script:\\n\\n\";\n    open my $script, \"<\", $0 or die \"Can't open $0: $!\";\n    print \"    $_\" while <$script>;\n    \n    __DATA__\n    https://stackoverflow.com/a/6163129/1039320\n    http://world.std.com/~swmcd/steven/perl/pm/xs/intro/index.html\n    http://www.masteringperl.org/2015/05/computing-excellent-numbers/\n    http://www.linuxcareer.com/perl-as-a-career-option\n    http://irclog.perlgeek.de/perl6/2013-10-17#i_7723873\n    http://www.perl.com/pub/2005/07/14/bestpractices.html\n    http://www.wall.org/~larry/pm.html\n    http://interviews.slashdot.org/story/02/09/06/1343222/larry-wall-on-perl-religion-and\n    http://www.perlmonks.org/bare/?node=Erudil\n    http://rjbs.manxome.org/rubric/entry/1959\n    http://blogs.perl.org/users/damian_conway/2012/03/why-i-love-my-job.html\n    http://perlbuzz.com/2010/11/29/think_for_perls_sake/\n    http://www.perlmonks.org/?node_id=1180698\n", "id": "dnowi15", "owner_tier": 0.1, "score": -2.1739130302663674e-10}], "link": "https://www.reddit.com/r/perl/comments/735nq2/whats_your_favorite_perl_tip/", "question": {"content": "Anything that blew your mind when you learned it, for example? Whether it's popularly known or not?", "id": "735nq2", "title": "What's your favorite Perl tip?", "traffic_rate": 2.8680186170212765}, "saved_time": 1721102271, "source": "reddit", "tags": []}, {"answers": [{"content": "The sorted() function takes a key= parameter Alternatively, you can use operator.itemgetter instead of defining the function yourself For completeness, add reverse=True to sort in descending order", "id": 73050, "owner_tier": 0.9, "score": 0.9999999999972804}, {"content": "I have been a big fan of a filter with lambda. However, it is not best option if you consider time complexity. 1000000 loops, best of 3: 0.736 \u00b5sec per loop 1000000 loops, best of 3: 0.438 \u00b5sec per loop", "id": 58179903, "owner_tier": 0.1, "score": 0.004079412561871091}, {"content": "You can sort a list of dictionaries with a key as shown below: Output: In addition, you can sort a list of dictionaries with a key and a list of values as shown below: Output:", "id": 76250224, "owner_tier": 0.1, "score": 0.0010878433478379113}, {"content": "It might be better to use dict.get() to fetch the values to sort by in the sorting key. One way it's better than dict[] is that a default value may be used to if a key is missing in some dictionary in the list. For example, if a list of dicts were sorted by 'age' but 'age' was missing in some dict, that dict can either be pushed to the back of the sorted list (or to the front) by simply passing inf as a default value to dict.get().", "id": 75935637, "owner_tier": 0.5, "score": 0.0010878433478379113}, {"content": "You can use the following:", "id": 75367181, "owner_tier": 0.5, "score": -2.7196083598654582e-12}, {"content": "Sometime we need to use lower() for case-insensitive sorting. For example,", "id": 45094029, "owner_tier": 0.5, "score": 0.00734294261354365}, {"content": "sorting by multiple columns, while in descending order on some of them:\nthe cmps array is global to the cmp function, containing field names and inv == -1 for desc 1 for asc", "id": 72939809, "owner_tier": 0.3, "score": -2.7196083598654582e-12}, {"content": "As indicated by @Claudiu to @monojohnny in comment section of this answer, given: to sort the list of dictionaries by key 'age', 'name'\n(like in SQL statement ORDER BY age, name), you can use: or, likewise print(newlist) [{'name': 'Bart', 'age': 10},  {'name': 'Milhouse', 'age': 10},\n{'name': 'Homer', 'age': 39}]", "id": 69072597, "owner_tier": 0.5, "score": 0.0005439216725591515}], "link": "https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python", "question": {"content": "How do I sort a list of dictionaries by a specific key's value? Given: When sorted by name, it should become:", "id": 72899, "title": "How to sort a list of dictionaries by a value of the dictionary in Python?", "traffic_rate": 245}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "list", "sorting", "dictionary", "data-structures"]}, {"answers": [{"content": "Python has a rule on methods. Either: Mutating methods never return the original object largely as a stylistic choice by Guido van Rossum (the original creator of Python); if a mutated object is returned, he wanted there to be no confusion over whether it was the original object (mutated in-place) or new mutated copy. Doing many different things on a single line was also something he wanted to discourage; without reverse=True, the cleaner way to write it would be: as two lines each doing one unique thing. That general rule is the case for all objects, not just list, but list.sort/list.reverse explicitly document this rationale: To remind users that [list.sort] operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request a new sorted list instance). For the specific case of list.sort, using reverse=True provides a couple benefits: Admittedly, the Python standard sorting algorithm (Timsort) (in place by the time reverse=True was added), optimizes for runs of data sorted in either direction (forward or reverse), so repeated sort-then-reverse would still perform well, but even so, it's costly and pointless to rearrange all the elements of the input only to swap them back again at the end. Sure, in a hypothetical design Python could have made bound list.sort methods a more complex object with methods of its own that tweaked the behavior, making X.sort.reverse() exactly equal to X.sort(reverse=True) (both would sort in reverse order directly without sorting in forward order, then reversing), but no other built-in type in Python does anything like that (bound methods can be called, and that's it, they don't provide extensible objects that can be tweaked). For that matter, I'm not aware of many other languages that allow anything similar (I wouldn't put it past some pure-functional languages, but Python is not pure-functional). Supporting this would complicate a lot of stuff (e.g. you can't just have a common interface for methods, you have to custom-design the descriptors and the objects they return to make new objects with methods of their own; that's a lot of work relative to the existing CPython design that basically just requires a C function definition, plus a one-liner macro mapping the string \"sort\" to that function for the purposes of method lookup, with the process of creating bound methods handled by a single uniform code path for every C-implemented method) and would introduce questions about consistency (should you also allow X.reverse.sort() to mean the same thing as X.sort.reverse()? If not, why not?), violate expectations from tons of other methods which use keyword arguments to make slight tweaks in behavior, and not provide any meaningful benefit. In a language without keyword arguments, sure, this might be useful (mylist.sort(True) or mylist.sort(False, lambda x: x[0]) for forward order with a key would be ugly), but with keyword arguments, X.sort(reverse=True) is not meaningfully worse than X.sort.reverse(), and unlike the latter, it doesn't create confusion between bound methods and full-fledged objects with methods of their own. As an historical note, prior to the introduction of TimSort in 2.3, Python used an unstable samplesort algorithm (with no optimizations for partially ordered data), and only supported a cmp argument (a function that accepted two values and returned a negative, 0, or positive number to indicate if the first argument was less than, equal to, or greater than the second). It was only when 2.4 introduced the key argument (to allow Python to do the far more efficient decorate-sort-undecorate, aka Schwartzian Transform under the hood for you) that the reverse argument was added alongside. When cmp was the only way to customize the comparison, you'd just write it to tweak the sort order yourself as needed, but with key functions, where you compute/extract new objects (typically simple built-ins like int, str, or tuples thereof), you could only say what to compare on, not how to compare that data; it's fairly common to want to sort on the key in reverse order, and if that had required writing a cmp function as well, just to flip the sort order, it would have eliminated most of the benefits of key functions (specifically, that they are called before sorting exactly once per input, where cmp functions must be called, on average, log n times per element). The newly stable sort also meant that, in the event you needed some comparisons to operate in reverse, and some in forward order (and you couldn't just invert values; int can be negated, but str cannot be), you could call sort repeatedly, from least important criteria to most, and the subsequent sorts will preserve the original order of elements that compare equal for that stage (and therefore the ordering imposed by the prior sorts). It's not great when you need to do this, but it's better than also having to add explicit reverse calls on top of all the multiple sort calls.", "id": 75187834, "owner_tier": 0.9, "score": 0.9999999900000002}], "link": "https://stackoverflow.com/questions/75177279/what-is-the-rationale-for-using-x-sort-reverse-true-and-not-x-sort-reverse", "question": {"content": "I have a list called X that I want to sort alphabetically in descending order\nand i want to know the rationale of \"reverse = True\" I would have written the following: But I see that it doesn't work and I want to know what it means to assign true to reverse", "id": 75177279, "title": "what is the rationale for using X.sort.(reverse=True) and not X.sort.reverse()", "traffic_rate": 272}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "python-3.x"]}, {"answers": [{"content": "While there should normally be no reason not to use the key argument for the sorted function or list.sort method, you can of course do without it, by creating a list of pairs (called tmp below) where the first item is the sort key and the second item is the original item. Due to lexicographical sorting, sorting this list will sort by the key first. Then you can take the items in the desired order from the sorted tmp list of pairs. Note that usually this would be written with list comprehensions instead of calling .append in a loop, but the purpose of this answer is to illustrate the underlying algorithm in a way most likely to be understood by beginners.", "id": 74964521, "owner_tier": 0.7, "score": 0.0}], "link": "https://stackoverflow.com/questions/44749903/how-do-i-implement-a-schwartzian-transform-in-python", "question": {"content": "In Perl I sometimes use the Schwartzian Transform to efficiently sort complex arrays: How to implement this transform in Python?", "id": 44749903, "title": "How do I implement a Schwartzian Transform in Python?", "traffic_rate": 0}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "sorting"]}, {"answers": [{"content": "I think this has something to do with the way the hasLayout property is implemented in the older browser. Have you tried your code in IE8 to see if works in there, too?\nIE8 has a Debugger (F12) and can also run in IE7 mode.", "id": 3998843, "owner_tier": 0.3, "score": 0.9999999997222222}], "link": "https://stackoverflow.com/questions/6/why-did-the-width-collapse-in-the-percentage-width-child-element-in-an-absolutel", "question": {"content": "I have an absolutely positioned div containing several children, one of which is a relatively positioned div. When I use a percentage-based width on the child div, it collapses to 0 width on IE7, but not on Firefox or Safari. If I use pixel width, it works. If the parent is relatively positioned, the percentage width on the child works.", "id": 6, "title": "Why did the width collapse in the percentage width child element in an absolutely positioned parent on Internet Explorer 7?", "traffic_rate": 5}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["html", "css", "internet-explorer-7"]}, {"answers": [], "link": "https://stackoverflow.com/questions/61683696/how-to-sort-hierarchical-data-represented-in-a-flat-list", "question": {"content": "I have a Python (version 2.7) list of objects that has an implied hierarchy. If one or more SubThings immediately follow a Thing, they belong with that Thing. I want to sort the Things by their value and when Things are moved for the sorting, I want their SubThings to move with them. All of these objects have values. Example input: I have working code for this in Python 2.7 but it's brute-force and feels inelegant - about 40 lines of code. First I create an intermediate data structure that groups Things with their SubThings, then I sort by Things' values, then I flatten the resulting structure. I have a feeling there is an elegant one or two (or three?)-liner for this. It even sounds like a classic opportunity for a Schwartzian Transform but I'm not making the \"Pythonic\" leap that easily groups SubThings with Things - maybe something with itertools.groupby()? For clarity: SubThings never occur without a parent Thing. Things may not have SubThings. I've simplified by leaving out the reality that the series of Things/SubThings can be preceded and followed by unrelated objects. It'd be awesome to see a solution that passes those through unsorted, i.e. in the position they were, but that's not as intellectually challenging to me.", "id": 61683696, "title": "How to sort hierarchical data represented in a flat list?", "traffic_rate": 452}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "sorting", "data-structures"]}, {"answers": [], "link": "https://stackoverflow.com/questions/46860219/how-do-i-make-a-merge-sort-built-for-a-single-list-sort-a-list-of-list-inste", "question": {"content": "So, let say I have a list of lists like this: If I wanted to sort this by the middle number in each sublist, it would end up like this: How can I cause a merge sort that is built to sort a list like this: [1, 2, 3, 4] To sort a list of list and specify it to merge based on the middle number of each sublist? Is there a trick with python that will allow this to work? Only thing I have been able to find about this is using the built-in sorting function, which I do not want to do. Below is the merge sort I am implementing. It works like this  mergeSort(list_here) And it merges the list. However, I would like to use a trick to make python merge based on a list of the list instead with the middle index of each sublist being what is being compared. Thanks for any guidance. Below is the merge sort:", "id": 46860219, "title": "How do I make a merge sort, built for a single list, sort a &quot;list of list&quot; instead?", "traffic_rate": 383}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python"]}, {"answers": [{"content": "The theoretical minimum number of comparisons needed to sort an array of n elements on average is lg (n!), which is about n lg n - n. There's no way to do better than this on average if you're using comparisons to order the elements. Of the standard O(n log n) comparison-based sorting algorithms, mergesort makes the lowest number of comparisons (just about n lg n, compared with about 1.44 n lg n for quicksort and about n lg n + 2n for heapsort), so it might be a good algorithm to use as a starting point.  Typically mergesort is slower than heapsort and quicksort, but that's usually under the assumption that comparisons are fast. If you do use mergesort, I'd recommend using an adaptive variant of mergesort like natural mergesort so that if the data is mostly sorted, the number of comparisons is closer to linear. There are a few other options available.  If you know for a fact that the data is already mostly sorted, you could use insertion sort or a standard variation of heapsort to try to speed up the sorting.  Alternatively, you could use mergesort but use an optimal sorting network as a base case when n is small.  This might shave off enough comparisons to give you a noticeable performance boost. Hope this helps! ", "id": 18386733, "owner_tier": 0.9, "score": 0.8888888877777777}, {"content": "Quicksort and mergesort are the fastest possible sorting algorithm, unless you have some additional information about the elements you want to sort. They will need O(n log(n)) comparisons, where n is the size of your array.\nIt is mathematically proved that any generic sorting algorithm cannot be more efficient than that. If you want to make the procedure faster, you might consider adding some metadata to accelerate the computation (can't be more precise unless you are, too). If you know something stronger, such as the existence of a maximum and a minimum, you can use faster sorting algorithms, such as radix sort or bucket sort. You can look for all the mentioned algorithms on wikipedia. As far as I know, you can't benefit from the expensive relationship. Even if you know that, you still need to perform such comparisons. As I said, you'd better try and cache some results. EDIT I took some time to think about it, and I came up with a slightly customized solution, that I think will make the minimum possible amount of expensive comparisons, but totally disregards the overall number of comparisons. It will make at most (n-m)*log(k) expensive comparisons, where Here is the description of the algorithm. It's worth nothing saying that it will perform much worse than a simple merge sort, unless m is big and k is little. The total running time is O[n^4 + E(n-m)log(k)], where E is the cost of an expensive comparison (I assumed E >> n, to prevent it from being wiped out from the asymptotic notation. That n^4 can probably be further reduced, at least in the mean case. EDIT The file I posted contained some errors. While trying it, I also fixed them (I overlooked the pseudocode for insert_sorted function, but the idea was correct. I made a Java program that sorts a vector of integers, with delays added as you described. Even if I was skeptical, it actually does better than mergesort, if the delay is significant (I used 1s delay agains integer comparison, which usually takes nanoseconds to execute)", "id": 18381740, "owner_tier": 0.5, "score": -1.111111111111111e-09}, {"content": "We can look at your problem in the another direction, Seems your problem is IO related, then you can use advantage of parallel sorting algorithms, In fact you can run many many threads to run comparison on files, then sort them by one of a best known parallel algorithms like Sample sort algorithm.", "id": 18400500, "owner_tier": 0.7, "score": -1.111111111111111e-09}, {"content": "Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? Merge insertion algorithm, described in D. Knuth's \"The art of computer programming\", Vol 3, chapter 5.3.1, uses less comparisons than other comparison-based algorithms. But still it needs O(N log N) comparisons. Would the existence of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? I think some of existing sorting algorithms may be modified to take into account expensive(i,j) predicate. Let's take the simplest of them - insertion sort. One of its variants, named in Wikipedia as binary insertion sort, uses only O(N log N) comparisons. It employs a binary search to determine the correct location to insert new elements. We could apply expensive(i,j) predicate after each binary search step to determine if it is cheap to compare the inserted element with \"middle\" element found in binary search step. If it is expensive we could try the \"middle\" element's neighbors, then their neighbors, etc. If no cheap comparisons could be found we just return to the \"middle\" element and perform expensive comparison. There are several possible optimizations. If predicate and/or cheap comparisons are not so cheap we could roll back to the \"middle\" element earlier than all other possibilities are tried. Also if move operations cannot be considered as very cheap, we could use some order statistics data structure (like Indexable skiplist) do reduce insertion cost to O(N log N). This modified insertion sort needs O(N log N) time for data movement, O(N2) predicate computations and cheap comparisons and O(N log N) expensive comparisons in the worst case. But more likely there would be only O(N log N) predicates and cheap comparisons and O(1) expensive comparisons. Consider a set of possibly large files. In this application the goal is to find duplicate files among them. If the only goal is to find duplicates, I think sorting (at least comparison sorting) is not necessary. You could just distribute the files between buckets depending on hash value computed for first megabyte of data from each file. If there are more than one file in some bucket, take other 10, 100, 1000, ... megabytes. If still more than one file in some bucket, compare them byte-by-byte. Actually this procedure is similar to radix sort.", "id": 18399928, "owner_tier": 0.7, "score": 0.11111111}, {"content": "Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? Edit: Ah, sorry. There are algorithms that minimize the number of comparisons (below), but not that I know of for specific elements. Would the existence of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? Not that I know of, but perhaps you'll find it in these papers below. I'd like pointers to further material on this topic. On Optimal and E\ufb03cient in Place Merging Stable Minimum Storage Merging by Symmetric Comparisons  Optimal Stable Merging (this one seems to be O(n log2 n) though Practical In-Place Mergesort If you implement any of them, posting them here might be useful for others too! :)", "id": 18387777, "owner_tier": 0.9, "score": 0.22222222111111112}, {"content": "Something to keep in mind is that if you are continuously sorting the list with new additions, and the comparison between two elements is guaranteed to never change, you can memoize the comparison operation which will lead to a performance increase. In most cases this won't be applicable, unfortunately.", "id": 18387652, "owner_tier": 0.1, "score": -1.111111111111111e-09}, {"content": "I'll try to answer each question as best as I can. Traditional sorting methods may have some variation, but in general, there is a mathematical limit to the minimum number of comparisons necessary to sort a list, and most algorithms take advantage of that, since comparisons are often not inexpensive. You could try sorting by something else, or try using a shortcut that may be faster that may approximate the real solution. I don't think you can get around the necessity of doing at least the minimum number of comparisons, but you may be able to change what you compare. If you can compare hashes or subsets of the data instead of the whole thing, that could certainly be helpful. Anything you can do to simplify the comparison operation will make a big difference, but without knowing specific details of the data, it's hard to suggest specific solutions. Check these out:", "id": 18381778, "owner_tier": 0.5, "score": 0.9999999988888888}, {"content": "A technique called the Schwartzian transform can be used to reduce any sorting problem to that of sorting integers. It requires you to apply a function f to each of your input items, where f(x) < f(y) if and only if x < y. (Python-oriented answer, when I thought the question was tagged [python]) If you can define a function f such that f(x) < f(y) if and only if x < y, then you can sort using Python guarantees that key is called at most once for each element of the iterable you are sorting. This provides support for the Schwartzian transform. Python 3 does not support specifying a cmp function, only the key parameter. This page provides a way of easily converting any cmp function to a key function.", "id": 18381772, "owner_tier": 0.9, "score": 0.4444444433333333}, {"content": "Most sorting algorithm out there try minimize the amount of comparisons during sorting. My advice:\nPick quick-sort as a base algorithm and memorize results of comparisons just in case you happen to compare the same problems again. This should help you in the O(N^2) worst case of quick-sort. Bear in mind that this will make you use O(N^2) memory.   Now if you are really adventurous you could try the Dual-Pivot quick-sort.", "id": 18381766, "owner_tier": 0.5, "score": -1.111111111111111e-09}], "link": "https://stackoverflow.com/questions/18381354/what-sorting-techniques-can-i-use-when-comparing-elements-is-expensive", "question": {"content": "I have an application where I want to sort an array a of elements a0, a1,...,an-1. I have a comparison function cmp(i,j) that compares elements ai and aj and a swap function swap(i,j), that swaps elements ai and aj of the array. In the application, execution of the cmp(i,j) function might be extremely expensive, to the point where one execution of cmp(i,j) takes longer than any other steps in the sort (except for other cmp(i,j) calls, of course) together. You may think of cmp(i,j) as a rather lengthy IO operation. Please assume for the sake of this question that there is no way to make cmp(i,j) faster. Assume all optimizations that could possibly make cmp(i,j) faster have already been done. Is there a sorting algorithm that minimizes the number of calls to cmp(i,j)? It is possible in my application to write a predicate expensive(i,j) that is true iff a call to cmp(i,j) would take a long time. expensive(i,j) is cheap and expensive(i,j) \u2227 expensive(j,k) \u2192 expensive(i,k) mostly holds in my current application. This is not guaranteed though. Would the existance of expensive(i,j) allow for a better algorithm that tries to avoid expensive comparing operations? If yes, can you point me to such an algorithm? I'd like pointers to further material on this topic. This is an example that is not entirely unlike the application I have. Consider a set of possibly large files. In this application the goal is to find duplicate files among them. This essentially boils down to sorting the files by some arbitrary criterium and then traversing them in order, outputting sequences of equal files that were encountered. Of course reader in large amounts of data is expensive, therefor one can, for instance, only read the first megabyte of each file and calculate a hash function on this data. If the files compare equal, so do the hashes, but the reverse may not hold. Two large file could only differ in one byte near the end. The implementation of expensive(i,j) in this case is simply a check whether the hashes are equal. If they are, an expensive deep comparison is neccessary.", "id": 18381354, "title": "What sorting techniques can I use when comparing elements is expensive?", "traffic_rate": 1039}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["performance", "algorithm", "sorting", "comparison"]}, {"answers": [], "link": "https://stackoverflow.com/questions/5760733/perl-in-place-sort-lines-in-a-text-file", "question": {"content": "I wish to modify a text file by sorting each line based on a given key and save the old file as a backup.  The key is a numeric character contained in each line.   Is there a simple script to get this done, preferably in-place? Thanks!", "id": 5760733, "title": "perl in-place sort lines in a text file", "traffic_rate": 711}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "perl"]}, {"answers": [], "link": "https://stackoverflow.com/questions/2019951/how-do-i-sort-this-list", "question": {"content": "I have a list of lists. Now I want to sort li in such a way that higher the foo(x) for a x higher it should appear in a sorted list. What is the best way in C#/Python/any other lang to this?", "id": 2019951, "title": "How do I sort this list?", "traffic_rate": 694}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["c#", "python", "ruby", "sorting", "haskell"]}, {"answers": [{"content": "I love Tries, so just for fun, I wrote a Trie-based solution : This should work for any string of any length. Note that the result is just printed to screen, not written back in a new list. You didn't write much code, so I'll leave it as an exercise ;)", "id": 42902140, "owner_tier": 0.9, "score": -1.4285714285714286e-09}, {"content": "Here is one way to do it: Traditionally, lexicographic sort order longer strings after their otherwise identical prefixes (i.e.  'abc' goes before 'abcd'). To meet your sort expectation, we first \"fix-up\" the shorter string by adding the remaining part of the longer string plus another character to make it the longer of the two: The functools.cmp_to_key() tool then converts the comparison function to a key function. This may seem like a lot of work, but the sort expectations are very much at odds with the built-in lexicographic sorting rules. FWIW, here's another way of writing it, that might or might not be considered clearer: The logic is:", "id": 42900060, "owner_tier": 0.9, "score": 0.9999999985714286}, {"content": "My first answer was: just negate the len criterion to reverse only on that criterion. But that doesn't work, because there's a conflict between alpha sort and length. Alpha sort puts small strings first. So length criterion doesn't work. You need to merge both criteria. There's no clear priority between each other. I found a way: first compute the max length of your strings, then return the chr(127) filled (the biggest char provided you're using only ASCII) version of the string as key so smallest strings are filled with big chars in the end: they always come last. result: BTW don't call your list list for obvious reasons.", "id": 42899432, "owner_tier": 0.9, "score": 0.14285714142857142}, {"content": "One could construct the key by taking: For example:", "id": 42899718, "owner_tier": 0.5, "score": -1.4285714285714286e-09}], "link": "https://stackoverflow.com/questions/42899405/sort-a-list-with-longest-items-first", "question": {"content": "I am using a lambda to modify the behaviour of sort. Sorting a list containing the elements A1,A2,A3,A,B1,B2,B3,B, the result is A,A1,A2,A3,B,B1,B2,B3. My expected sorted list would be A1,A2,A3,A,B1,B2,B3,B. I've already tried to include the len(item) for sorting, which didn't work. How to modify the lambda so that the sort result is instead? ", "id": 42899405, "title": "Sort a list with longest items first", "traffic_rate": 1424}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "stackoverflow", "tags": ["python", "python-3.x", "sorting", "lambda"]}, {"answers": [{"content": "As you've more or less stated, the `sort` package assumes that you'll be sorting items based on their values, or values that are at the ready.  You don't go into too much detail about exactly the kinds of values or objects you were sorting, and why their \"true value\", which is apparently some int value, wasn't simply precomputed and already available.  But I can believe there are good reasons for it; perhaps that value is simply never necessary after the values are in order.\n\nGoing back to this \"Schwartzian transform\", it's a nice idiom in Perl for sure.  But even in Go, the general approach is pretty sound.  All you're doing is making a new list of values easier to compare, each value corresponding to an element in the original list, and sorting by proxy.  Here's an example of a generalized way of doing that, applied to your example: http://play.golang.org/p/ph2EOXC9pd\n\nOne of the major advantages of the `sort` package's approach with its `Interface` type is that it lets one defined type act upon itself without bothering with a great deal of `interface{}` conversions.  For example, it's really cheap for the IntSlice type to compare two elements with `x.Less(i,j)`.  Must less expensive than calling `x.LessVal(x.Key(i), x.Key(j))`, even if your key values have been cached.\n\nTake that advice as you wish.  But, concretely, there are some things in your approach you should fix.  You're really not gaining anything but complexity by trying to lazily initialize your Key values, because you will need to compute all of them.  It also doesn't make much sense to use a `map[int]T` when the values of your keys are bounded, and you expect to use all values of 0..N.  In short, use a slice.  By using a slice, you can \"prime\" all your values in parallel without worrying about synchronizing access.\n", "id": "cu4gdm7", "owner_tier": 0.5, "score": 0.14285714142857142}, {"content": "\"sort\" is a really interesting case, because it shows off both the parts of generics that Go does have, and shows off the parts it does not. Contrary to popular opinion, Go does have some \"generics\", in that interfaces can provide \"generic algorithms\" ignorant of the underlying implementation, which the sort package does. What it does not have are generic types.\n\n(This isn't rhetorical slight of hand trying to make Go look good... in my considered opinion it's actually important to understand that \"generic\" is not an atomic thing on its own and consists of many parts, or you don't really understand how to use them in any language.)\n\nConsequently, one of the answers to the question \"How can you program in a language missing generics?\" is that it's only half-missing generics, and arguably has the more useful half. (And arguably does not. I mean the \"arguably\" literally.)\n\nLack of generic types has a lot of consequences here. One of them is that trying to reuse `sort` code is actually a pain because it's virtually impossible to express the 'real' types. The other one is that because of that, the real \"Go\" thing to do here is to... not. When you can't use the type system to reuse code, Go's pragmatism is that maybe you just need to duplicate a bit of code.[1]\n\nThe Keysort package should go ahead and simply provide its own sorting routines. Start by copying & pasting the sort code if you like. It's OK, it'll end up diverging enough to make it worthwhile. Then you can fix all those other problems you mention, too. It's OK. It is not part of the Go philosophy that, having provided \"sort\" in the core library, you must always and only use that sort. It's the Go philosophy that you ought to have a really good reason to use another implementation and not just reach willy-nilly for half-a-dozen sort routines because they're cool or Web Scale, but if you really do have the need, go for it. I suspect you'll find quite a few things you can do that makes this approach work better for you.\n\nAlso, regardless of the fact that it's called `sort.Interface` in the core library, I would consider calling an interface `Interface` to be a code smell, as silly as `type Struct struct`. That's just old Go code that can't be changed now ([copyright shows as 2009](https://golang.org/src/sort/sort.go)) but probably shouldn't be copied.\n\n[1]: To be honest, this is where Go's pragmatism and I have some disagreements, but I'll concede it works better than I'd expect. Still, Go is clearly not averse to some copying and pasting, and my current score for \"core libraries I've copied and started modifying for some very good reason\" is at 3. It turns out that they can provide a really nice base of working code for you to mutate if you need to... the vast bulk of them are quite nice under the hood, and well covered in unit tests, and it's been a pleasure to start with them rather than start from scratch.", "id": "cu4xiyq", "owner_tier": 0.7, "score": -1.428571419889327e-09}, {"content": "I wish this was simpler in go.\n\n    sort.SortByKey(people, func( p *Person ) { return p.Age } )\n\nGuess that needs generics though :(\n\nYour helper func doesn't define a return type to be compared.  Imposing a return type on that would break the generalized functionality provided by the `Less(i, j int) bool` function in the `sort.Interface` type.\n\nIn Python, `sort(people, key=lambda p: p.age)` works because `cmp(a.age, b.age)` is probably going to return _something_ usable.  But in Go, you need to do something which works with statically defined types.\n\nThe upshot is that with complicated structs you can define an order of precedence of attributes.  E.g.,\n\n    type sortPeople []*Person\n    \n    func (s sortPeople) Less(i, j int) bool {\n        a, b := s[i],  s[j]\n        return a.Age < b.Age || a.Name < b.Name || a.Phone < b.Phone\n    }\n    \n\nYour general idea is fine, but your Less implementation is not going to behave how you expect. You'll want something like this:\n\n    func (s sortPeople) Less(i, j int) bool {\n        a, b := s[i],  s[j]\n        if a.Age != b.Age { return a.Age < b.Age }\n        if a.Name != b.Name { return a.Name < b.Name }\n        return a.Phone < b.Phone\n    }\n\nI don't disagree with what you said, I just wish it was simpler.  I'm not an expect on the finer points of types an interfaces, but couldn't go implement an Orderable interface for types to enable this sort of thing?  I think that is how things work in something like haskell with `Ord`...\n\n\tfmt.Println(1 < 10)\n\tfmt.Println(\"bob\" < \"alice\")\n\nworks, but you can't write\n\n    func isLessThan(a, b: Orderable) {\n        return a < b\n    }\n\nIs this the generics issue again? I don't really care what it is called, just that writing\n\n    sort.SortByKey(people, func( p *Person ) { return p.Age } )\n    sort.SortByKey(people, func( p *Person ) { return p.FirstName } )\n    sort.SortByKey(people, func( p *Person ) { return p.LastName } )\n\nshouldn't be any more complicated than that.  Go knows how to sort numbers and strings, I'm giving it a function that converts from a Person to something it already knows how to sort.  Why can't it just sort it?\n\nI have a feeling I am going to get downvoted again for using the work 'generics'.. I bet that in a year or two this will be possible and people will say things like \"Remember when we had to write 10 lines of code to sort arrays by different things?\"\n\nYour `SortByProxy` is nice though, I modified it for the Person example.  It gets things down to:\n\n\tproxy := make([]int, len(needsSorting))\n\tfor i := range needsSorting {\n\t\tproxy[i] = needsSorting[i].Age\n\t}\n\tSortByProxy(needsSorting, sort.IntSlice(proxy))\n\t\n\n\tfproxy := make([]string, len(needsSorting))\n\tfor i := range needsSorting {\n\t\tfproxy[i] = needsSorting[i].FirstName\n\t}\n\tSortByProxy(needsSorting, sort.StringSlice(fproxy))\n\nThen I wrote:\n\n    func (pl People) BuildStringProxy(tf func(p *Person) string) sort.StringSlice {\n\tproxy := make([]string, len(pl))\n\tfor i, person := range pl {\n\t\tproxy[i] = tf(&person)\n\t}\n\treturn sort.StringSlice(proxy)\n    }\n\nWhich gets it down to\n\n\tSortByProxy(needsSorting, needsSorting.BuildStringProxy(func(p *Person) string { return p.FirstName }))\n\tSortByProxy(needsSorting, needsSorting.BuildStringProxy(func(p *Person) string { return p.LastName }))\n\nWhich isn't too far off from what I would like to see.\n\nYup, too hasty an example.", "id": "cu4dfc1", "owner_tier": 0.5, "score": 0.9999999985714284}], "link": "https://www.reddit.com/r/golang/comments/3h4ccr/keysort_the_schwartzian_transform_in_go/", "question": {"content": "", "id": "3h4ccr", "title": "Keysort: The Schwartzian Transform in Go", "traffic_rate": 48.87912702853945}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "Functions are objects in Python, and can be used like any other type.  Passing a function as an argument works pretty much the same was as passing a function pointer in C++.\n\nThe `key` parameter specifies a function that the sort algorithm will use to transform all the values in the sequence prior to sorting it, in effect creating a new temporary list that contains the values that will be used for comparisons.  This is how you customize sorting.  This technique is actually much more powerful than the usual technique in C++ of passing a comparison function, and it can result in much higher performance.  In essence it's a case of the [Schwartzian transform](https://en.wikipedia.org/wiki/Schwartzian_transform).\n\nThe `key` used here returns the last element in whatever is passed to it.  The first thing the sort algorithm does is call the key function on each element in the input sequence to create a temporary list of the resulting return values.  Then that temporary list is used as the keys for comparing elements in the original list.  So it's as if you're sorting `[7, 3, 5, 2]`, but each time you swap a pair of elements in the temporary list you also swap the corresponding pair in the real list.\n\nThe reason this can result in higher performance is because this transformation function is called exactly once per item in the list.  The comparison function on the other hand is called more frequently, generally O(n log n).  If the comparison function has to do anything complicated, it makes sense to \"precompute\" all the transforms once prior to starting the sort.  (This is not always necessarily a guaranteed win.)\n\nThis is a very powerful way of customizing the sort.  There are even [specialized helper functions in the `operator` module](https://docs.python.org/3/library/operator.html) meant to make this even easier (and faster, since they're implemented in C.)  This sort could be written even more compactly as:\n\n    import operator\n\n    def sort_last(items):\n        return sorted(items, key=operator.itemgetter(-1))\n\nAnd for example if you had a list of objects that each have some attribute, and you want to sort them by that attribute, you could write:\n\n    items.sort(key=operator.attrgetter('foo'))\n\n(`list.sort()` is the in-place version.)\n\nAnother example is sorting strings while ignoring case:\n\n    foo = sorted(bar, key=str.casefold)\n\n(In versions prior to 3.3 you'd have to use `str.lower` because `str.casefold` did not exist, but that does not provide true, proper case-insensitive sorting.  And hopefully you're not using 2.x.)\n\n\nHow is it that your programming language comprehension seems to be both comprehensive and godlike across many languages? You always seem to show up and answer my questions!\n\nAnyway, your awesome responses, as usual, are really helpful, and also gives me some homework to look into things a bit further.\n\nThanks!", "id": "cyl3bxd", "owner_tier": 0.7, "score": -1.6666666565375481e-09}, {"content": "This sorting function applies that function assigned to key to every item of the sequence and sort that sequence according to return values of that function. In this case it takes every tuple in tuples, returns last number in that tuple and sorts the sequence according to those last numbers.\n\nI should have clarified - I know what is happening, but the syntax is confusing to me. For example - how does last() know that it should operate on an element in the tuple, as opposed to the whole list? Is it not true that when we call [-1] from a list, or a tuple, both are equally valid syntax? Can we just assume that when we use sorted(), it knows that when we give \"key\" an instruction, that this instruction is applied to each entry in the list, and the type is derived from context?\n\nDoes sorted handle multi-dimensional objects - such as a list of tuples of tuples, i.e. \n    \n    complex_tuple = [((1,3),(2,4)),((3,4,6),(1,),(2,5,7)),((0,))]\n\n?\n\n> For example - how does last() know that it should operate on an element in the tuple, as opposed to the whole list? \n\nIt doesn't know that.  It has no idea what's going to be passed to it.  The sort algorithm calls the key function repeatedly, each time passing it one element in the input iterable that's being sorted.  In this case those elements are tuples, so the function is returning the last value in each tuple.  But it would work the same if it was a list of lists, or a list of any other sequence type that supports indexing (e.g. a deque, string, bytes, etc.)\n\nEdit: To address your addendum:\n\n> Does sorted handle multi-dimensional objects - such as a list of tuples of tuples, i.e. \n\nThe sort algorithm doesn't really care what types you pass to it or what they contain.  It's going to try to order each top-level value with `<`.  In your example, the input iterable contains three items, and it's going to simply compare each of them with each other, e.g. it will ask whether `x[0]` is less than `x[1]`, then whether the result of that is less than `x[2]`, and so on.  That's it as far as `sorted()` is concerned; the definition of less-than is not decided by `sorted()`, it's decided by the types being compared and their respective comparison operators, just as in C++ where you overload `operator<()`.\n\nIn the case of tuples, in Python they compare lexicographically, the same as `std::tuple` in C++.\n\n    >>> (1, 2) < (1, 1, 4)\n    False\n    >>> (3, 2) < (1, 2)\n    False\n\nThis applies recursively, so if the first elements of both are tuples, they are compared lexicographically as well:\n\n    >>> ((2, 1), 3) < ((2, 2), 1)\n    True\n\n\nAh okay, so when you have an iterable container, sorted() looks only at the elements in that container, not sub-elements in the case where the iterable container might contain other iterable containers, gotcha. I think I understand better now - since key=last tells \"last\" to try to do something to each element in the list container, and because the return value of tuple[-1] will give us a key value which sorted understands (in this case, numeric) it should do the right thing. Did I get that right?\n\n> because the return value of tuple[-1] will give us a key value which sorted understands (in this case, numeric)\n\nRight.  Except that `sorted()` doesn't have to understand anything really.  Comparing numeric values with less-than does the obvious thing, so `sorted()` is happy and oblivious.\n\nYou don't actually have to return a simple scalar value though; it's a general purpose transformation.  Another neat trick is that the key function can return a tuple.  That's how you specify multiple sort criteria.  Let's say you have some data with several attributes:\n\n    import collections\n\n    Person = collections.namedtuple('Person', 'name age hometown')\n\n    people = [\n        Person('Lisa', 29, 'Omaha, NE'),\n        Person('Robert', 31, 'Fresno, CA'),\n        Person('Alexanda', 31, 'Bridgeport, CT'),\n        Person('Ted', 27, 'El Paso, TX')\n    ]\n\n(The `namedtuple` is a subclass of the tuple that allows access to the fields by name as well as by index.)\n\nIf you ask for `sorted(people)`, you get them sorted by name, because that's the first field of the tuple.\n\n    >>> sorted(people)\n    [Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Robert', age=31, hometown='Fresno, CA'),\n     Person(name='Ted', age=27, hometown='El Paso, TX')]\n\nIn this version without giving a key function it's comparing tuples directly, so it checks the fields in order.  It compares names, and if there were two names that were the same, then ages would be compared, and if those are the same then hometowns would be compared.\n\nSuppose instead you want them sorted by age:\n\n    >>> sorted(people, key=operator.attrgetter('age'))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Robert', age=31, hometown='Fresno, CA'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT')]\n\nSince the key now only contains the age, and because Timsort is stable, when there's a tie there's no more fields to check, and the values retain their original ordering (Robert before Alexandra.)  If you want to sort by age and then by hometown, you can do it by having the key function return a tuple with the attributes in the desired order:\n\n    >>> sorted(people, key=operator.attrgetter('age', 'hometown'))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Robert', age=31, hometown='Fresno, CA')]\n\nThis uses a feature of `attrgetter()` that you can supply multiple attributes and it will return a tuple of the corresponding values, but to be clear you can do this yourself:\n\n    >>> sorted(people, key=lambda person: (person.age, person.hometown))\n    [Person(name='Ted', age=27, hometown='El Paso, TX'),\n     Person(name='Lisa', age=29, hometown='Omaha, NE'),\n     Person(name='Alexanda', age=31, hometown='Bridgeport, CT'),\n     Person(name='Robert', age=31, hometown='Fresno, CA')]\n\nThis lambda is a more compact version of defining a function that would look like:\n\n    def key_func(person):\n        return person.age, person.hometown\n\n    sorted(..., key=key_func)\n\nAnd thanks for the gold!\n", "id": "cyl2zvk", "owner_tier": 0.1, "score": 0.9999999983333332}, {"content": "Neat! Always interesting to see the perspective of an experienced programmer first encountering Python.\n\n>Also, the arguments one seems to be able to pass to the sorted() function seems almost magical!\n\nPython has a bunch of cool argument stuff that simply doesn't have a good analogy in C++.\n\nIn C++, if you call a function with the same name twice with different numbers of arguments, I believe that generally means you are calling different overloaded versions of the function with different distinct signatures, and that there are a finite, well-defined number of these functions.\n\nPython has a number of different concepts which largely eliminate the need for function overloading: a single function signature can be made to accommodate an arbitrary number of arguments of arbitrary type. Targets for your google searching in regards to these concepts: positional arguments, optional arguments, keyword arguments.\n\n>is there some cost associated with this?\n\nIf you are talking about performance costs, almost certainly! In Python, however, costs for a feature such as this would usually be considered negligible in the grand scheme of things, and worth the sacrifice for better flexibility, readability, simplicity, ease of use, or even elegance (depending on the feature in question). This is because the Python philosophy tends to value programmer productivity over machine performance.\n\nFinally, \n\n> I've never seen a function passed as an argument to the key argument to sorted. ... how does the sorting algorithm know that I want to access the last piece of each tuple in the list, rather than the last element of the list?\n\nFrom the docs: https://docs.python.org/3/library/functions.html#sorted\n\n>*key* specifies a function of one argument that is used to extract a comparison key from each list element\n\nThis is a bit inaccurate, since it should say a comparison key from each *iterable* element, not each `list` element. You could pass any iterable to `sorted()`.\n\n    def last(list):\n        return list[-1]\n\nSo `last(currentelement)` gets called for every element of whatever is passed to the first argument of `sorted()`. For example, if you wanted to build a list of keys that will be used by the `sorted()` function, you could do something like:\n\n    keys = []\n    for item in mylist:\n        keys.append(last(item))\n\nCalling the argument of `last()` 'list' is a bit of a misnomer, as it is definitely not guaranteed to be a `list` (and furthermore, masks the name of the important built-in function [list](https://docs.python.org/3/library/stdtypes.html#list) ).  Indeed, if you try passing an iterable containing any non-sequence type to your `sort_last` function, such as the tuple `(1, 2, 3, 4)`, you will almost certainly get a runtime error, since a non-sequence type will typically not have subscripting implemented.\n\n    >>> t = (1, 2, 3, 4)\n    None\n    >>> sort_last(t)\n    Traceback (most recent call last):\n      File \"python\", line 1, in <module>\n      File \"python\", line 4, in sort_last\n      File \"python\", line 2, in last\n    TypeError: 'int' object is not subscriptable\n\n(Note I am getting towards the edge of being too tired to post in this subreddit, so I hope not too many mistakes. I've already edited out a couple!)\n\nThanks very much for the help. The documentation for sorted, and your answer makes things more clear for me. One oddity I encountered while poking around with sorting is the difference between\n\n    container.sort()\n\nand\n\n    sorted(container,key=<condition>)\n\nI guess using sorted() seems to be the preferred method, as it preserves the original ordering of the container, and returns a new container. Its cool to have the flexibility to sort the container itself, or get a sorted copy of the container subject to some arbitrary condition.\n\nThanks again!\n\nNo problem. I think someone already discussed the in-place nature of `.sort()`. I think it should be noted that `.sort()` as a method is relatively unique to the `list`class among the built-in iterable data types. `list`s happen to have the combination of traits of being iterable, ordered, and mutable, so an in-place sorting function can kind of make sense.  This is an uncommon combination among the built-ins. For instance, `str`s, are iterable and ordered, but immutable, so no in-place `.sort()` method was implemented (theoretically to avoid confusion). `dict`s, meanwhile, are iterable and mutable, but are not ordered, so again, an in-place `.sort()` doesn't really make sense.\n\nGotcha - so in a sense, the most general thing to use would be sorted() rather than sort(), especially if there is some expectation that an iterable data-type may not be known ahead of time, for some kind of sorting application? This calls to mind use of iterators, from C++ standard template library.\n\n> the most general thing to use would be sorted() rather than sort(), especially if there is some expectation that an iterable data-type may not be known ahead of time\n\nYep, exactly.\n\n>This calls to mind use of iterators, from C++ standard template library.\n\n\nI agree. Oh man, you have no idea how happy I was when [range-based for loops](http://en.cppreference.com/w/cpp/language/range-for) and the [`auto`](http://en.cppreference.com/w/cpp/language/auto) keyword came to C++. `list`s are actually quite similar to `vector`s in their implementation, disregarding the obvious differences of typing.", "id": "cyl4cu3", "owner_tier": 0.3, "score": 0.3333333316666666}], "link": "https://www.reddit.com/r/learnpython/comments/3zcsfh/learning_python_with_google_codes_python_course/", "question": {"content": "[deleted]", "id": "3zcsfh", "title": "Learning python with Google Code's python course, lots of C++ experience, how does key-sorting with tuples work?", "traffic_rate": 153.13018518518518}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "I\u2019d say the lack of built in dependency management.  Between requirements.txt, setup.py, pipenv and now poetry, there is no consistency and it makes the dx worst.\n\nThere is consistency available now, sort of. You can use whatever dependency management system you want, and you specify *what that system is* in the `pyproject.toml`. So long as your dependencies, and your dependency manager, are both compliant with PEP 517 and 518, you shouldn't have any problems installing them.\n\nPip and venv *is* built-in dependency management. `requirements.txt` and `setup.py:setup:install_requires` have different purposes and should be used together. Granted, it's not great (especially coming from the build system nirvana that is rust's `cargo`).\n\nFor what it\u2019s worth, I see light at the end of this particular tunnel.  PEPs 517 and 518 have created a standard with pyproject,toml that solves many past problems. Poetry does a brilliant job actualizing those improvements.\n\n[deleted]\n\nReally could use some improvement or consolidation in this area!\n\nAs far as I\u2019m concerned, dependency management has been solved since ~2015.  Pip and wheels just work.\n\nThere is consistency if you only use one method...\n\nDeveloper experience\n\npip is broken. You have just been lucky.\n\nIt what way?  Just make your software work on multiple versions of dependencies and you won\u2019t have nearly as many issues.  When you create an unresolvable package, yeah, you have issues.\n\nI will admit, I wish pip came with a compiler on Windows.\n\nThe problem is that it can install incompatible versions without realising. Its dependency resolver does not look at the whole tree. It looks as it moves along. \n\nImagine you have a situation like this.\n\n1. you ask pip to install a set of packages (with requirements.txt). The first entry is foo\n2. foo requires bar < 3.0. Pip installs bar 2.4.2 which is the latest not three.\n3. pip continues to check requirements.txt and finds you want quux\n4. quux wants bar > 3.0, <4.0\n5. pip finds that you already have bar 2.4.2 installed, so it proceeds to upgrade it to 3.4.7 (latest of the 3.x series)\n6. Now your foo is broken, and nothing has warned you.\n\nNow I am not sure if the latest pip still behaves like this. It is constantly updated and I know they are working on a depsolver. but my point above is over simplified. It gets even worse when you bring in different platforms (which have different dependencies) and C extensions which have their own ABI.", "id": "g3thjeo", "owner_tier": 0.1, "score": 0.9999999998387097}, {"content": "Not so much a language design mistake, but tkinter.\n\nCan you elaborate?", "id": "g3tkafq", "owner_tier": 0.5, "score": 0.37096774177419356}, {"content": "The expressions for default arguments are evaluated at function definition time rather than call time.\n\nAnd if they're mutable and you mutate them in the function body, it can lead to very subtle bugs.\n\nThere are use cases for this though. Think of a function that behaves as a form of cache where you *want* the mutable default.\n\nYeah I just noticed that the other day when bugbear told me.. B006\nhttps://pypi.org/project/flake8-bugbear/\n\nI agree that there are use cases, but it's an implicit quirk that not many people are aware of, clearly going against python's \"zen\". If I wanted a cache then I'd turn to either decorators or the yield statement based on use-case, as it's more obviously breaking the function's re-entrancy.", "id": "g3tjguo", "owner_tier": 0.3, "score": 0.38709677403225806}, {"content": "I think the type annotation system is really yucky to work with. Having to import List from typing every time you want to declare a list input sucks. Other bolted-on type systems like Flow or TypeScript are much nicer to work with - they feel more terse and easier to compose.\n\nI think I understand why it's like that though. I think the Python maintainers want to keep the code of CPython relatively simple and easy to understand, which is the opposite of the nightmarish toolchain required to do a hello world in a modern JS. Given this, they were limited in what they could add. So perhaps not a \"design mistake\" as much a choice that I don't like.\n\nThat said I wouldn't mind using a preprocessor that strips away type annotations before running the code, if it meant the type annotation system could be better.\n\nFixed in 3.9, you'll be able to annotate types with the actual builtins. Until then, PyCharm can handle imports for you (really wish VScode python would get round to implementing that).\n\nThis one is getting fixed in Python 3.9.\n\nhttps://www.python.org/dev/peps/pep-0585/\n\nYou can just write things like list[str] without having to import a separate List from typing.\n\nI wish type-checking actually mattered at compile time.", "id": "g3tly8r", "owner_tier": 0.5, "score": 0.5483870966129032}, {"content": "Not saying only spaces for indenting, a tab should be a hard compilation error\n\nIIRC mixing tabs & spaces is now a syntax error. Also [black](https://github.com/psf/black) is solving this for you.\n\nI would go even further and raise an error any indentation which is not 4 spaces per level.\n\nEdit: \n\nI agree that this is not possible to change anymore because there is too much code with non PEP-8 compliant identation. This restriction could have only be made in the very beginning.\n\nFortunately, with PyCharm fixing the indentations is only 2 clicks away ;-)\n\nI dont care so much about that as long as it is consistent.  If we want to go down that rabbit hole he should have used s-exper like the lord intended\n\nThat would break most of the Python code at Google with IIRC uses 2 spaces", "id": "g3tgki6", "owner_tier": 0.7, "score": 0.27419354822580644}, {"content": "\nIn an otherwise nice looking language, these stick out like sore thumbs to me:\n\n    __dunders__\n    self.\n\nExplicit self is a great idea. You don't always know all of a class' member variables due to inheritance and the fact that you can add and remove them at runtime.\n\nI meant the syntax, not the concept.", "id": "g3tq6ie", "owner_tier": 0.5, "score": 0.2419354837096774}, {"content": "* `a += b` doesn\u2019t do the same thing as `a = a + b`.\n\n* `async`-`await` is OK, I understand why Python chose that approach over Lua-style coroutines where it\u2019s all implicit. However, `asyncio`, the standard library built on top of the `async`-`await` language feature, is widely disliked.\n\n> a += b doesn\u2019t do the same thing as a = a + b\n\nand it should not. One modifies in place. the other creates a new object and rebinds. That's not a design mistake. it's how programming works.\n\n> a += b doesn\u2019t do the same thing as a = a + b.\n\nexplain?\n\n> it's how programming works.\n\nIt\u2019s how *Python* works, but not all languages.\n\nI consider [C#\u2019s behaviour](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/assignment-operator#compound-assignment) to be more intuitive: \u201c`x op= y` is equivalent to `x = x op y` except that `x` is only evaluated once\u201d.\n\nA simple example:\n\n    >>> a = [1, 2]\n    >>> b = a\n    >>> a = a + [3, 4]\n    >>> print(a, b)\n    [1, 2, 3, 4] [1, 2]\n\n    >>> a = [1, 2]\n    >>> b = a\n    >>> a += [3, 4]\n    >>> print(a, b)\n    [1, 2, 3, 4] [1, 2, 3, 4]\n\nAnd a very strange example:\n\n    >>> t = ([1, 2], 5, 6)\n    >>> t[0] = t[0] + [3, 4]\n    TypeError: 'tuple' object does not support item assignment\n    >>> print(t)\n    ([1, 2], 5, 6)\n\n    >>> t = ([1, 2], 5, 6)\n    >>> t[0] += [3, 4]\n    TypeError: 'tuple' object does not support item assignment\n    >>> print(t)\n    ([1, 2, 3, 4], 5, 6)\n\nIt's two different operators. For immutable objects, the result is the same, but for the mutables, the documented behaviour is that the inplace versions mutate the LHS object in place.\n\nso now my question is why would it evaluate `x` twice in `x = x op y`? Because it's doing an assignment on `x`?\n\nthanks\n\nIf we have `int[] a = {0, 1, 2, 3}; var i = 0;`, the line `a[i++] = a[i++] + 1;` will increment `i` twice. Once on the LHS of the assignment, to work out where to write to, and then again on the RHS to work out where to read from.\n\n~~that's a completely different operator and situation. How is that related?~~\n\nok i got it. In any case, that's also the case in python. a\\[f()\\] = a\\[f()\\] + 1 evaluates f twice. a\\[f()\\] += 1 evaluates it only once.", "id": "g3tm2ip", "owner_tier": 0.3, "score": 0.4516129030645161}, {"content": "* The walrus operator, `:=`, a.k.a. assignment expressions. Almost every use of them makes code harder to read and understand.\n\n* The proposed pattern matching feature ([PEP 622](https://www.python.org/dev/peps/pep-0622/)). See [this critique](https://github.com/markshannon/pep622-critique) for examples of the issues with it.\n\nI have to disagree with your opinion on the walrus operator. I have found that is extremely useful in cases like\n\n    if k in dic:\n        v = dic.get(k)\n        # ... do stuff with v\n\ncan be rewritten as\n\n    if v := dic.get(k):\n        # ... do stuff with v\n\nIMO that's best written as:\n\n    v = dic.get(k)\n    if v:\n        # ...", "id": "g3v4h8f", "owner_tier": 0.3, "score": 0.19354838693548385}, {"content": "Python may be the worst mainstream language around for accidental mutations. On top of the standard baggage reference semantic languages like Java, C#, Kotlin, etc have, Python:\n\n - has the nasty gotcha for function default arguments\n - has the same nasty gotcha for dataclass field defaults\n - it first classes in a specific set of data structures, which are all mutable, via comprehensions. If you look at how a language like say Kotlin handles the equivalent of a list comprehension, it's almost as concise but you can use any structure, so you can use an Immutable list (and even the default is a read only list)\n - it has pretty poor support for keeping things as expressions; it tends to encourage a lot of creating things followed by in place mutation, which increases the likelihood of mutation errors. Why poor support for expressions? No real lambdas, no left to right chaining, if's are not expressions, to name a few.\n\nCould you explain what you mean by \u201cno left to right chaining\u201d?\n\nSure. Let's say you have a string. You write a function to extra a token (sub-string) from that string according to some rules. You want to do that, then turn it into an integer, and then take its log.\n\n```\nx = math.log(int(token(my_string)))\n```\n\nThis reads inside out. The data flows right to left, whereas we read left to right. You read the log first, but it's the last thing that happens. If you were to write this out with a few lines, then as we reading (from top to bottom), each step in the computation is sequenced in the same order it's carried out.\n\nIf you take kotlin for example, it has extension functions. So you would instead see something like:\n\n```\nx = my_string.token().toInt().let { log(it) }\n```\n\nSome languages (like F#, and JS experimentally) have a pipe operator:\n\n```\nx = my_string |> token |> int |> log\n```\n\nThis example is pretty simple because it involves a variable. But once you get into collections, the difference is pretty stark. Considering this piece of kotlin:\n\n```\nval x = my_list\n    .map { f1(it) }\n    .filter { p1(it) }\n    .map { f2(it) }\n    .filter { p2(it) }\n```\n\nMany languages nowadays like C#, Java, Rust, even C++, will allow you to chain operations on collections like this. In python, nobody would try to write this in a single expression:\n\n```\nx = [y2 for x2 in \n        [y1 for x1 in my_list if p1((y1 := f1(x))]\n        if p2((y2 := f2(x))]\n```\n\nI'm not even sure if that works (whether that's legal use of the walrus operator). But you can see how awful it is. In python people realistically are going to do that in several steps. That results in a lot more temporary/in-between variables lying around, all of which are mutable. Look at everything in itertools in python, it's all free functions so as you chain things together you have this reverse ordering problem. In languages like Kotlin, Rust, C#, Java, collections have member functions or extension functions for all these operations.", "id": "g3w1isu", "owner_tier": 0.5, "score": 0.1129032256451613}, {"content": "* clumsy lambda syntax\n\n* `dict.values()[0]` doesn't work (mostly for REPL use)\n\n* gotta do `itertools.chain(i1,i2)`. Why not `i1+i2`?\n\n* logging module. 'nuff said\n\n* defining the same function twice doesn't throw an error. Like, when is it *not* a bug?\n\n> defining the same function twice doesn't throw an error. Like, when is it not a bug?\n\nOne specific case I can think of is when you want to overwrite an inherited class method.\n\n> clumsy lambda syntax \n\nwhat do you mean?\n\n> gotta do itertools.chain(i1,i2). Why not i1+i2?\n\nYou can do that, for iterables where that makes sense, like lists. But you don't want all iterables to behave like that (see numpy arrays). It makes sense to have a function which performs that explicit task for any iterable.\n\n+1 on logging module, actually the most obtuse and least pythonic module\n\nThat's a special case that can be easily treated differently.\n\nSame function, different language:\n\nJavascript:\n\n    (x) => x + 1\n\nJulia:\n\n    x -> x + 1\n\n\nPython:\n\n    lambda x: x + 1\n\nIt's a small difference, but it's noticeable.\n\nI meant for actual iterators, like `iter(a) + iter(b)`\n\nJulia's lambdas are so beautiful. Probably my favorite language.\n\nI've been (unfortunately) writing a ton of JS at work lately, and this is one of the only things I miss when I come back to Python.\n\nYou have inspired me to add this to my iterator convenience library [f_it](https://pypi.org/project/f-it/).", "id": "g3tj3nn", "owner_tier": 0.5, "score": 0.4999999998387097}, {"content": "GIL and intentionally gimping lambdas. Actually, any instances where a feature is intentionally gimped for ideological reasons is a flaw.\n\nHow are Python's lambdas 'gimped'? Not disagreeing, just not sure what you mean.\n\nPython\u2019s lambdas are not full anonymous functions - they can only contain a single expression.\n\nPersonally I\u2018m starting to think this is a good thing, because it makes it impossible to write complex Python code like [this JavaScript \u201crun\u201d function](https://github.com/getify/You-Dont-Know-JS/blob/1st-ed/async%20%26%20performance/ch4.md#promise-aware-generator-runner).\n\nYou will never ever prevent people from writing bad code by limiting the language.", "id": "g3toxm7", "owner_tier": 0.7, "score": 0.1451612901612903}, {"content": "Not having good ABCs/ interfaces for shared functionality, and therefore needing free functions cluttering the global namespace which may or may not work; then when you want to implement your own class with that interface, you have to write your own methods on the class anyway, just without any guidance on what the right dunder methods are, and then force users to guess that they can call the free function on it because it doesn't show up in the API.\n\nCan this be treated like a file? Who knows!\n\nPost-hoc ABCs for e.g. sequences are being added but it's a long way behind the curve.", "id": "g3tsx97", "owner_tier": 0.9, "score": 0.03225806435483871}, {"content": "The ability to use a variable which is stated at the bottom of the source code tobuse it at the top of the source code", "id": "g3vb4c6", "owner_tier": 0.1, "score": 0.016129032096774194}, {"content": "I am perennially annoyed by the decision to have the lamda argument to .sort() take one object and return a scalar or a tuple of scalars. Why this lambda cannot be written to take two objects and return a boolean indicating whether the first should be sorted before than the second (i.e. as comparators in C++ and Java work) is beyond my understanding.\n\nYou can define a cmp function and use `functools.cmp_to_key`.\n\nYou can also create a class and implement `__lt__`, `__gt__` methods.\n\nThis was supported in Python 2 (`sort` accepted a `cmp` parameter), but removed in Python 3. I\u2019m not sure exactly why.\n\nThere are a couple things wrong here:\n\n- `list.sort()` doesn't take lambda argument, it takes a `key=` argument which can be any callables, lambda is just one type of callable\n\n- the `key=` function doesn't have to return scalars or tuple of scalars, it can return any comparable objects\n\n> Why this lambda cannot be written to take two objects and return a boolean indicating whether the first should be sorted before than the second\n\nPython 2's `.sort()` used to take a comparator `cmp=`, but it was removed in Python 3. The `key=` argument implements a technique that used to be very common, called Decorate-Sort-Undecorate or Schwartzian Transform.\n\nThe comparator mechanism was removed to remove the conflict between comparisons using `__cmp__` and how it relates to other rich arithmetic comparators like `__eq__`, `__lt__`, `__le__`,  `__rt__`, `__re__`.\n\nIt is a lot easier to write a `key=` function correctly than to write a `cmp=` function, and it is a lot easier to reuse existing functions as `key=` compared to writing `cmp=` which almost always will have to be custom written. For example, to do case insensitive comparison, you can simply do `.sort(key=str.lower)`.\n\nIt's also quite easy to accidentally write a cmp method that doesn't satisfy [total ordering](https://en.m.wikipedia.org/wiki/Total_order), which is usually what you want 95% of the time you sort something. \n\nAnd since you can easily convert a `cmp` method with `cmp_to_key()`, there's really no more actual need to support `cmp=`. Personally, in the last ten years or so of coding nearly everyday, I never really felt the need for the old `cmp=`, it may be different to you, but I'd never seen a use case where sorting with comparator will make a better code.\n\nI knew about __lt__ and __gt__, kind of cool to know that functools has a cmpt_to_key function, I'll have to explore how that works.\n\nThanks!", "id": "g3wdjub", "owner_tier": 0.3, "score": 0.1290322579032258}, {"content": "I think that the lacks of proper type checking is what always makes it difficult to scale with more hands on the code base", "id": "g4mr6i9", "owner_tier": 0.1, "score": 0.016129032096774194}, {"content": "significant whitespace\n\n*ducks and hides*\n\n``from __future__ import braces``", "id": "g3tse0l", "owner_tier": 0.5, "score": -1.6129032258064515e-10}], "link": "https://www.reddit.com/r/Python/comments/iloezk/what_do_you_think_are_pythons_design_mistakes/", "question": {"content": "I'm curious to know what do you think are Python's design mistakes?\n\nI'm not looking to trash Python - I've been using it for 25 years almost on a daily basis and love it. I also know that Python started way back when the computation landscape was different, and that hindsight is 20/20. But humor me, if you can change something in Python - what will it be and why?\n\nI'll start with implicit variable declaration (lack of var/let), which leads to:\n- Bugs that are hard to catch automatically: Is that a typo or did you mean to create a new variable here?\n- Lack of proper lexical scoping and oddities such as \"global\" and \"nolocal\"\n\nI'd love to hear your thoughts.\n(What triggered this question is [this tweet](https://twitter.com/dabeaz/status/1300949834514530304) by David Beazley)", "id": "iloezk", "title": "What do you think are Python's design mistakes?", "traffic_rate": 207.942496260595}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "    random.shuffle(array)\r\n\r\nOk, ok, seriously, yeah. You use Knuth's shuffle:\r\n\r\n    for i in range(len(array)):\r\n        s = random.randint(i, len(array) - 1)\r\n        array[i], array[s] = array[s], array[i]\r\n\r\nThis is a well-studied and well documented problem, and it's not as easy as it looks to get a well-shuffled result.\r\n\r\nMoral of the story: Studied algorithms are almost always better than yours. ;)\n\n> This is a well-studied and well documented problem, and it's not as easy as it looks to get a well-shuffled result.\n\nrandom.shuffle was added to Python's standard library after we noticed that the following scenarios occurred regularly on comp.lang.python:\n\n> Q. How do I shuffle a list?\n\n> A. Here's how to do it: <broken solution>\n\nor\n\n> Q. How do I shuffle a list?\n\n> A1. Here's how to do it: <correct solution>\n\n> A2. No, that one's biased, here's the right way: <broken solution>\n\n>   array = random.shuffle(array)\n>\n> Ok, ok, seriously, yeah.\n\nNo, that's the correct answer -- you use a library function.\n\nIf a library function doesn't exist to do what you need, *then* you get out your copy of TAOCP, look up a suitable algorithm, write the code, and test it.\n\n\nTwo names to trust in the shuffle business: Fisher-Yates!  Think of Fisher-Price.  What's price about?  Lots of money.  And who has lots of money?  Bill Gates.  And what sounds like Gates?  Yates!  Fisher-Yates.\n\nOr you be boring and go for [Knuth](http://en.wikipedia.org/wiki/Knuth_shuffle).  It's the same thing, but it doesn't sound as cool.\n\nIt wasn't very clear in the article but that's the same as the \"correct\" solution given.\n\n> array = random.shuffle(array)\n\nrandom.shuffle modifies the list in place, it returns None :-)\n\nI've said it before, and I'll say it again.  [Don Knuth is my homeboy.](http://geekz.co.uk/shop/store/show/knuth-tshirt)\n\nSadly, this is exactly what is wrong with the practice of programming. If one has the mental ability to write a program that's worth something, then he/she most certainly has the ability to learn to use elementary formal methods to check their work.\n\nWhy don't we then? Because too many people cry \"boohoo, this is too hard.. I can't do it\" when they really mean \"I can do this, but I'm just lazy\"\n\nNote, I'm not blaming the lazy folks (I'm one of them). I'm blaming the faulty education practices of making too little demands.\n\nThat's pretty funny; I'd have thought there'd be someone who'd just give a wikipedia link to Knuth's shuffle and be done with it. It's really not difficult. (I did it from memory, and I don't think I'd ever actually implemented it before. The actual implementation is trivial.)\n\nWell, right. And then the interviewer goes \"pretend that random.shuffle doesn't exist\", and then you say, \"I'd use a Fisher-Yates shuffle (a Knuth shuffle)\", and write it out. (Or, if you don't know it offhand, you say \"I'd pull out my copy of TAOCP for the actual implementation.\")\n\nStill, it's always nice to understand how a library works!", "id": "c05ji5v", "owner_tier": 0.5, "score": 0.9999999999350648}, {"content": "sure, just toss the array in the air, let it fall on the floor, and pick it up the elements randomly.\n\nWell, that used to work, but ever since memory shrunk to the point where you could fit more than about 64KB on a stick, it's gotten really hard to see where the elements land since they're so small. Tossing memory elements from a modern 2GB stick is like tossing dust in the air, and the expense of getting illegal immigrants to cut it apart in the first place is pretty significant, you know; it's a lot of cutting! That's a mi$take you'll only make once.\n\nThat's why I like to keep some 16KB sticks around, it makes this algorithm a lot easier, and you'll never need an array that takes more than 16KB shuffled anyhow.\n\nThat's basically what I do. This whole shuffling in place thing is silly.", "id": "c05jm5l", "owner_tier": 0.3, "score": 0.20129870123376623}, {"content": "I generate an array of 52 random numbers alongside my deck, then I jointly sort them. (I resolve conflicts by generating another pair of numbers to determine which of the pair is higher).\n\nDecorate sort undecorate, basically.\n\nThe problem there is that sorting is (at best) O(n log(n)), whereas the Fisher-Yates shuffle is O(n).\n\nYour parlance doesn't seem universal to me. Could you rephrase that in more standard terminology?\n\nno, it's O(52). Are we talking about arbitrary sized decks of cards? I've never had to deal with them for any actual application..\n\nAside from that, the rand() function is so massively expensive compared to anything else done in those algorithms that we'd need decks in the millions of cards range before rand would stop dominating the profile.\n\nThe bigger problem is that without the conflict resolution specifics, the shuffle is biased.\n\nThese specifics alone make the algo more complex and error-prone to implement than Fisher-Yates, imo.\n\n\n> Decorate-sort-undecorate, a programming design pattern, also known as the Schwartzian transform\n\nhttp://en.wikipedia.org/wiki/Schwartzian_transform\n\n> The Schwartzian Transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items.\n\nEdit: It's also common in the Python community\n\nRTFA... hell, even just the title. It's not about shuffling a deck of cards it's about shuffling arrays in general.  Deck of cards is just the canonical example.\n\nI don't know why so many agree with you.  I'm with phil_g on this. It's O(n log(n)).\n\nIf you're just doing a card game fine; the difference is negligible in that specific case.\n\nNo. That's not how Big O Notation works:\n\nhttp://en.wikipedia.org/wiki/Big_o_notation\n\nBig O Notation is the study of the dominant terms in the time equation t(n) where n is some measure of the size of the input. Your implementation for shuffling an array is, as mentioned above, O(n*log(n)), which is the general order of sorts. \n\nIf you are attempting to say that your sort would complete in under 52 units of time in the given case, you are also wrong because, as mentioned the worst case time for sorting an array is n*log(n), which means your algorithm will finish in under 89 units of time, compared to 52 units of time for the O(n) Fisher-Yates algorithm.\n\nAh, thank-you. It confused me because I couldn't see the relationship to python's decorators :-)\n\nRTFC. My comment was specifically about a deck of cards. (And the array size has to get into the billions before the sort time gets close to the rand() time)", "id": "c05jmo1", "owner_tier": 0.5, "score": 0.17532467525974024}, {"content": "This was one of the first homework assignments in my Intro CS class, and yes, the professor did go into why the obvious solution doesn't work.", "id": "c05jw9v", "owner_tier": 0.7, "score": 0.07792207785714285}, {"content": "Huh. I've been making an array of random ints and sorting the array with those random ints as keys. Thorough, but much less efficient. I didn't realize there was a simple and much more efficient solution.\n\n[deleted]\n\nMy old method was correct. It gives a fairly even spread of orders.\n\n[deleted]\n\nYou're so full of shit. I just tested it. The distribution is even, within a fraction of a percent. It's just inefficient, which is bad but not as bad as being incorrect.\n\nC# test code I used:\n\n    using System;\n    using System.Collections.Generic;\n    using System.Text;\n\n    namespace shuffletest\n    {\n        class Program\n        {\n            static Random r = new Random();\n            const int MAX = 1000000;\n            private static bool ArrayEqual(int[] a, int[] b)\n            {\n                if (a.Length != b.Length)\n                {\n                    return false;\n                }\n                else\n                {\n                    for (int i = 0; i < a.Length; i++)\n                    {\n                        if (a[i] != b[i])\n                        {\n                            return false;\n                        }\n                    }\n                }\n                return true;\n            }\n            private static void Shuffle(int[] a)\n            {\n                int[] b = new int[a.Length];\n                for (int i = 0; i < b.Length; i++)\n                {\n                    b[i] = r.Next();\n                }\n                Array.Sort(b, a);\n            }\n            static void Main(string[] args)\n            {\n                int[][] basic = new int[][] { new int[] { 0, 1, 2 },\n                                              new int[] { 0, 2, 1 },\n                                              new int[] { 1, 0, 2 },\n                                              new int[] { 1, 2, 0 },\n                                              new int[] { 2, 0, 1 },\n                                              new int[] { 2, 1, 0 }};\n                int[] counts = new int[] { 0, 0, 0, 0, 0, 0 };\n                int[] shuffle;\n                for (int i = 0; i < MAX; i++)\n                {\n                    shuffle = basic[0].Clone() as int[];\n                    Shuffle(shuffle);\n                    for (int k = 0; k < counts.Length; k++)\n                    {\n                        if (ArrayEqual(shuffle, basic[k]))\n                        {\n                            counts[k]++;\n                        }\n                    }\n                }\n                foreach (int c in counts)\n                {\n                    Console.WriteLine(c);\n                }\n                Console.ReadLine();\n            }\n        }\n    }\n\n\n[deleted]\n\nOK OK I just read Oleg's paper.\n\nYeah my system is wrong and only works in practice for arrays that have less than 4 billion members.\n\nDouchebag.\n\nYour algorithm is computationally inefficient; it will *never* produce unbiased results; and it completely breaks down with only **13** items.\n\nYou betray a misunderstanding of the intricacies of these algorithms.  That's okay.  Stubbornly defending your code when presented with your ignorance, however, is not.\n\nTake a step back and actually learn how Fisher-Yates works.  Understand its nuances.  You'll be better off for it.\n\nMy ignorance? My misunderstanding? What a douchebag.\n\nI just did some calculations, and it turns out that you are also full of shit. In fact, I have no idea how you come to these conclusions.\n\nAt 13 items, the probability of having no duplicate keys is over 99.99999%. This is not a noticeable problem with an algorithm that is supposed to give a random ordering. If you want to know the actual way to calculate that, look up the birthday paradox. At 6000 items, the odds of having a duplicate key are still under 0.5%. Such a bias, if it occurs, will be **unpredictable** in where occurs.\n\nWhat's cute is that I think the Knuth shuffle is neat and I intend to use it from now on. I freely admit that it is better. There is no doubt that it is a lot more efficient. I said so from the start. But you are seriously a complete dumbass if you think bias with this other algorithm would be a problem.\n\n[deleted]", "id": "c05jpls", "owner_tier": 0.5, "score": 0.05844155837662337}, {"content": "Why bother trying to shuffle an array in place?\r\n\r\nJust copy the array into a mutable list, then refill the original array with items randomly selected from said list.\r\n\r\nIt trivial to show this is random (well, as random as your random number generator), hard to screw up, and easy to generalize for any collection.\n\nhow do you plan to \"randomly select\" items from the copied list?\n\nJust like grabbing marbles from a sack.\r\n\r\n    while oldList.length > 0\r\n        index = Random(0, oldList.length-1)\r\n        newList.Add oldList(index)\r\n        oldList.RemoveAt(index)\n\nThat's exactly what the in-place shuffle does. The newList is the elements from 0 to k-1, and the oldList is the elements from k onward.\n\nOnce you see that it's completely conceptually identical, you sort it in place because it's easier, and it's faster.\n\nThe time complexity of this would be O(n**2), no? If oldList is an array, oldList.RemoveAt(index) would take O(n) time. If it is a linked list oldList(index) would take O(n) time.\n\nCompare to in place algorithm which is O(n) time.\n\nLets see. N to copy the array + n to select each element + n * (n..1) to find and remove elements from a single-linked list. So I think that ends up being roughly O(n Log n), with n^2 as the worse case. Of course you can speed this up by using a doubly-linked list and seeking both directions rather than always starting at 0.\r\n\r\nBut so what?\r\n\r\nWe are talking about randomizing values. Generally speaking you are not going to have a lot of values in your list, nor is speed going to be very important under most circumstances.\r\n\r\nMost programs that randomize values actually have added delays in order to give the user the impression that they are actually being shuffled.\n\nFYI, O(n!) is not \"roughly\" O(n log n), it's O(n^2).\n\nIt would only be n! if you used a singly-linked list and your random number generator always returned the highest possible value.\r\n\r\nAnd again, so what? It isn't like you are going to randomize 10,000 items.\n\nConstant factors don't affect big-O complexity. So for example using a doubly-linked list halves the amount of work, but it's still O(n!). And though the random number generator doesn't always pick the last element, if the RNG is fair the total of the numbers chosen will converge on (n/2)!, which is still O(n!).\n\nI'm not saying it's a bad algorithm -- it has the nice benefit of being purely functional -- but it's still O(n^2). Someone already linked this, but in case you missed it, http://okmij.org/ftp/Haskell/perfect-shuffle.txt shows how to make this algorithm O(n log n) using a binary tree instead of a list.", "id": "c05jv0u", "owner_tier": 0.9, "score": 0.18831168824675323}, {"content": "I am probably way out of my depth here, but I never shuffle the array...\n\no.o I try to generate a random index for the (already in order) array, then pick a new random index out of the remainder.", "id": "c05jx16", "owner_tier": 0.7, "score": 0.07142857136363635}, {"content": "I'm lazy, I tend to to do a random schwartzian transform and sort it.  Shuffle in O(n log n)\n\nThe advantage to this algorithm is that it is functional. The disadvantage is that it is actually slightly biased.\n\nSee oleg for an explanation and a proper perfect functional shuffle (not that I don't tend to use the above-described shuffle for \"good enough\" applications anyway,\"):\n\nhttp://okmij.org/ftp/Haskell/perfect-shuffle.txt\n\nYou call yourself \"Lazy\" for using an O(n lg n) algorithm when an O(n) algorithm would do?  For hand-writing code that exists in a library already?  When did either of these actions become \"lazy\"?\n\nLazy means he does the least work for himself, not the computer.  That means he doesn't care about complexity and won't look something up if he can code it out quickly enough.\n\nDepending on the problem, O(n) usually takes *more* work for the programmer than O(n log n)\n\nJust because it exists in a library doesn't mean it exists in a library I have readily available to me.\n\nThe last time I needed a shuffle, I didn't have a library to do it.  It took thirteen characters to do what I described above.\n\nThat's true, but this isn't one of those problems.\r\n\r\nIt takes [three lines of Python](http://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/c05ji5v).\n\nYou missed the point. It takes three lines of Python because you know Knuth's shuffle and the results behind it.\n\nThe actual *design and analysis* of an algorithm (not just typing it) takes more work..\n\nSure it is.  Ruby implementation (where I most recently did what I described above):\n\n   a.sort_by { rand }\n\n(except I added that to the Array class as \"shuffle\" -- if it somehow performs poorly ever, I'll replace my implementation with a new one)\n\nNote that this is really easy to understand as well.  If you trust your random number generator to be random enough (which the Knuth one also requires) and you trust your sort, then it's pretty obvious what will happen there.\n\nHmm.\r\n\r\nI see your point, but I never had to design or analize this problem. I've got other things to do; that's why I trust Knuth to do this kind of thing for me.\r\n\r\nAnd it's not like this is a gorram \"best path through a weighted cyclic graph including negative weights with other special constraints\" problem. It's a shuffle. You shouldn't be designing your own anything that's this common -- you should be just using your languages implementation, and if your language doesn't have one, then [checking wikipedia](http://en.wikipedia.org/wiki/Shuffling#Implementation_of_shuffling_algorithms).\n\nI must say then when I first designed a shuffle algorithm, the Knuth method came to me immediately as the natural solution.\n", "id": "c05ji8g", "owner_tier": 0.5, "score": 0.27922077915584415}, {"content": "Just checked my [poker server](http://github.com/philluminati/texas_holdem/tree/master/card.c) and I don't think I fell for it. This is my implementation and it is the correct one yeah?\n\n    for (i = 0; i < SHUFFLE_INTENSITY; i++)\n    {\n       a = (rand() % 52);\n       b = (rand() % 52);\n \n       debug (5, \"i=%i,a=%i,b=%i\\n\",i,a,b);\n \n       tmp = deckArray[a];\n       deckArray[a] = deckArray[b];\n       deckArray[b] = tmp;\n    }\n\n\n I don't loop through A and swap with B, I choose two cards evenly and swap them. This is correct isn't it?\n\nNo, that is another *slightly* wrong one.  Take a look at \"Knuth's Shuffle\", linked to in the other comments.\n\nIt is more inefficient than wrong, though.  With Knuth's Shuffle, you swap *n*-1 times where *n* is the size of the list.  With your algorithm, you need to set SHUFFLE_INTENSITY significantly higher than *n* to get a decent shuffle.\n\nTo illustrate, these are the occurences of each permutation after 100000 shuffles on a list of 3 elements:\n\n    PhilShuffle with SHUFFLE_INTESTITY=10 (10 swap operations):\n    [0, 1, 2] : 16524\n    [2, 0, 1] : 16608\n    [1, 2, 0] : 16763\n    [0, 2, 1] : 16657\n    [2, 1, 0] : 16598\n    [1, 0, 2] : 16850\n    PhilShuffle with SHUFFLE_INTESTITY=3 (3 swap operations):\n    [0, 1, 2] : 18618\n    [2, 0, 1] : 14669\n    [1, 2, 0] : 14969\n    [0, 2, 1] : 17231\n    [2, 1, 0] : 17374\n    [1, 0, 2] : 17139\n    Knuth's Shuffle (2 swap operations):\n    [0, 1, 2] : 16604\n    [2, 0, 1] : 16571\n    [1, 2, 0] : 16570\n    [0, 2, 1] : 16782\n    [2, 1, 0] : 16891\n    [1, 0, 2] : 16582\n\n\nIn addition to what Deestan mentioned about the algorithm, there are two other quirks as well.\n\nFirst, using rand() is going to limit the number of distinct permutations to the period of the random number generator.  You should use an algorithm with a period that is significantly higher than 52!.\n\nSecond, most PRNG's return a value in a domain of [0..2^n).  Blindly taking that value modulo 52 can introduce bias.  Consider the simple scenario of [0..63]:\n\n[0..51] : 1/64 for each card\n\n[52..63] : 1/64 for the first 12 cards\n\nI don't think it's correct. If your SHUFFLE_INTENSITY isn't high enough, there's a high probability that many cards just won't move.\n\nI think you'll get a better result if you set SHUFFLE_INTENSITY to 52, and set a to i, so you'll swap every card in the deck with some card (possibly itself) at least once.\n\nThanks, I think I understand. My technique is unbiased but capable of picking up the same card, or swapping the same two cards multiple times, thus requiring a shuffle intensity well above 52. (In the code I have it set to 3000. *well, it's C ;-)*) Thus it is simply inefficient since Knuth can do the same job in 52 operations exactly.\n\nSounds like someone didn't read the article.\n\nNope, still biased.\n\nA card is more likely to end up in the position in started in than any other position.\n\nYou can do the same combinatorical proof that it's biased.\n\nFirst of all your shuffle is biased - it does not return each possible shuffled deck with the same probability.\n\nSecondly, the use of a poor shuffling algorithm [has already been used to attack a poker server](http://www.cigital.com/papers/download/developer_gambling.php). Ironically, in that case the developers published their shuffling algorithm to demonstrate that it was fair. The attack consisted of two parts: first that the shuffle was not fair, and secondly that they used an off-the-shelf RNG.\n\nI have realised it is bias, when I looked back at Deestan's post.\n\n> PhilShuffle with SHUFFLE_INTESTITY=3 (3 swap operations):\n\n> [0, 1, 2] : 18618 <- Too many.\n\n> [2, 0, 1] : 14669\n\n> [1, 2, 0] : 14969\n\nI've made the change now to Knuth (Fisher-Yates) algorithm. That is an interesting link, thank you. It really isn't a trivial matter.\n\nI'm curious how you didn't end up with any [0, 2, 1]s and friends.\n\nThat seems odd.\n\nLol, that is a snippet from [Deestan's comment](http://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/c05jldm).\n\n", "id": "c05jlbq", "owner_tier": 0.7, "score": 0.28571428564935064}, {"content": "My naive fix would have been to create an empty tree-map of index to item, iterate the array, assign a random index until I found one not yet defined in the map, insert the item into the map, and then iterate the map ordered by index to re-fill the array.\n\nNowhere near as efficient, but it would have been correct.\n\nEdit: a cute improvement would be to use much wider indexes in the map. Then collisions would be unlikely. After all, we only care about the *order*. The iterator implies the real index.", "id": "c05jnas", "owner_tier": 0.5, "score": 0.05844155837662337}, {"content": "An important note, since he mentioned Poker Servers:  For a serious poker server, you should *not* use computer random number generators.\n\n(Edit: substitute \"computer random number generators\" for \"standard library pseudorandom number generators indiscriminately\" in the previous sentence.  Thanks, theeth.)\n\nThis is because the number of possible permutation results are capped by the number of possible random seeds.  I.e. a standard 32-bit integer has 4294967296 possible values, while a deck of 52 cards has 52! = 80658175170943878571660636856403766975289505440883277824000000000000 possible permutations.  \n\nThis means that *no matter how clever your shuffling algorithm is*, there are umzillions of permutations it will never generate.\n\nAs an example, I tried making a Python script to shuffle a list of 52 cards via the standard Python shuffle(), draw 4 cards, and try to guess the 5th.  With a perfect shuffle, it should have a success rate of 1/48.  My script had a success rate somewhere around 1/20.\n\n> For a serious poker server, you should not use computer random number generators.\n\nThat's ludicrous.\n\nThis problem with seeding only appears because you're limiting yourself (or rather, the library is) to setting an infinitely small portion of the initial state of the algorithm.\n\nFor example, to fully seed Mersenne Twister (the algo used in Python), you'd have to provide 624 different 32bit ints, letting you position yourself anywhere along the 2^19937 \u2212 1 (more than 10^6001) period.\n\nIf you really want to push further, you could go for George Marsaglia relatively simple multiple with carry algorithm (period of 10^39460, seed of 131072 bits).\n\nNo matter how clever you think you are, if you don't understand how the algorithms you're using works, you're bound to make silly assumptions.\n\nI wonder what's the current state of software random number generation. While studying the Montecarlo method, I remember reading about specialized hardware cards with radioactive isotopes that guarantees a true random number always.\n\n[deleted]\n\n> That's ludicrous.\n\nAch, indeed it is.  Thanks for the explanation.  I had the [GNU C RNG](http://www.delorie.com/gnu/docs/glibc/libc_396.html) in mind when I tested, Python was just used for convenience.\n\n[Random.org](http://www.random.org/integers/) creates numbers from atmospheric noise (you can subscribe to them and get a sort of 'random number service'), and I've also heard of creating numbers from the background radiation of the universe. \n\nInteresting stuff, in my opinion. \n\nIt was just a throwaway script I lost ages ago, but here's a *pseudocode* version showing the possible candidates for 5th card:\n\n    import random\n    given_hand = [47, 34, 27, 3] # seed(1) - next is 7\n    candidates = {}\n    init_list = range(52)\n    for i in xrange(2**32):\n        random.seed(i)\n        test_list = init_list[:]\n        random.shuffle(test_list)\n        if test_list[:4] == given_hand:\n            candidates[test_list[5]] = 1\n    print candidates.keys()\n\nNote:\n\n * It is horribly inefficient and runs stupid slow.\n * The Python random module seed is not really limited to 32 bits.  This was only written to give actual data of how the probability would be biased with a 32 bit seed.\n\nWebcam + Lava Lamp.\n\n[deleted]\n\nYes, it's not really feasible to attack anything brute-force like this.\n\nThough with some clever coding and a more heuristic approach, you could slightly improve your poker odds in real-time even with a normal desktop computer.", "id": "c05jlyd", "owner_tier": 0.7, "score": 0.3506493505844156}, {"content": "For a class practical, I used the first method outlined in the article. The 2nd way never occurred to me. Still shuffled the array rather well.\n\nJust to illustrate the bias (I *had* to test this with code), both shuffle techniques performed 100000 times on the initial list [1, 2, 3].\n\n    <function stupidShuffle at 0x0232E570> occurences:\n    [0, 1, 2] : 22302\n    [2, 0, 1] : 11198\n    [1, 2, 0] : 21875\n    [0, 2, 1] : 10988\n    [2, 1, 0] : 11282\n    [1, 0, 2] : 22355\n    ----------------------------------------\n    <function cleverShuffle at 0x0232E530> occurences:\n    [0, 1, 2] : 16665\n    [2, 0, 1] : 16754\n    [1, 2, 0] : 16728\n    [0, 2, 1] : 16542\n    [2, 1, 0] : 16707\n    [1, 0, 2] : 16604\n\n\nNo it didn't. I'm with you, I also used the wrong solution before I knew better, but the harsh truth is simply that it doesn't shuffle correctly. (Note that the correct solution is not the slightest bit more complicated, so a \"good enough\" argument doesn't hold up either.)\n\nThe simple way of seeing that it will be biased is to think of what happens when you shuffle an array of size 3.\n\nYou make 3 calls to the random function, each returning one of 3 possible values, so you have 9 different outcomes, each one is equally valid.\n\nHowever, when there are only 3 items, there are 3 * 2 * 1 = 6 different ways of shuffling them.\n\nThis means that of the 9 equally valid outcomes from that shuffling algorithm, some must be more likely than others.\n\nIt is funny how we are often more convinced with a sample of 100000, rather than just doing an exact brute force \"every possible shuffle\" method (where in this case, the number of different shuffles is quite small).\n\nSame thing with the Monty Hall problem - to many people it is more convincing to see the results of lots of runs, even though the number of different combinations is tiny.\n\nHmmm, very interesting. I'll have to read this article a bit more closely. I only had a cursory glance at the \"clever shuffle\" algorithm when I initially followed the link. Thanks for the trouble you went to. It's appreciated.\n\n> You make 3 calls to the random function, each returning one of 3 possible values, so you have 9 different outcomes\n\nThat would be 18 different outcomes.\n\nGah, actually, it would be 3 * 3 * 3 = 27 possible outcomes. 6 doesn't divide 27 though, so the argument still stands. (if it was 18 different outcomes, then the shuffle could be unbiased)\n\nOh, of course. I'll second that \"gah\"...", "id": "c05jjci", "owner_tier": 0.9, "score": 0.3246753246103896}, {"content": "I bet I could shuffle 100 arrays.", "id": "c05jkn0", "owner_tier": 0.5, "score": -6.493506454042397e-11}], "link": "https://www.reddit.com/r/programming/comments/72yhu/do_you_know_how_to_shuffle_an_array/", "question": {"content": "", "id": "72yhu", "title": "Do you know how to shuffle an array?", "traffic_rate": 935.2064958283671}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "There is also a video of the talk, recorded at ICFP 2009 in Edinburgh: http://www.vimeo.com/6624203\n\nThanks; this gets a little hard to follow in the middle with just the slides!", "id": "c0kbseo", "owner_tier": 0.1, "score": 0.23728813550847458}, {"content": "I want to downvote on principle for using \"considered harmful\", but the content is actually pretty good.\n\nGuy Steele is a master, and the masters know when they can get away with breaking the rules.\n\n''considered harmful' considered harmful' considered harmful\n\n_Slightly_ harmful! That's a different thing!\n\nGuy Steele can slam a revolving door.\n\nGeneralising, \n\n    let rec considered_harmful n =\n      if n = 1 then\n        \"considered harmful\"\n      else\n        Printf.sprintf \"'%s' considered harmful\" (considered_harmful (n - 1))\n\nI considered an infinite lazy version, but unfortunately it is left recursive.\n\nMostly harmless?\n\nMy take on it is that foldr/foldl are inheritently bad, but rather the problem is in linked lists. \n\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''...\n\n;-)\n\n    let considered_harmful n =\n      List.fold_left\n        (fun s _ -> sprintf \"'%s' considered harmful\" s)\n        \"considered harmful\"\n        (List.range n)\n\nYou meant this one.  It's tail-recursive *and* ironic. :)\n\nAnd for the nit-pickers yes List.range isn't part of the standard library so suppose this was at the top:\n\n    module List = struct\n      include List\n      let range n =                                     \n        let rec loop accum = function\n          | 0 -> List.rev accum\n          | i -> (loop ((n-i)::accum) (i-1))        \n        in\n        loop [] n\n    end\n\n\nNo, the problem is that foldr and foldl inherently promote a sequential order, which is bad (for parallelisation). Whether you do it on a linked list, an array or a tree the problem is the same: There's a linear data dependency. reduce avoids this problem by assuming associativity and thus giving you much more freedom.\n\n(Of course, linked lists are also generally a problem for similar reasons)", "id": "c0kbinl", "owner_tier": 0.5, "score": 0.9067796609322034}, {"content": "Thanks for using Google Docs!\n\nWhy is Google Docs considered good?  I was actually thinking the opposite.  Google Docs gave up downloading the images for me about halfway through\u2026\n\nBetter than fucking scribd\n\nThere's a PDF [download link](http://research.sun.com/projects/plrg/Publications/ICFPAugust2009Steele.pdf) right at the top, if you like.\n\nI like the fact that you don't need any additional software installed to read a PDF, and those who are using Adobe Acrobat are protected from file using an unpatched security exploit.\n\nGoogle docs considered.... harmful? \u0ca0_\u0ca0\n\nand it's blocked by our proxy as online file storage.\n\n> Why is Google Docs considered good? \n\nI second this question. A simple S5 presentation would have worked perfectly. A PDF would have been ok with a [pdf] tag. This thing good isn't.\n\nThere are extensions for pretty much every browser that needs it allowing users to view PDF files in Google Docs without visiting the original document.  There are no such things for allowing users to view shitty Google Docs-converted PDFs in their original form without having to go to Google.\n\nTherefore, linking to shitty, low-quality, poorly-anti-aliased Google Docs conversions should **never** become common practice.  It only makes those of us who have our shit together (using a decent PDF viewer, or OS X, which has these features built in) have to suffer for the idiots who can\u2019t click an \u201cInstall Extension\u201d button.\n\nI'm pretty sure this was sarcastic.\n\nThat's like saying being shanked is better than being burned alive. It might be true, but...", "id": "c0kbg4b", "owner_tier": 0.9, "score": 0.9999999999152543}, {"content": "I particularly like slide 33 - I've never seen Mergesort and Quicksort compared quite that way.\n\nThere are more analogies like that. \n\nFor example you can generate permutations by selection, where you select arbitrary element from list and cons it to permuted rest or take head and insert it into permuted tail. In prolog insert and select is the same predicate, just with different modes:\n\n    % For insert you call as (+,+,-), for select call as (-,-,+). \n    select(X,T,[X|T]).\n    select(X, [H|T], [H|S]) :- select(X, T, S).\n\nAnd now:\n    % (+,-)\n    permute_select([],[]).\n    permute_select(List, [Head|PermutedTail]) :-\n        select(Head, Tail, List).\n        permute_select(Tail, PermutedTail).\n\n    % (-,+)\n    permute_insert([],[]).\n    permute_insert(Permuted, [Head|Tail]) :- \n        permute_insert(PermutedTail, Tail);\n        select(Head, PermutedTail, Permuted).\n\nIt's pretty easy to take it further and prove that insert sort and selection sort are actually the same.\n\nYou can find similar analogy with graph traversal. You can write DFS and BFS as the same algoritm with the only difference in the accumulator data structure. For DFS it's FIFO and for BFS it's LIFO queue.\n\nI like 72\n\n> The programming idioms that have become second nature to us as everyday tools DON\u2019T WORK.\n\n> You can find similar analogy with graph traversal. You can write DFS \n> and BFS as the same algoritm with the only difference in the \n> accumulator data structure. For DFS it's FIFO and for BFS it's LIFO \n> queue.\n\nHere's an implementation of that idea for binary trees (not general graphs) in Java, with a module hierarchy diagram and code set side-by-side (it's a little wide: sorry about that):\n\n[BFS and DFS traversal code](http://www.willamette.edu/~fruehr/241/samples/traversal/traversalFrames.html)\n\nThe implementation is a bit baroque; this was done for a beginning data structures class which was taught in Java. (Also note that the diagram may not be anything like standard UML or other such notations, except by perhaps accident.)", "id": "c0kbqsu", "owner_tier": 0.5, "score": 0.1186440677118644}, {"content": "This is true. I once used `foldr` and almost lost a thumb. \n\nNow how many thumbs do you have?\n\nHe lost a lot of blood, but luckily, he found most of it.\n\nalmost less than 2\n\nMore than the average person.\n\n(2>1.999)\n\nOne less than he did.\n\nI know the average person, and he has two thumbs like most everyone else\n\nAlmost.\n\nThe average person has around 1.99 thumbs, 0.98 testicle and 0.49 penis.\n\n[deleted]\n\nWrong, that's the median or the modal person.  The average person does not exist, because they would need to have 1.99 thumbs.", "id": "c0kbifx", "owner_tier": 0.9, "score": 0.4745762711016949}, {"content": "Brilliant, as expected from Guy Steele.", "id": "c0kbka9", "owner_tier": 0.9, "score": 0.04237288127118644}, {"content": "I read this topic title, and had no idea what it meant.\r\n\r\nI clicked this link, to figure out the topic title, but had no idea what I was looking at.\r\n\r\nI came to these comments, to figure out what I had been looking at, but still have no idea.\r\n\r\n*sob*\n\nI can't explain the whole presentation, but `foldl` and `foldr` are functions that take a list and produce a single value out of it, by combining all the elements together in a particular way.\n\nThey do this by repeatedly applying a function or operator that can combine only two elements together.  By repeatedly applying the two-argument function, they can handle all the items in the list.\n\nThey usually define a value that would correspond to an empty list.  (For example, the sum of the empty list is zero.  Or the product of the empty list is 1.)\n\nFor example, say you want to find the sum of these integers:\n\n    1 2 3 4 5\n\nFolding would involve choosing an addition function (I'll call it `add(a,b)`) to be the function used to combine pairs together, starting at one end and moving toward the other.  The steps might resemble this:\n\n    1 2 3 4 5\n    -> add(1, 2) 3 4 5\n    -> 3 3 4 5\n    -> add(3, 3) 4 5\n    -> 6 4 5\n    -> add(6, 4) 5\n    -> 10 5\n    -> add(10, 5)\n    -> 15\n\nOr, if you want to work from the other end, like this:\n\n    1 2 3 4 5\n    -> 1 2 3 add(4, 5)\n    -> 1 2 3 9\n    -> 1 2 add(3, 9)\n    -> 1 2 12\n    -> 1 add(2, 12)\n    -> 1 14\n    -> add(1, 14)\n    -> 15\n\nThe `foldl` (\"fold left\") and `foldr` (\"fold right\") functions work from different ends of the list.  They are usually defined recursively, and the examples above don't exactly mirror which operations go on in what order and which recursive calls are made.  They're just meant to demonstrate that you're working from one end of the list only, which makes your processing very serial.\n\nFor what it's worth, `foldl` and `foldr` come in very handy.  They aren't just used for summing lists of integers.  If you have a whole bunch of sets and you want the intersection of all of them (say each set represents the results of a single search criterion, and you want the result of searching for all the criteria together), and if you had a function which takes the intersection of two sets, you could use a fold function to make it trivial work to generalize it to into taking the intersection of N sets.\n\nAnyway, the relevance of all this is that folding is extremely convenient (it can turn any two-argument, associative operator into an N-argument operator), but it doesn't parallelize well at all, because it just attacking the list from one end, doing one combination at once.  So even though it was a very successful programming idea, something more is needed for parallel programming.\n\nI thought I was a fairly good programmer, and I have no clue what this means. I share your pain", "id": "c0kc9vq", "owner_tier": 0.5, "score": 0.18644067788135593}, {"content": "> Just derive a weak right inverse function and then apply the\nThird Homomorphism Theorem. See\u2014it\u2019s easy!\n\n...\n\nA link to the paper mentioned on this slide is [here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.5695) if anyone is interested. Another paper proving the Third Homomorphism Theorem is [here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.2247). It's not really very difficult--Neighborhood of Infinity posts are usually more complex.", "id": "c0kbkj1", "owner_tier": 0.7, "score": 0.07627118635593219}, {"content": "Nice presentation.\n\nWhat he calls conjugate transforms is also known as tupling in the program calculation literature.\n\nA useful example everyone likely knows is the bombastically named Schwartzian transform. You want to sort a list by some feature `f x` of each datum `x` without constantly recomputing `f x`. A way to do it is `map fst . sortBy snd . map (\\\\x -> (x, f x))`. If `f :: A -> B` then in Steele's terminology `T = [A]` is the original data type and `U = [(A, B)]` is the enriched data type used in the intermediate steps.\n\nAnother example is computing Fibonacci numbers in linear time. Then `T = [Integer]` and `U = [(Integer, Integer)]`. Using `U` we may compute the sequence by iterating `\\\\(x, y) -> (y, x+y)` starting with `(0, 1)` and finally projecting away the second component of each list element, same as we did in the first example.\n\nI thought I'd also draw attention to this foot note:\n\n> It would be nice to have a simple facility to cache any monoid in a tree. It\u2019s straightforward to cache more than one monoid, because the\ncross-product of two monoids is a monoid. Deforestation of monoid-cached trees may turn out to be an important optimization.\n\n", "id": "c0kd3d9", "owner_tier": 0.5, "score": 0.033898305000000004}, {"content": "For Perl programmers, I'd just like to point out that the Conjugate Transforms idea on slide 41 (if you look at the numbers on the slides themselves; otherwise, slide 42) is essentially the same idea as the [Schwartzian Transform](http://en.wikipedia.org/wiki/Schwartzian_transform).", "id": "c0kclmy", "owner_tier": 0.9, "score": 0.01694915245762712}, {"content": "Wow, nice article. Seeing when it was presented I am also happy that I could read it freely and not on a publisher page like science direct :D", "id": "c0kbotc", "owner_tier": 0.1, "score": 0.008474576186440678}, {"content": "\"If you can construct two sequential versions of a function that\nis a homomorphism on lists, one that operates left-to-right and\none right-to-left, then there is a technique for constructing a\nparallel version automatically.\n\n...\n\nJust derive a weak right inverse function and then apply the\nThird Homomorphism Theorem. See\u2014it\u2019s easy!\"\n\nI'll get right on that...", "id": "c0kc1id", "owner_tier": 0.5, "score": 0.008474576186440678}, {"content": "What did I ever do to be considered harmful?\n\nImpostor!\n\nshhhhhh, this is probably the best chance I'll get at having one of those cleverly relevant names that everyone else seems to get.", "id": "c0kco9s", "owner_tier": 0.5, "score": 0.02542372872881356}, {"content": "\"going forward\" considered harmful. ", "id": "c0kbvfx", "owner_tier": 0.5, "score": -8.47457621968245e-11}, {"content": "Isn't this what nested data parallelism is mostly about?\n\nUsing a fold over an associative operation should actually use mconcat - associativity can then be exploited in interesting parallel ways. Though it might be nice to generalize mconcat to non-lists so that it can work on arrays/et-al.", "id": "c0kc61h", "owner_tier": 0.7, "score": -8.47457621968245e-11}, {"content": "What is the difference between 'delay' and 'work'? \n\nI know that total work = number of steps = time complexity, but I haven't seen 'delay' before. What does it measure?\n\nIn a parallel computation, I'm guessing it is the minimum duration of the computation, given unlimited parallel resources. I.e., how many time units you have to wait for the answer.\n\nTotal work could be a measure of duration, except for the fact that many \"workers\" can complete the same amount of work in less time.\n\nSo, let's say that you have two methods for making a chair.  In one of them one guy spends two units of time making the seat, and then he passes it to another guy, who spends two units of time making the back to fit the seat.  In the other method, the first guy still spends two units of time making the seat, but the other guy is making the back simultaneously; it takes him three units of time to do that, because he doesn't have the seat to match against.\r\n\r\nIn the first method, the total work is four units (first guy for two, second for two), and the delay is also four because it's four units of time from start to finish.\r\n\r\nIn the second method, the total work is five units (first guy for two, second for three), but the delay is only three units because they're working at the same time for most of that.\r\n", "id": "c0kcjuq", "owner_tier": 0.3, "score": 0.05084745754237288}], "link": "https://www.reddit.com/r/programming/comments/b0eck/or_foldl_and_foldr_considered_slightly_harmful/", "question": {"content": "", "id": "b0eck", "title": "or, foldl and foldr considered slightly harmful", "traffic_rate": 935.2064958283671}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "Python, #19/10. Glad to see they're getting tougher. Video of me solving at https://www.youtube.com/watch?v=YQjPXSlSelc. Not reading everything up front probably cost me time here. This code solves part 2 (part 1 requires a minor modification).\n\n     from collections import defaultdict\n     lines = open('4.in').read().split('\\n')\n     lines.sort()\n\n     def parseTime(line):\n         words = line.split()\n         date, time = words[0][1:], words[1][:-1]\n         return int(time.split(':')[1])\n\n     C = defaultdict(int)\n     CM = defaultdict(int)\n     guard = None\n     asleep = None\n     for line in lines:\n         if line:\n             time = parseTime(line)\n             if 'begins shift' in line:\n                 guard = int(line.split()[3][1:])\n                 asleep = None\n             elif 'falls asleep' in line:\n                 asleep = time\n             elif 'wakes up' in line:\n                 for t in range(asleep, time):\n                     CM[(guard, t)] += 1\n                     C[guard] += 1\n\n     def argmax(d):\n         best = None\n         for k,v in d.items():\n             if best is None or v > d[best]:\n                 best = k\n         return best\n\n     best_guard, best_min = argmax(CM)\n     print best_guard, best_min\n\n     print best_guard * best_min\n\nNot reading everything is what cost me this time, too. After 30 minutes I gave up and went to bed. Woke up this morning and completed it to find that I spent the previous night solving for part #2 without even knowing what it was.\n\nThank god I never delete the unused code.\n\n>argmax\n\nargmax also works with this \"implementation\":   \ud83d\ude42\n\n    def argmax(d):\n        k = max(d, key=d.get)\n        return key, d[k]\n\nWhat do you think about this suggestion:\n\n    best_guard = max(d, key=d.get)\n    best_min = d[best_guard]\n\n\ud83d\udc4d\n\nI love the videos! Keep it up!\n\nI love how you probably have countless fancy programs and text editors installed but you still use Vim like a real pro! Cool video, and congrats for the result :)\n\nPS: lol @ those 35k unread emails\n\nObviously you're a pro and you know better but I have a feeling that if you used re or other parsing library you would get much quicker. Also have \"from collections import defaultdict\" in a boilerplate file.\n\n> lines = open('4.in').read().split('\\n')\n\nIs that different than:\n\n    lines = open('4.in').readlines()\n\n\n[deleted]\n\nYour code appears to be wrong, at least the very end : you have to first find which guard sleeps the most, in total. That would use your C defaultdict, yet you never use it. Only after finding said guard, you have to find HIS personal favorite minute. How did this code work for you ? \n\nSame here!\n\nThough, it was really easy to change code back to solve part two then xD\n\nNice! Much cleaner", "id": "eb1wb5a", "owner_tier": 0.3, "score": 0.5679012344444444}, {"content": "**AWK**\n\n4.1\n\n    sort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}/falls/{s=$3}/wakes/{for(t=s;t<$3;++t){++zg[g];++zgt[g\",\"t]}}END{for(g in zg)if(zg[g]>zg[og])og=g;for(t=0;t<60;++t)if(zgt[og\",\"t]>zgt[og\",\"ot])ot=t;print og*ot}'\n\n4.2\n\n    sort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}/falls/{s=$3}/wakes/{for(t=s;t<$3;++t)++zgt[g\",\"t]}END{for(gt in zgt)if(zgt[gt]>zgt[ogt])ogt=gt;split(ogt,oa,\",\");print oa[1]*oa[2]}'\n\n\nHorray for awk :)\n\nI tried to indent it a bit nicer so its easier to read:\n\n```awk\nsort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}\n                                    /falls/{s=$3}\n                                    /wakes/{for(t=s;t<$3;++t){\n                                              ++zg[g];++zgt[g\",\"t]\n                                           }\n                                    }\n                                    END{for(g in zg)\n                                          if(zg[g]>zg[og])\n                                            og=g;\n                                        for(t=0;t<60;++t)\n                                          if(zgt[og\",\"t]>zgt[og\",\"ot])\n                                            ot=t;\n                                        print og*ot\n                                    }'\n\n```\n\n```awk\nsort dat.txt | awk -v FS=\"[\\]:# ]\" '/Guard/{g=$7}\n                                    /falls/{s=$3}\n                                    /wakes/{for(t=s;t<$3;++t)\n                                              ++zgt[g\",\"t]\n                                    }\n                                    END{for(gt in zgt)\n                                          if(zgt[gt]>zgt[ogt])\n                                            ogt=gt;\n                                        split(ogt,oa,\",\");\n                                        print oa[1]*oa[2]\n                                    }'\n\n```", "id": "eb24n3b", "owner_tier": 0.3, "score": 0.19753086407407405}, {"content": "Taken from my **[Day 4 Reflections Post](https://github.com/mstksg/advent-of-code-2018/blob/master/reflections.md#day-4)**:\n\n[Haskell] Day 4 was fun because it's something that, on the surface, sounds like it\nrequires a state machine to run through a stateful log and accumulate a bunch\nof time sheets.\n\nHowever, if we think of the log as just a stream of tokens, we can look at at\nit as *parsing* this stream of tokens into time sheets -- no state or mutation\nrequired.\n\nFirst, the types at play:\n\n\ttype Minute = Finite 60\n\n\ttype TimeCard = Map Minute Int\n\n\tdata Time = T { _tYear   :: Integer\n\t              , _tMonth  :: Integer\n\t              , _tDay    :: Integer\n\t              , _tHour   :: Finite 24\n\t              , _tMinute :: Minute\n\t              }\n\t  deriving (Eq, Ord)\n\n\tnewtype Guard = G { _gId :: Int }\n\t  deriving (Eq, Ord)\n\n\tdata Action = AShift Guard\n\t            | ASleep\n\t            | AWake\n\nNote that we have a bunch of \"integer-like\" quantities going on: the\nyear/month/day/hour/minute, the guard ID, and the \"frequency\" in the `TimeCard`\nfrequency map.  Just to help us accidentally not mix things up (like I\npersonally did many times), we'll make them all different types.  A `Minute` is\na `Finite 60` (`Finite 60`, from the *finite-typelits* library, is a type that\nis basically the integers limited from 0 to 59).  Our hours are `Finite 24`.\nOur Guard ID will be a newtype `Guard`, just so we don't accidentally mix it up\nwith other types.\n\nNow, after parsing our input, we have a `Map Time Action`: a map of times to\nactions committed at that time.  The fact that we store it in a `Map` ensures\nthat the log items are ordered and unique.\n\nWe now essentially want to parse a stream of `(Time, Action)` pairs into a `Map\nGuard TimeCard`: A map of `TimeCard`s indexed by the guard that has that time\ncard.\n\nTo do that, we'll use the *parsec* library, which lets us parse over streams of\narbitrary token type.  Our parser type will take a `(Time, Action)` stream:\n\n\timport qualified Text.Parsec as P\n\n\ttype Parser = P.Parsec [(Time, Action)] ()\n\nA `Parser Blah` will be a parser that, given a stream of `(Time, Action)`\npairs, will aggregate them into a value of type `Blah`.\n\nTurning our stream into a `Map Guard TimeCard` is now your standard\nrun-of-the-mill parser combinator program.\n\n\t\n\t-- | We define a nap as an `ASleep` action followed by an `AWake` action.  The\n\t-- result is a list of minutes slept.\n\tnap :: Parser [Minute]\n\tnap = do\n\t    (T _ _ _ _ m0, ASleep) <- P.anyToken\n\t    (T _ _ _ _ m1, AWake ) <- P.anyToken\n\t    pure [m0 .. m1 - 1]     -- we can do this because m0 < m1 always in the\n\t                            --   input data.\n\n\t-- | We define a a guard's shift as a `AShift g` action, followed by\n\t-- \"many\" naps.  The result is a list of minutes slept along with the ID of the\n\t-- guard that slept them.\n\tguardShift :: Parser (Guard, [Minute])\n\tguardShift = do\n\t    (_, AShift g) <- P.anyToken\n\t    napMinutes    <- concat <$> many (P.try nap)\n\t    pure (g, napMinutes)\n\n\t-- | A log stream is many guard shifts. The result is the accumulation of all\n\t-- of those shifts into a massive `Map Guard [Minute]` map, but turning all of\n\t-- those [Minutes] into a frequency map instead by using `fmap freqs`.\n\tbuildTimeCards :: Parser (Map Guard TimeCard)\n\tbuildTimeCards = do\n\t    shifts <- M.fromListWith (++) <$> many guardShift\n\t    pure (fmap freqs shifts)\n\n\nWe re-use the handy `freqs :: Ord a => [a] -> Map a Int` function, to build a\nfrequency map, from [Day 2](https://github.com/mstksg/advent-of-code-2018/blob/master/reflections.md#day-2).\n\nWe can run a parser on our `[(Time, Action)]` stream by using `P.parse ::\nParser a -> [(Time, Action)] -> SourceName -> Either ParseError a`.\n\nThe rest of the challenge involves \"the X with the biggest Y\" situations, which\nall boil down to \"The key-value pair with the biggest *some property of\nvalue*\".\n\nWe can abstract over this by writing a function that will find the key-value\npair with the biggest *some property of value*:\n\n\timport qualified Data.List.NonEmpty as NE\n\n\tmaximumValBy\n\t    :: (a -> a -> Ordring)  -- ^ function to compare values\n\t    -> Map k a\n\t    -> Maybe (k, a)         -- ^ biggest key-value pair, using comparator function\n\tmaximumValBy c = fmap (maximumBy (c `on` snd)) . NE.nonEmpty . M.toList\n\n\t-- | Get the key-value pair with highest value\n\tmaximumVal :: Ord a => Map k a -> Maybe (k, a)\n\tmaximumVal = maximumValBy compare\n\nWe use `fmap (maximumBy ...) . NE.nonEmpty` as basically a \"safe maximum\",\nallowing us to return `Nothing` in the case that the map was empty. This works\nbecause `NE.nonEmpty` will return `Nothing` if the list was empty, and `Just`\notherwise...meaning that `maximumBy` is safe since it is never given to a\nnon-empty list.\n\nThe rest of the challenge is just querying this `Map Guard TimeCard` using some\nrather finicky applications of the predicates specified by the challenge.\nLuckily we have our safe types to keep us from mixing up different concepts by\naccident.\n\n\teitherToMaybe :: Either e a -> Maybe a\n\teitherToMaybe = either (const Nothing) Just\n\n\tday04a :: Map Time Action -> Maybe Int\n\tday04a logs = do\n\t    -- build time cards\n\t    timeCards               <- eitherToMaybe $ P.parse buildTimeCards \"\" (M.toList logs)\n\t    -- get the worst guard/time card pair, by finding the pair with the\n\t    --   highest total minutes slept\n\t    (worstGuard , timeCard) <- maximumValBy (comparing sum) timeCards\n\t    -- get the minute in the time card with the highest frequency\n\t    (worstMinute, _       ) <- maximumVal timeCard\n\t    -- checksum\n\t    pure $ _gId worstGuard * fromIntegral worstMinute\n\n\tday04b :: Map Time Action -> Maybe Int\n\tday04b logs = do\n\t    -- build time cards\n\t    timeCards                      <- eitherToMaybe $ P.parse buildTimeCards \"\" (M.toList logs)\n\t    -- build a map of guards to their most slept minutes\n\t    let worstMinutes :: Map Guard (Minute, Int)\n\t        worstMinutes = M.mapMaybe maximumVal timeCards\n\t    -- find the guard with the highest most-slept-minute\n\t    (worstGuard, (worstMinute, _)) <- maximumValBy (comparing snd) worstMinutes\n\t    -- checksum\n\t    pure $ _gId worstGuard * fromIntegral worstMinute\n\nLike I said, these are just some complicated queries, but they are a direct\ntranslation of the problem prompt.  The real interesting part is the building\nof the time cards, I think!  And not necessarily the querying part.\n\nParsing, again, can be done by stripping the lines of spaces and using\n`words` and `readMaybe`s.  We can use `packFinite :: Integer -> Maybe (Finite\nn)` to get our hours and minutes into the `Finite` type that `T` expects.\n\n\tparseLine :: String -> Maybe (Time, Action)\n\tparseLine str = do\n\t    [y,mo,d,h,mi] <- traverse readMaybe timeStamp\n\t    t             <- T y mo d <$> packFinite h <*> packFinite mi\n\t    a             <- case rest of\n\t      \"falls\":\"asleep\":_ -> Just ASleep\n\t      \"wakes\":\"up\":_     -> Just AWake\n\t      \"Guard\":n:_        -> AShift . G <$> readMaybe n\n\t      _                  -> Nothing\n\t    pure (t, a)\n\t  where\n\t    (timeStamp, rest) = splitAt 5\n\t                      . words\n\t                      . clearOut (not . isAlphaNum)\n\t                      $ str\n\n\nLook, you need to write a Blog. This is too good to be forgotten after 24 hours. I always appreciate a good post explaining how someone solved a problem, especially in haskell.\n\nComing from a duck-typing language I could really appreciate types in this problem, if nothing else to not have to do some pre-coding scanning to see if guards slept until after 01:00, whether 2 guards overlapped etc. As it was I eyeballed the data and accounted for some obvious things (like guards who never sleep on one shift) and hacked together something that worked good enough.\n\nI created a new Record data type with three constructors matching the three line types:\n\n    data Record = BeginShift  Timestamp BadgeID\n                | FallsAsleep Timestamp\n                | WakesUp     Timestamp\n                deriving (Show)\n    \n    parseRecord :: Parser Record\n    parseRecord = begins <|> fallsAsleep <|> wakesUp\n    \n    -- [1518-11-01 00:00] Guard #10 begins shift\n    -- [1518-11-01 00:05] falls asleep\n    -- [1518-11-01 00:25] wakes up\n    begins, fallsAsleep, wakesUp :: Parser Record\n    begins      = BeginShift  <$> timestamp <* string \" Guard #\" <*> number <* string \" begins shift\"\n    fallsAsleep = FallsAsleep <$> timestamp <* string \" falls asleep\"\n    wakesUp     = WakesUp     <$> timestamp <* string \" wakes up\"\n\nI built up the list of guard/minutes asleep using a bit of recursion and pattern matching:\n\n    -- Analyze a (sorted) list of records to determine which minutes\n    -- of the night were slept through by which guards\n    analyze :: [Record] -> [(BadgeID, Minute)]\n    analyze rs = go 0 rs\n    \n      where go :: BadgeID -> [Record] -> [(BadgeID, Minute)]\n    \n            -- Switch the guard we're analyzing\n            go _ (BeginShift _ badge : rs) = go badge rs\n    \n            -- A FallsAsleep record is always followed by a WakesUp record, so pattern match on this pair\n            go badge (FallsAsleep a : WakesUp w : rs) \n                = minutes badge a w ++ go badge rs\n    \n            go _ [] = []\n    \n            -- In our puzzle input, guards don't fall asleep before midnight, which simplifies this\n            minutes b a w = [ (b, minute a + i) | i <- [0 .. minute w - minute a - 1] ]", "id": "eb2576e", "owner_tier": 0.5, "score": 0.2962962961728395}, {"content": "Mods, first off, love your work, you're the best! \n\nSmall request, can the time the thread is unlocked be added to the message? \n\nI.e.\n\n> edit: Leaderboard capped, thread unlocked __at 00:45__!\n\nCan do.\n\nYou may already know this, but you can find that data on the daily leaderboard for each puzzle, [e.g. for day 5](https://adventofcode.com/2018/leaderboard/day/5) it was 00:10.\n\nAs an aside, oh how I wish the links for a personal and private leaderboards were right there in the header. \n\nAh, I actually didn't know that! \n\n", "id": "eb278k4", "owner_tier": 0.7, "score": 0.19753086407407405}, {"content": "[**APL**](https://github.com/jayfoad/aoc2018apl/blob/master/p4.dyalog) \\#89/71\n\nSorting the input is trivial in Dyalog 17.0 thanks to the [Total Array Ordering](https://www.youtube.com/watch?v=8cbPLRAcC7M). Parsing the input lines is a little messy, using global assignments from a dfn to update global state. Once we've got an array `a` telling us how many times each guard is asleep for each minute of the witching hour, it becomes much more elegant:\n\n    f\u2190{\u2283\u2378\u2375=\u2308/,\u2375} \u235d coordinates of maximum value\n    {\u2375\u00d7f \u2375\u2337a}f+/a \u235d part 1\n    \u00d7/f a \u235d part 2\n\n&#x200B;\n\nAre you a human?\n\nwhat.. wizardry is this?!? \n\nFantastic.\n\nBleep bloop, I mean yes. That's why I like writing short programs. Some of these solutions, even ones that made the leaderboard, take 5 lines of code to find the maximum value (or its location) in a list. I can do it with `\u2308/vec` (or `vec\u2373\u2308/vec`, the \"vec-index of the max reduce of vec\"). What's not to like? Especially when you're coding against the clock.\n\nIf you want to learn, you could check out [The APL Orchard](https://chat.stackexchange.com/rooms/52405/the-apl-orchard).\n\nArray wizardry!\n\nIt looks like the notes I took in 400 level math classes(set theory etc). I\u2019m gonna dig into this language later for shits and giggles. \n\nWhat do you... do.. with it? \n\nAnything you want. It's general purpose and you can write [web servers](https://github.com/Dyalog/MiServer) with it if you want. It excels at wrangling arrays of data, in a way that is naturally data-parallel, but you'll find that many problems lend themselves to array-based solutions once you learn to look at them right.\n\ncool shit dude! my background is solely numerical dataset manipulation (giant array output from simulations) so I'm using AoC to force myself to learn better string manipulation and other crap.\n\nkeep on doing insane stuff!\n\nAPL can do insane stuff with numerical datasets! As for strings, the approach is that you just use a 1-d array of characters. But arrays can be arbitrarily nested, so of course you can have arrays of strings, and it works just like any other array of arrays.\n\nThanks for the information.  I did a lot of wolfram/mathematica, and I've been slowly forcing myself back to python because it's free.  R is probably a closer corollary to wolfram but python is just more extensible I think.\n\nat any rate, thanks again for the info! so cool to see all these different approaches.", "id": "eb2btjl", "owner_tier": 0.1, "score": 0.41975308629629626}, {"content": "So did anyone else lose time because they had a correct solution but returned just the minute they chose, forgetting to multiply it with the guard ID? I lost so much time catching that error that it made the difference between spot 60ish and 160 on Part 1.\n\nI try to make the highlighted part of the last paragraph of each puzzle a summary of that puzzle.  Today, it was:\n\n>*What is the ID of the guard you chose multiplied by the minute you chose?*\n\nMy problem was I kept multiplying by the amount of times they slept on the minute, instead of the minute itself. Made that mistake for both parts.\n\nI lost time because I skim read \"*Strategy 1: Find the guard that has the most minutes asleep*\" and stopped reading and tried to put the guard ID.\n\nI got stumped because I did not see that a guard could fall asleep more than once in a shift. I got the first question right, so I assumed that I parsed the input correctly. So I kept trying out new ways to do part 2, but it always gave the same answer. It was not untill I manually sorted the file that I saw that a guard could fall asleep several times!\n\nYou were *totally* clear. I even read that sentence. It's just that my dumb ass got excited about finding a solution and forgot what result was requested.\n\nIn your examples you seem to count as asleep, the minute when the guard fell asleep and NOT the minute when the guard wakes up.\nIs this the right way to identify the second factor?\n\nThe good news is that this comment made me realize why I wasn't getting the right answer for part 1, so thanks!\n\nI'm an idiot\n\nAh, that's rough.\n\nYup, that's what I did as well.", "id": "eb1woib", "owner_tier": 0.7, "score": 0.9999999998765433}, {"content": "Python 3, #2/#2. Reached for dateutil in the heat of the moment although turns out you only need the minute!\n\n\n    guards = collections.defaultdict(list)\n    times = collections.defaultdict(int)\n\n    for line in sorted(inp(4).splitlines()):\n        time, action = line.split('] ')\n\n        time = dateutil.parser.parse(time[1:])\n\n        if action.startswith('Guard'):\n            guard = int(action.split()[1][1:])\n        elif action == 'falls asleep':\n            start = time\n        elif action == 'wakes up':\n            end = time\n            guards[guard].append((start.minute, end.minute))\n            times[guard] += (end - start).seconds\n\n    (guard, time) = max(times.items(), key=lambda i: i[1])\n    (minute, count) = max([\n        (minute, sum(1 for start, end in guards[guard] if start <= minute < end))\n    for minute in range(60)], key=lambda i: i[1])\n\n    print('part 1:', guard * minute)\n\n    (guard, minute, count) = max([\n        (guard, minute, sum(1 for start, end in guards[guard] if start <= minute < end))\n    for minute in range(60) for guard in guards], key=lambda i: i[2])\n\n    print('part 2:', guard * minute)\n\n\nhaha, the dates set me off as well, didn't realize they were there just for sorting.\n\nAnd since they use the ISO standard, you can just sort them as strings and don't need to worry about it haha.\n\n[deleted]\n\nI took advantage of this!\n\n[](/facehoof) And here I went and made an entire linked-list sorting algorithm when I could have just used 'sort'... *sigh*.\n\nI didn't figure that out, even though it's the main reason I advocate for ISO standard datetime. *sigh*.\n\nBut Powershell will just `get-date 1518-01-02` without any problem, so I didn't really need to, but it would still have been quicker. D'oh.\n\nThe date is pre-Gregorian, did you account for Old-Style vs New-Style dates? ;)\n\nPtsch, I'm a high level scripting pleb, and high level languages are the real world version of Douglas Adam's [somebody else's problem field](https://www.goodreads.com/quotes/691174-the-somebody-else-s-problem-field-is-much-simpler-and-more) (from Hitchhiker's Guide to the Galaxy).\n\nI'm sure the .Net elves will have understood and taken care of that hard stuff, so I don't have to ;P", "id": "eb1w7xy", "owner_tier": 0.1, "score": 0.32098765419753084}, {"content": "Finally got top 100 :) 62/43 with Gawk (used multidimensional arrays so I guess it's not awk anymore)\n\nThis is exact code I submitted, no ifs ands or buts. \n\nRun `sort -V` on the file before inputting to Awk\n\nPart 1:\n\n    BEGIN {\n    #   RS\n      FPAT = \"#?[0-9]+\"\n      guard=99999999\n    }\n    /#[0-9]+/{\n      guard=$6\n    }\n    /wake/{\n      minute=$5\n      for(i=fell_asleep[guard];i<minute;i++) {\n        asleep_minutes[guard][i]++\n      }\n      sleep_time[guard] += (minute - fell_asleep[guard])\n    }\n    /falls/ {\n      minute=$5\n      fell_asleep[guard] = minute\n    }\n    END {\n      max_sleep_time=0\n      max_sleep_guard=999999999\n      for(guard in sleep_time) {\n        if (sleep_time[guard] > max_sleep_time) {\n          max_sleep_guard = guard\n          max_sleep_time = sleep_time[guard]\n        }\n      }\n      max_minute=9999999\n      max_minute_val=0\n      for(minute in asleep_minutes[max_sleep_guard]) {\n        if (asleep_minutes[max_sleep_guard][minute] > max_minute_val) {\n          max_minute= minute\n          max_minute_val=asleep_minutes[max_sleep_guard][minute]\n        }\n      }\n      print max_sleep_guard\n      print max_minute\n    }\n\nPart 2:\n\n    BEGIN {\n    #   RS\n      FPAT = \"#?[0-9]+\"\n      guard=99999999\n    }\n    /#[0-9]+/{\n      guard=$6\n    }\n    /wake/{\n      minute=$5\n      for(i=fell_asleep[guard];i<minute;i++) {\n        asleep_minutes[guard][i]++\n      }\n      sleep_time[guard] += (minute - fell_asleep[guard])\n    }\n    /falls/ {\n      minute=$5\n      fell_asleep[guard] = minute\n    }\n    END {\n    #   max_sleep_time=0\n    #   max_sleep_guard=999999999\n    #   for(guard in sleep_time) {\n    #     if (sleep_time[guard] > max_sleep_time) {\n    #       max_sleep_guard = guard\n    #       max_sleep_time = sleep_time[guard]\n    #     }\n    #   }\n      max_minute=9999999\n      max_minute_val=0\n      max_guard=99999999999\n      for(guard in asleep_minutes) {\n        for(minute in asleep_minutes[guard]) {\n          if (asleep_minutes[guard][minute] > max_minute_val) {\n            max_minute= minute\n            max_guard=guard\n            max_minute_val=asleep_minutes[guard][minute]\n          }\n        }\n      }\n      print max_guard\n      print max_minute\n    }\n\n\nWhat am I looking at?\n\nI've been loving your AWK solutions. You have a Github?\n\nNice use of pattern selectors, I'm just getting familiar with the language atm and that's awesome.\n\n\nIt's the language that Larry Wall thought was too readable, so he invented Perl.\n\n[Awk](https://en.wikipedia.org/wiki/AWK)\n\nThanks :)\n\nhttps://github.com/markasoftware\n\nI don't make anything OSS with Awk. I have used it only for other speed-coding stuff and some golf, as well as that one time I needed to process 300GB of CSV files */me shudders*.", "id": "eb1w92n", "owner_tier": 0.5, "score": 0.2222222220987654}, {"content": "**FORTRAN**\n\nWill edit with cleaned up code later. Got held up a looong time by having copy pasted a line into another loop without then updating which loop index it was using. \n\n>While this example listed the entries in chronological order, your  entries are in the order you found them. You'll need to organize them  before they can be analyzed.\n\nNo.\n\n    PROGRAM DAY4\n      IMPLICIT NONE\n      INTEGER :: I,J,K,L,HOUR,MINUTE,DAY\n      INTEGER :: IERR\n      CHARACTER(LEN=50),ALLOCATABLE :: LINES(:)\n      CHARACTER(LEN=50) :: LINE\n      INTEGER, ALLOCATABLE :: GUARDS(:)\n      LOGICAL, ALLOCATABLE :: ASLEEP(:,:)\n      INTEGER, ALLOCATABLE :: DATES(:)\n      CHARACTER(LEN=8) :: DATE\n      INTEGER :: GUARD,BESTGUARD,SLEEP,BESTSLEEP,SLEEPMINUTE(0:59),BESTMINUTE,BESTMINUTEAMMOUNT\n      OPEN(1,FILE='input.txt')\n      I=0\n      DO\n         READ(1,*,IOSTAT=IERR)\n         IF(IERR.NE.0)EXIT\n         I=I+1\n      END DO\n      ALLOCATE(LINES(I))\n      REWIND(1)\n      READ(1,'(A)')LINES\n      CLOSE(1)\n    \n      J=1\n      DO K=2,I\n         IF(ANY(LINES(1:K-1)(1:12).EQ.LINES(K)(1:12)))CYCLE\n         J=J+1\n      END DO\n    \n      ALLOCATE(GUARDS(J),ASLEEP(J,0:59),DATES(J))\n      DATES=99999999\n      ASLEEP=.FALSE.\n      GUARDS=0\n      WRITE(DATE,'(A)') LINES(1)(2:5)//LINES(1)(7:8)//LINES(1)(10:11)\n      READ(DATE,*) DAY\n      DATES(1)=DAY\n      L=2\n      DO K=2,I\n         WRITE(DATE,'(A)') LINES(K)(2:5)//LINES(K)(7:8)//LINES(K)(10:11)\n         READ(DATE,*) DAY\n         IF(ANY(DATES(1:L-1).EQ.DAY))CYCLE\n         DATES(L)=DAY\n         L=L+1\n      END DO\n    \n      DO J=1,I\n         WRITE(DATE,'(A)') LINES(J)(2:5)//LINES(J)(7:8)//LINES(J)(10:11)\n         READ(DATE,*) DAY\n         READ(LINES(J)(13:14),*)HOUR\n         READ(LINES(J)(16:17),*)MINUTE\n         IF(HOUR>0)DAY=MINVAL(DATES,MASK=DATES>DAY)\n         SELECT CASE (LINES(J)(20:24))\n         CASE ('Guard')\n            READ(LINES(J)(SCAN(LINES(J),'#')+1:SCAN(LINES(J),'b')-1),*) GUARDS(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1))\n         CASE('wakes')\n            ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)=.NOT.ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)\n         CASE('falls')\n            ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)=.NOT.ASLEEP(MINLOC(DATES,MASK=DATES.EQ.DAY,DIM=1),MINUTE:59)\n         END SELECT\n      END DO\n    \n      !Part 1\n      BESTGUARD=0\n      BESTSLEEP=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         SLEEP=0\n         GUARD=GUARDS(I)\n         DO J=1,SIZE(GUARDS,DIM=1)\n            IF (GUARDS(J).EQ.GUARD) SLEEP=SLEEP+COUNT(ASLEEP(J,:))\n         END DO\n         IF(SLEEP>BESTSLEEP)THEN\n            BESTSLEEP=SLEEP\n            BESTGUARD=GUARD\n         END IF\n      END DO\n      SLEEPMINUTE=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         IF(GUARDS(I).NE.BESTGUARD)CYCLE\n         DO J=0,59\n            IF(ASLEEP(I,J)) SLEEPMINUTE(J)=SLEEPMINUTE(J)+1\n         END DO\n      END DO\n      WRITE(*,'(A,I0)') 'Part 1: ',(MAXLOC(SLEEPMINUTE,DIM=1)-1)*BESTGUARD\n    \n      !PART 2\n      BESTMINUTE=0\n      BESTMINUTEAMMOUNT=0\n      DO I=1,SIZE(GUARDS,DIM=1)\n         SLEEPMINUTE=0\n         GUARD=GUARDS(I)\n         DO J=1,SIZE(GUARDS,DIM=1)\n            IF (GUARDS(J).NE.GUARD)CYCLE\n            DO K=0,59\n               IF(ASLEEP(J,K)) SLEEPMINUTE(K)=SLEEPMINUTE(K)+1\n            END DO\n         END DO\n         IF(MAXVAL(SLEEPMINUTE)>BESTMINUTEAMMOUNT)THEN\n            BESTMINUTE=MAXLOC(SLEEPMINUTE,DIM=1)-1\n            BESTGUARD=GUARD\n            BESTMINUTEAMMOUNT=MAXVAL(SLEEPMINUTE)\n         END IF\n      END DO\n      WRITE(*,'(A,I0)') 'Part 2: ',BESTGUARD*BESTMINUTE\n    \n      DEALLOCATE(LINES,DATES,GUARDS,ASLEEP)\n    END PROGRAM DAY4", "id": "eb20muy", "owner_tier": 0.5, "score": 0.04938271592592593}, {"content": "#Perl 6\n\nLooks like I'm posting the first Perl 6 solution of the day.\n\nThis is post-cleanup... but more or less what I wrote, except all the functions were inline. My cleanup was just moving them all into (anonymous) functions so the implementation reads better.\n\n    # Helper functions\n    my &guard              = { .substr(26).words.head         }\n    my &mins               = { .substr(15, 2).Int             }\n    my &total-sleep-length = { .value.total                   }\n    my &most-slept-minute  = { .value.max(*.value).value      }\n    my &magic-value        = { .key \u00d7 .value.max(*.value).key }\n\n    # State variables\n    my (%sleep, $guard, $falls);\n\n    # Data collection\n    for 'input'.IO.lines.sort {\n        when *.contains('Guard') { $guard = .&guard }\n        when *.contains('falls') { $falls = .&mins  }\n        when *.contains('wakes') { %sleep{$guard} \u228e= $falls ..^ .&mins }\n    }\n\n    # Output solution\n    for &total-sleep-length, &most-slept-minute -> &func {\n        say %sleep.max(&func).&magic-value\n    }\n\nThe `\u228e` operator is the Multiset Union operator (ASCII version: `(+)`). Perl 6 has a `Bag` type (aka. [Multiset](https://oeis.org/wiki/Multisets)). I create a range of the minutes slept and add those to the Bag of values for that guard.\n\n# Python 3\n\nRe-implementation of the above in Python, just because.\n\nI laugh in the face of PEP8!\n\n    from collections import Counter, defaultdict\n\n    # Helper functions\n    get_guard          = lambda x: int(x[26:].split()[0])\n    get_mins           = lambda x: int(x[15:17])\n    total_sleep_length = lambda x: sum(x[1].values())\n    most_slept_minute  = lambda x: max(x[1].values())\n    magic_value        = lambda x: x[0] * max(x[1].items(), key=lambda y: y[1])[0]\n\n    # State variables\n    sleep = defaultdict(Counter)\n\n    # Data collection\n    for line in sorted(open('input').readlines()):\n        if   'Guard' in line: guard = get_guard(line)\n        elif 'falls' in line: falls = get_mins(line)\n        elif 'wakes' in line: sleep[guard].update(range(falls, get_mins(line)))\n\n    # Output solution\n    for func in (total_sleep_length, most_slept_minute):\n        print(magic_value(max(sleep.items(), key=func)))", "id": "eb26m6l", "owner_tier": 0.3, "score": 0.03703703691358025}, {"content": "Day 4...IN EXCEL??!?!?\n\n     =COUNTBYCELLCOLOR(M4:BT4,$I$1)\n\nhttps://github.com/thatlegoguy/AoC2018", "id": "eb385eh", "owner_tier": 0.3, "score": 0.03703703691358025}, {"content": "Python 2, just missed the leaderboard for the second star. I completely forgot about 'in' and spent too long indexing the strings, as well as not initially sorting the data by time *and* sorting the list of guards the wrong way, which I figured out after I tried with the test code.\n(The code has been edited.)\n\n\tl = sorted(lines)\n\tguards = defaultdict(lambda:[0 for x in range(60)])\n\tfor s in l:\n\t\tif s[25]==\"#\":\n\t\t\tg=s.split()[3]\n\t\telif s[25]==\"a\":\n\t\t\tst=int(s[15:17])\n\t\telse: # wake up\n\t\t\tt=int(s[15:17])\n\t\t\tfor x in range(st,t):\n\t\t\t\tguards[g][x]+=1\n\n\t# part 1\n\tg1 = sorted(guards.keys(), key=lambda g:-sum(guards[g]))[0]\n\t# part 2\n\tg2 = sorted(guards.keys(), key=lambda g:-max(guards[g]))[0]\n\n\tfor g in [g1,g2]:\n\t\tgh = guards[g]\n\t\tminute = gh.index(max(gh))\n\t\tprint int(g[1:])*minute\n\n\nNice strategy!", "id": "eb1wu1a", "owner_tier": 0.1, "score": 0.02469135790123457}, {"content": "**Python 3** 51/63\n\n    from collections import Counter, defaultdict\n    from datetime import datetime\n\n    guards = defaultdict(Counter)\n    for t, m in [l.split('] ') for l in sorted(s.splitlines()) if l]:\n        t = datetime.strptime(t, '[%Y-%m-%d %H:%M')\n        if '#' in m:     g = int(m.split('#')[1].split()[0])\n        if 'falls' in m: start = t\n        if 'wakes' in m:\n            minutes = int((t - start).total_seconds() // 60)\n            guards[g].update(Counter((start.minute+i)%60 for i in range(minutes)))\n\n    _, id = max((sum(c.values()), id) for id, c in guards.items())\n    part1 = id * guards[id].most_common()[0][0]\n\n    (_, minute), id = max((c.most_common()[0][::-1], id) for id, c in guards.items())\n    part2 = id * minute\n\n\nAt which point do you read the input? Looks like the variable `s` is undefined, so presumably that's the input?\n\n[deleted]\n\n> so presumably that's the input\n\nThat's right. I just copy and paste it manually to `s` variable in IPython.\n\n> Counter\n\nIt looks like Counter is a multiset, is that right?\n\nhow did you copy paste the .txt to s?\n\n\nI am not the guy you asked, but I assume they just copy and paste the text using their computer's clipboard or similar.\n\n[deleted]\n\nPersonally I just read the input files using Python and then process them as necessary.  \nSomething like this works fine:\n\n    with open(\"input.txt\") as f:\n        data = f.readlines()\n        # do other things with the data here", "id": "eb1x7yq", "owner_tier": 0.1, "score": 0.16049382703703702}, {"content": "I thought I was wayyy out of the running on that one, but 19 minutes and first time into leaderboard part 2!  Thanks PowerShell date handling!  #37 / #61\n\n[Card] today's puzzle would have been a lot easier if my language supported: Python's enumerate().\n\n#####PowerShell\n\nCode:\n\n    # hashtable of Guard ID -> (0,0,0,0 ... array of 60 minutes for that guard.\n    $guards = @{}\n\n    Get-Content .\\data.txt  | sort-object {\n\n        [void]($_ -match '\\[(.*)\\] (.*)')\n        Get-Date $matches[1]\n    \n    } | foreach { \n\n        [void]($_ -match '\\[(.*)\\] (.*)')\n        $date = get-date $matches[1]\n        $message = $matches[2]\n\n        switch -regex ($message)    # switch state machine\n        {\n            # For a Guard identifier line, get the ID, set them up with 60 blank minutes.\n            'Guard #(\\d+)' { \n                $script:guard = $matches[1]\n                if (-not $guards.ContainsKey($script:guard)){ $guards[$script:guard] = @(0) * 60 }\n            }\n\n            # If they fell asleep, store the date for use when they wake.\n            'sleep' { $script:sleep = $date }\n        \n            # If they wake, loop over the minutes from sleep..wake and increment their array\n            'wakes' {\n                $script:sleep.Minute..($date.Minute-1)| foreach-object {\n                    $guards[$script:guard][$_]++\n                }\n            }\n        }\n    }\n\n    # Part 1, most minutes asleep, which minute is highest\n    $mostSleepy = $guards.GetEnumerator() | sort-object { $_.Value | measure -sum | % sum } | select -Last 1\n    $minute = $mostSleepy.Value.IndexOf(($mostSleepy.Value | sort)[-1])\n    \"Part 1: Guard $($mostSleepy.Name), minute $minute\"\n    $minute * $mostSleepy.Name\n\n    # Part 2, guard with most same-minute asleep\n    $mostSame = $guards.GetEnumerator() | sort-object { ($_.Value | sort)[-1] } | select -Last 1\n    $minute = $mostSame.Value.IndexOf(($mostSame.Value | sort)[-1])\n    \"Part 2: Guard $($mostSame.Name), minute: $minute\"\n     $minute * $mostSame.Name\n\n(I actually made the hashtable and eyeballed the largest value and counted the minutes by hand, because it was quicker than writing code to do it, but I've added that code here)\n\n\n\n\nNo date handling for me, the input format meant a simple alphabetic sort put them in the correct chronological order.\n\nSame sort of process for checking which line type we're on, set the current guard and start minute on the respective line types, and add to tables during wake up lines.\n\nThen just like you I did the relevant sorting to find the sleepiest guard, and found his sleepy minute.\n\nI'm quite pleased with my part 2 solution.  I stored the current Guard and current Minute concatenated as a string as the key to a hash table.  Each time that Guard/Minute was hit, we only need to do a simple increment.  At first pass I was going to parse this string out at the end, but instead I switched my 'delimiter' between Guard and Minute to be `*`.  The multiplication to get the answer is then an Invoke-Expression on the key of the highest value in the hash table.\n\n    $in = gc .\\04-input.txt | sort\n    \n    $gHash = @{}\n    $mHash = @{}\n    \n    for($i = 0;$i -lt $in.count;$i++){\n    \t$m = @(([regex]\"\\d+\").matches($in[$i]).value)\n    \t\n    \t\n    \tif($m[5]){\n    \t\t$curGuard = +$m[5]\n    \t}elseif($in[$i] -like \"*falls asleep*\"){\n    \t\t$sleepStart = +$m[4]\n    \t}else{\n    \t\t$sleepEnd = +$m[4]-1\n    \t\t\n    \t\t$sleepStart..$sleepEnd | % {\n    \t\t\t$gHash[$curGuard]+=@($_)\n    \t\t\t$mHash[\"$curGuard * $_\"]++\n    \t\t}\n    \t}\n    \t\n    }\n    \n    $sleepyGuard = ($gHash.getEnumerator() | sort {$_.value.count} -descending)[0].name\n    $sleepyMinute = ($gHash[$sleepyGuard] | group | sort count -descending)[0].name\n    \n    $sleepyGuard*$sleepyMinute\n    \n    ($mHash.getEnumerator() | sort value -descending)[0].name | iex  \n\n>\\[void\\]($\\_ -match '\\\\\\[(.\\*)\\\\\\] (.\\*)')\n\nHi ka-splam, been folliowing (4 days behind at the moment!) your PS solutions.\n\nWhat does the above do/mean please?\n\nYou said you visually did calculation - which i did also - because I could nt get the minute part to work in Part 1. Evaluates to -1.\n\nDid you test it?\n\nHi \ud83d\udc4b,\n\nThe `-match` operator takes a string on the left and a pattern on the right, and tries to match the string with the pattern. `$_` is my input line each time. So this pattern is `sort-object { $_ ...}`  and sort-object gets the input, runs the scriptblock on it, and then sorts on the result of that. So I pull the date out of the lines, make it a PowerShell DateTime so it will sort properly.\n\n(I later learned that most of this is unnecessary, and if I'd just put `get-content .\\data.txt | sort-object | foreach {}` it would have sorted properly anyway. Oops.).\n\n`-match` returns true/false, which I don\u2019t care about here because I made sure it will always match on my input data. So I take the result and cast it to \u2018[void]` which makes it vanish. Instead I could have used `$result = $_ -match '..pattern..'` but that leaves a variable hanging around that is not going to be used, matter of choice if that bothers you.\n\nWhen `-match` can match the pattern to the text, it sets the automatic variable `$matches` with some information. So the next line works with that as a way to use the result of this line. `$matches[1]` is the first group `()` in the pattern.\n\nI think it\u2019s pulling the date out of the square brackets and then the text after the square brackets, is it? (The pattern is a regex).\n\n(Was the hashtable explanation any help at all btw?)\n\nDid I test what?  This code works on my input, yes.\n\nWhen I first did it by hand, I poked my finger at the screen and counted 59, 58, 57, 56 .. ok minute 45 has the biggest for guard 2663 so 2663*45 and put that into the site, no double-check then, too much hurry.\n\nI am sure it is me and my code. Thanks.", "id": "eb1wuff", "owner_tier": 0.7, "score": 0.1111111109876543}, {"content": "Yeah!!! I have no idea what I'm doing anymore!!! Ruby\n\n[Card] [Image](https://i.imgur.com/U8iWZRt.png)\n\n    h={}\n    ll=0\n    sl={}\n    p (a=$<.sort).map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    sl[ll]=b\n    elsif $3\n    u,i,l,n=sl[ll]\n    kk=h[ll] ? h[ll] : h[ll]=0\n    kk +=  (o*60+q)- (l*60+n)\n    h[ll] = kk\n    end\n    }\n    guard = h.max_by{|x,y|y}.first\n    p guard\n    \n    # part 1\n    \n    sl=0\n    qqq=Hash.new 0\n    a.map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    (sl=q) if ll == guard\n    elsif $3\n    (n=sl\n    (qqq[n]+=1\n    \n    n+=1\n    n=0 if n==60)while n!=q) if ll == guard\n    end\n    }\n    p qqq.max_by{|x,y|y}.first*guard\n    \n    # part 2\n    qqqt=Hash.new{Hash.new 0}\n    qsl={}\n    a.map{|x|m = x.chomp\n    m =~ /^\\[1518-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2})\\] (.+)$/\n    j,k,o,q,s=[$1, $2, $3, $4, $5]\n    j,k,o,q=b=[j,k,o,q].map(&:to_i)\n    #p b\n    s =~ /^(?:Guard #(\\d+)|(falls)|(wakes))/\n    if $1\n    ll=$1.to_i\n    elsif $2\n    (qsl[ll]=q) #if ll == guard\n    elsif $3\n    (n=qsl[ll]\n    qqq=qqqt.has_key?(ll) ? qqqt[ll] : (qqqt[ll]=qqqt[ll])\n    (qqq[n]+=1\n    \n    n+=1\n    n=0 if n==60)while n!=q) #if ll == guard\n    end\n    }\n    p qqqt\n    a,b= qqqt.max_by{|x,y|y.max_by{|x,y|y}.last}\n    p a*b.max_by{|x,y|y}.first\n    p b.max_by{|x,y|y}.last\n    \n\nEdit: Yes, just like all the previous days, I didn't read the question...\n\nEdit: Changed the layout of the code so you can run it without any hassle and it will just work. If you're reading this, bless you", "id": "eb1xcir", "owner_tier": 0.3, "score": 0.02469135790123457}, {"content": "Today's puzzle would have been a lot easier if my language supported looser types.\n\nI also need to learn how to read the instructions closer to make my life simpler!\n\nAnother day, another Rust:\n\n    use aoc2018::*;\n    use chrono::{Timelike, Duration};\n    \n    fn main() -> Result<(), Error> {\n        let mut records = Vec::new();\n    \n        for line in BufReader::new(input!(\"day4.txt\")).lines() {\n            let line = line?;\n            let (date, rest) = line.split_at(18);\n            let date = chrono::NaiveDateTime::parse_from_str(date, \"[%Y-%m-%d %H:%M]\")?;\n            records.push((date, rest.trim().to_string()));\n        }\n    \n        records.sort_by(|a, b| a.0.cmp(&b.0));\n    \n        let mut current = 0u32;\n        let mut asleep = None;\n    \n        let mut minutes_asleep = HashMap::<_, u32>::new();\n        let mut guard_asleep_at = HashMap::<_, HashMap<u32, u32>>::new();\n        let mut minute_asleep = HashMap::<_, HashMap<u32, u32>>::new();\n    \n        for (date, rest) in records {\n            if rest.ends_with(\"begins shift\") {\n                current = str::parse(&rest.split(\" \").nth(1).unwrap()[1..])?;\n                asleep = None;\n                continue;\n            }\n    \n            match rest.as_str() {\n                \"falls asleep\" => {\n                    asleep = Some(date);\n                }\n                \"wakes up\" => {\n                    let asleep = asleep.as_ref().expect(\"guard did not fall asleep\");\n                    let count = date.signed_duration_since(asleep.clone()).num_minutes() as u32;\n    \n                    *minutes_asleep.entry(current).or_default() += count;\n    \n                    let mut m = asleep.minute();\n    \n                    for i in 0..count {\n                        *guard_asleep_at\n                            .entry(current)\n                            .or_default()\n                            .entry(m)\n                            .or_default() += 1;\n    \n                        *minute_asleep\n                            .entry(m)\n                            .or_default()\n                            .entry(current)\n                            .or_default() += 1;\n    \n                        m += 1;\n                    }\n                }\n                other => {\n                    panic!(\"other: {}\", other);\n                }\n            }\n        }\n    \n        let max = minutes_asleep\n            .into_iter()\n            .max_by(|a, b| a.1.cmp(&b.1))\n            .expect(\"no max asleep\");\n    \n        let pair = guard_asleep_at\n            .remove(&max.0)\n            .unwrap_or_default()\n            .into_iter()\n            .max_by(|a, b| a.1.cmp(&b.1))\n            .expect(\"no max guard asleep\");\n    \n        let mut sleep_min = None;\n    \n        for (ts, guards) in minute_asleep {\n            if let Some((guard, times)) = guards.into_iter().max_by(|a, b| a.1.cmp(&b.1)) {\n                sleep_min = match sleep_min {\n                    Some((ts, guard, max)) if max > times => Some((ts, guard, max)),\n                    Some(_) | None => Some((ts, guard, times)),\n                };\n            }\n        }\n    \n        let sleep_min = sleep_min.expect(\"no result found\");\n    \n        assert_eq!(pair.0 * max.0, 19830);\n        assert_eq!(sleep_min.0 * sleep_min.1, 43695);\n        Ok(())\n    }\n\nIn case you didn't know, you can use the include_str macro instead of requiring a BufReader with the input macro, e.g.\n\n    for line in include_str!(\"day4.txt\").lines() {\n        ...\n    }\n\nSaves a few keystrokes!", "id": "eb1y2im", "owner_tier": 0.5, "score": 0.01234567888888889}, {"content": "##Powershell 5.1\n[card] Python's sum()\n\nBoth halves used this basic layout:\n\n    $data = Get-Content $inputPath | Sort-Object\n    \n    $timer = New-Object System.Diagnostics.Stopwatch\n    $timer.Start()\n    \n    $guards = @{}\n    $curDate = ''\n    $curGuard = -1\n    [int]$sleepMinute = -1\n    foreach ($line in $data) {\n        $tokens = $line.Replace('[', '').Replace(']', '').Replace(':', ' ').Replace('#', '').Split(' ')\n        if ($tokens[0] -ne $curDate -or [int]$tokens[1] -eq 23) {\n            $curDate = $tokens[0]\n            if ($tokens[4] -ne 'asleep') {\n                [int]$curGuard = $tokens[4]\n            }\n            if ($null -eq $guards[$curGuard]) {\n                $guards[$curGuard] = New-Object int[] 60\n            }\n        }\n    \n        if ($tokens[3] -eq 'falls') {\n            [int]$sleepMinute = $tokens[2]\n        }\n        elseif ($tokens[3] -eq 'wakes') {\n            for ($i = $sleepMinute; $i -lt [int]$tokens[2]; $i++) {\n                $guards[$curGuard][$i]++\n            }\n        }\n    \n    }\n\nBasically enumerate everything a hashtable, with the key as the guard's ID and a 60 int array for the minutes as the value, and the amount of time they were asleep. then hook the appropriate ending on the bottom from below\n\n###Part 1\n\n    $curBestGuard = -1\n    $curBestGuardTime = -1\n    $curBestMinute = -1\n    \n    foreach ($g in $guards.Keys) {\n        $sum = 0\n        $guards[$g] | ForEach-Object {$sum += $_}\n        if ($sum -gt $curBestGuardTime) {\n            $curBestGuard = $g\n            $curBestGuardTime = $sum\n            $maximum = ($guards[$g] | Measure-Object -Max).Maximum\n            [int]$curBestMinute = [Array]::IndexOf($guards[$g], [int]$maximum) \n            \n        }\n    }\n    \n    $check = [int]$curBestGuard * [int]$curBestMinute\n    Write-Host $check\n    $timer.Stop()\n    Write-Host $timer.Elapsed\n\nAverage runtime: 0.058s\n\n###Part 2\n\n    $curBestGuard = -1\n    $curBestGuardTime = -1\n    $curBestMinute = -1\n    \n    foreach ($g in $guards.Keys) {\n        $maximum = ($guards[$g] | Measure-Object -Max).Maximum\n        if ($maximum -gt $curBestMinute) {\n            [int]$curBestGuardTime = [Array]::IndexOf($guards[$g], [int]$maximum)\n            $curBestGuard = $g\n            $curBestMinute = $maximum\n        }\n            \n    }\n    \n    \n    $check = [int]$curBestGuard * [int]$curBestGuardTime\n    Write-Host $check\n    \n    $timer.Stop()\n    Write-Host $timer.Elapsed\n\n\nAverage runtime 0.053s \n\n", "id": "eb1ycuy", "owner_tier": 0.9, "score": -1.2345678937315173e-10}, {"content": "##**C++**\n\n    int main(int argc, char* argv[]) {\n        std::ifstream ifs(argv[1]); std::vector<std::string> lines;\n        for (std::string l; std::getline(ifs, l);) { lines.push_back(l); }\n        std::sort(lines.begin(), lines.end());\n\n        std::map<int, std::array<int, 60>> guards;\n        int s = -1, id = -1, *g = nullptr; std::smatch m;\n        for (const auto& l : lines)\n            if (std::regex_match(l, m, std::regex(R\"(.*Guard #(\\d+) begins shift)\")))\n                g = guards[std::stoi(m[1])].data();\n            else if (std::regex_match(l, m, std::regex(R\"(.*:(\\d+)\\] wakes up)\")))\n                std::for_each(g+s, g+std::stoi(m[1]), [](auto& x){x++;});\n            else if (std::regex_match(l, m, std::regex(R\"(.*:(\\d+)\\] falls asleep)\")))\n                s = std::stoi(m[1]);\n        \n        auto maxMinute = [](const auto& gi) { return[&gi](auto m) -> std::pair<int,int>{\n            return {std::distance(gi.begin(), m), *m};}(std::max_element(gi.begin(), gi.end()));};\n\n        const auto& [id1,guardInfo1] = (*std::max_element(guards.begin(), guards.end(),\n            [](auto& l, auto& r) { return std::reduce(l.second.begin(), l.second.end()) <\n                std::reduce(r.second.begin(), r.second.end()); }));\n        const auto& minute1 = maxMinute(guardInfo1).first;\n\n        const auto& [id2,guardInfo2] = *std::max_element(guards.begin(), guards.end(),\n            [&](auto& l, auto& r) { return maxMinute(l.second).second < maxMinute(r.second).second; });\n        const auto& minute2 = maxMinute(guardInfo2).first;\n\n        std::cout << \"1: \" << minute1 * id1 << \"\\n\" << \"2: \" << minute2 * id2 << \"\\n\";\n    }\n\nI think Part 2 should be the following, since maxMinute returns an index, and not the value there!\n\n    const auto &[id2, guardInfo2] =\n            *std::max_element(guards.begin(), guards.end(), [&](auto &l, auto &r) {\n                return l.second[maxMinute(l.second)] <\n                       r.second[maxMinute(r.second)];\n            });\n\n\n\nWow, my solution was very similar to yours.\n\n    #include <unordered_map>\n    #include <vector>\n    #include <range/v3/algorithm.hpp>\n    #include <range/v3/getlines.hpp>\n    #include <range/v3/numeric.hpp>\n    #include <range/v3/view/slice.hpp>\n    \n    namespace view = ranges::view;\n    \n    template <>\n    template <bool part2>\n    void\n    Day<4>::solve(std::istream& is, std::ostream& os) {\n      std::vector<std::string> events(ranges::getlines(is));\n      ranges::sort(events);\n      std::unordered_map<int, std::array<int, 60>> guards;\n      {\n        int id {0}, start {0}, stop {0};\n        char c;\n        for (std::string const& s : events) {\n          if (sscanf(s.c_str() + 15, \"%d] %c\", &stop, &c) == 2 && c == 'w') {\n            for (auto& t : guards[id] | view::slice(start, stop)) {\n              ++t;\n            }\n          } else if (sscanf(s.c_str() + 15, \"%d] %c\", &start, &c) == 2 && c == 'f') {\n          } else if (sscanf(s.c_str() + 19, \"Guard #%d\", &id) == 1) {\n          }\n        }\n      }\n      auto max = [](const auto& l) {\n        return ranges::distance(ranges::begin(l), ranges::max_element(l));\n      };\n      auto map = [&](auto const& pair) {\n        if constexpr (auto& [_, l] = pair; part2) {\n          return ranges::max(l);\n        } else {\n          return ranges::accumulate(l, 0);\n        }\n      };\n      const auto& [id, data] = *ranges::max_element(guards, std::less<>{}, map);\n      const auto& minute = max(data);\n      os << id * minute << std::endl;\n    }\n\nI'm going to have to study this later. It's short but not hackey. Chaining algorithms ftw!\n\nWell spotted.  I think I screwed that up when trying to condense it, although weirdly I thought I checked and the answer was still correct :/\n\nI'll fix it when I get home from work.\n\nFixed now.  Interestingly, it still gave the right answer with the wrong code.  I guess in my input, the minute with the maximum sleep time just happened to always be a higher value than the minute it was compared to, ha!\n\nNice!  I've gotta start playing with ranges - they're going to change everything...\n\nI use AOC as a testbed so I can learn how to use them effectively", "id": "eb1zdih", "owner_tier": 0.3, "score": 0.12345678999999998}], "link": "https://www.reddit.com/r/adventofcode/comments/a2xef8/2018_day_4_solutions/", "question": {"content": "#--- Day 4: Repose Record ---\n\n***\n\nPost your solution as a comment or, for longer solutions, consider linking to your repo (e.g. GitHub/gists/Pastebin/blag or whatever).\n\nNote: The Solution Megathreads are for *solutions* only. If you have questions, please post your own thread and make sure to flair it with `Help`.\n\n***\n\n### Advent of Code: The Party Game!\n\n[Click here for rules](/r/adventofcode/w/aoctpg)\n\nPlease prefix your card submission with something like [Card] to make scanning the megathread easier.  THANK YOU!\n\n#### [Card prompt: Day 4](https://i.imgur.com/ZmeXVnnm.jpg)\n\nTranscript:\n> Today\u2019s puzzle would have been a lot easier if my language supported ___.\n\n***\n\n###~~This thread will be unlocked when there are a significant number of people on the leaderboard with gold stars for today's puzzle.~~\n###*edit:* Leaderboard capped, thread unlocked!", "id": "a2xef8", "title": "-\ud83c\udf84- 2018 Day 4 Solutions -\ud83c\udf84-", "traffic_rate": 38.337674714104196}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "I wrote about [the history of the Schwartzian Transform](https://www.perl.com/article/the-history-of-the-schwartzian-transform/), including its origins in Lisp.", "id": "ffd6f5w", "owner_tier": 0.3, "score": 0.6363636354545454}, {"content": "I'm not surprised. I mean programming languages borrow and steal stuff from each other since the beginning of time, but I love it when the languages that are \"cool\" right now start adding things that Perl has had for ages, even though these same people *usually* shit on Perl.\n\nI think ES6 added a few that I knew Perl had for a while. String interpolation and optional chaining just off the top of my head. Just waiting for autovivification to inevitably arrive and be touted for how awesome and novel it is.\n\n> but I love it when the languages that are \"cool\" right now start adding things that Perl has had for ages\n\nOr things that aren't so much \"cool\" as \"needed for security\".\n\nThe one that seemed to hit a number of languages a few years ago (with papers and [presentations](https://fahrplan.events.ccc.de/congress/2011/Fahrplan/attachments/2007_28C3_Effective_DoS_on_web_application_platforms.pdf)) was denial-of-service through hash collisions. Perl has never guaranteed hash ordering, which means the hash implementation can be changed without breaking (correctly written) code. Perl 5.8.1, back in September 2003, introduced hash randomization to thwart this attack, and various other countermeasures have been added to its hash implementation since then.\n\nRuby 2.8.7 got hash randomization in 2008. PHP 5.3.9, Python 2.6.8 and Python 3.3 only got it in 2012!\n\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode\n\nStrange, hash randomization is mentioned in the [5.8.1 perldelta](https://perldoc.pl/perl581delta#Hash-Randomisation), but the breaking change actually happened in [5.18](https://perldoc.pl/perl5180delta#Hash-randomization) in 2013. I can't determine what actually changed in 5.8.1.\n\nThe change in 5.18 was the introduction of \"Hash Traversal Randomization\" (and the `PERL_PERTURB_KEYS` environment variable), which randomizes each hash's order independently. I think the order can also change more dramatically as entries get added and removed.\n\nThe initial implementation back in 5.8.1 (with `PERL_HASH_SEED`) was a simple global random seed for all hashes.\n\nSee `perlsec` for details.", "id": "ffd4izv", "owner_tier": 0.7, "score": 0.999999999090909}, {"content": "Many years ago, O'Reilly sent me a free copy of the *Python Cookbook* along with a note thanking me for my contribution. Puzzled, I dug a bit further and found out it was because I had pointed out the derivation of the DSU sort on mailing list and that information had now been included in the book.\n\nOf course, it's originally a Lisp trick (which is how Randal knew it).", "id": "ffer36x", "owner_tier": 0.5, "score": -9.090909035659355e-10}, {"content": "Probably how List::UtilsBy::sort_by works!\n\nIn fact it is!", "id": "ffera1b", "owner_tier": 0.7, "score": 0.09090909000000001}], "link": "https://www.reddit.com/r/perl/comments/esyvc5/giving_credit_where_credit_is_due/", "question": {"content": "From the Python Wiki [https://wiki.python.org/moin/HowTo/Sorting#Sorting\\_Basics](https://wiki.python.org/moin/HowTo/Sorting#Sorting_Basics), note the last line ...  \n\n\n## The Old Way Using Decorate-Sort-Undecorate\n\nThis idiom is called Decorate-Sort-Undecorate after its three steps: \n\n* First, the initial list is decorated with new values that control the sort order. \n* Second, the decorated list is sorted. \n* Finally, the decorations are removed, creating a list that contains only the initial values in the new order.   \n...  \n\n\nAnother name for this idiom is [Schwartzian transform](http://en.wikipedia.org/wiki/Schwartzian_transform), after Randal L. Schwartz, who popularized it among Perl programmers.", "id": "esyvc5", "title": "Giving credit where credit is due.", "traffic_rate": 2.8680186170212765}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "    smaller = filter (\\(_, x, _) (_, y, _) -> x < y) xs\n\nchecks the value of the second element of a three-tuple.\n\nThe title asks a different question though:\n\n    setEl2of3tuple (x, y, z) p = (x, p, z)", "id": "gtwmok0", "owner_tier": 0.3, "score": 0.9999999966666667}], "link": "https://www.reddit.com/r/haskell/comments/mn5dmj/how_to_change_a_specific_element_in_a_tuple/", "question": {"content": " \n\nso, I have a list of trios (\\[Char\\], int, int), and I want to sort it from smallest to largest, but according to the second item of each trio. So far I have this:\n\nsort :: \\[(\\[Char\\], Int, Int)\\] -> \\[(\\[Char\\], Int, Int)\\]\n\nsort \\[\\] = \\[\\]\n\nsort (x:xs) = sort smaller ++ \\[x\\] ++ sort larger\n\nwhere\n\nsmaller = filter (<= x) xs\n\nlarger = filter (> x) xs\n\nBut this function sorts the list according to the first item, and I don't know how to change it?\n\n this type of tuple operation is being very difficult for me, I\u2019ve tried it in many ways and I can\u2019t get an answer", "id": "mn5dmj", "title": "how to change a specific element in a tuple", "traffic_rate": 13.1328125}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "The three new sorting algorithms are for sorting lists of size 3, 4, and 5, respectively. This is interesting, but if you're reading this thinking that AI techniques have found replacements for Merge Sort or Introsort or something like that, then you're going to be disappointed.\n\nYes, but they are primitives used to implement those larger sort algorithms. \n\n>The sort 3, sort 4 and sort 5 algorithms in the LLVM libc++ standard sorting library are called many times by larger sorting algorithms and are therefore fundamental components of the library. \n\n>We reverse engineered the low-level assembly sorting algorithms discovered by AlphaDev for sort 3, sort 4 and sort 5 to C++ and discovered that our sort implementations led to improvements of up to 70% for sequences of a length of five and roughly 1.7% for sequences exceeding 250,000 elements.\n\n>These algorithms were sent for review and have officially been included in the libc++ standard sorting library3. It is the first change to these sub-routines in over a decade\n\nYeah but I mean when you have lists of size 3,4 or 5, \n\nI\u2019ll see you in the dust loser\n\n>Introsort \n\nTIL about [introsort](https://en.wikipedia.org/wiki/Introsort), thanks!\n\n1.7% sounds huge! Btw, isn't 4, 5 elements a small enough list that people should have already brute forced all the combinations of comparisons and sequences and just know the optimal steps from that?\n\n> Yes, but they are primitives used to implement those larger sort algorithms.\n\nThey are. And this makes them significantly faster, but it does not make them conceptually different. I suspect that a random reader of the post title will think the latter is true.\n\n> Yes, but they are primitives used to implement those larger sort algorithms. \n\nOnly if your input is a primitive.\n\nFor all other cases they're disabled by templates.\n\nGlobal minimization techniques (GAs, Boltzmann machines) have been applied to finding best-possible small sorts since the '80s. This just changes the optimization method and the cost function.\n\n[deleted]\n\n> that people should have already brute forced all the combinations of comparisons and sequences and just know the optimal steps from that?\n\nThe orders of elements are easy to brute force, but the number of operations and the orders in which you can call them is very large. What they're doing isn't just optimizing the order of comparisons and moves to minimize each, but also optimizing the order of the different calls inside each of those to reduce those as well as combining sorting functions to reduce the cost of sorting larger sets.\n\nThey show some of the examples they found in their paper, so it's worth a read. A couple examples though:\n\n* They were able to remove a move operation from sort3 by changing the order in which they compared/swapped things. \n* They were able to optimize the results of some variable sort functions (functions that sort a variable number of elements 0-N) by using different fixed sized sort functions and using those as inputs into optimized versions of larger fixed sized sort functions.\n\nYes. There are optimal sorting networks at those sizes that are proven to be optimal", "id": "jn9ur0b", "owner_tier": 0.7, "score": 0.9999999999970336}, {"content": "The paper represents algorithms as CPU instructions in assembly. But then they are submitted to LLVM as C++ code which is then translated back to machine code. I'm curious whether that machine code is as efficient as the original CPU instructions. Perhaps there are a few more percent to be gained by generating exactly the right assembly for each CPU make and model. In general, I wonder how much power there is in our computers that will be unlocked by AI-guided code optimization.\n\nThere was a cool paper I read a while back where a team used evolutionary techniques to program FPGAs to, IIRC, determine the frequency of an analog audio input tone. Then they examined the resulting programs and found a very unexpected result. Some of the programs contained what should have been no-ops - logic blocks not connected to input or output. But when they removed the no-ops, the program's performance was degraded! The programs had evolved to solve the problem efficiently *on those individual chips*, taking advantage of tiny flaws and idiosyncrasies - logic block A leaks a tiny bit of current to logic block B on this chip, that kind of thing.\n\n[deleted]\n\nThe benefit depends on how different the target system is. Those improvements assume the architecture and relative cost of operations stays the same, which is not guaranteed in the coming decades.\n\nI worked on a system where large matrix dot product was a trivial operation, but abs() of a float was a massive performance hit. I also worked on a system where 2d Fourier transformation was near instant, but adding two integers was a challenge to be avoided.\n\nWe are likely going to need efficient sorting algorithms for those platforms in the coming decades, but there is little research in the area.\n\n[deleted]\n\nI haven't found the specific patches yet, but the code in their repo here ~~would suggest that that they're just using the assembly verbatim~~ https://github.com/deepmind/alphadev/blob/main/sort_functions_test.cc\n\nEdit: nevermind, found the patch https://reviews.llvm.org/D118029\n\nIn general, \"AI code optimization for arbitrary code\" is an extremely challenging topic. Currently, I doubt there's really even an idea about how someone would even approach designing that system.\n\nFor the algorithmic improvements that Google has discovered recently (matrix inversion, and now array sorting) they have used a variation of the same Monte Carlo Tree Search algorithm that they've been using for something like 15 years. In *extreme* brevity, this algorithm requires that the designers clearly and exhaustively define a model of the problem that the AI is meant to solve. Then, the AI learns to solve that *exact* problem. It may or may not result in some new emergent solution - but extrapolating what an AI learns from one problem to another is still an extremely undeveloped field of active research.\n\nThis is all to say, adapting AlphaZero to a clearly defined and heavily studied and deeply important abstract math problem such as sorting an array or inverting a matrix makes sense as a use case for this type of AI. Optimizing arbitrary code is still *waaay* off in the future - the possibility space of a problem that vague is so *insanely* large.\n\nSame thing happens when I remove logging statements in my code. \ud83d\ude02\n\nhttps://www.damninteresting.com/on-the-origin-of-circuits/\n\nThat sounds incredibly interesting, do you have a link to that paper by chance?\n\nYou are asking the wrong question.\n\n50 years or more ago they started asking whether AI could do hand writing recognition as well as humans. They would spend tens of thousands of dollars to show that it could recognize a few thousand digits.\n\n\u201cWhat a waste! A human could have done that in a single hour.\u201d\n\nYeah, and since then they have replaced tens of thousands of human letter sorters with computers.\n\nThis project was not designed to find a new sorting algorithm. That was a side effect. The project was intended to discover if algorithms could discover useful new algorithms.\n\nThe answer was yes. Which implies that they are just getting started.\n\nThe software they built is not called \u201calphasortalgorithm.\u201d\n\nIt\u2019s called AlphaDev. It is a general purpose engine for searching for new algorithms. And it is the VAX/VMS of such engines. Wait until we get to the M2 MacBook version.", "id": "jn9xreb", "owner_tier": 0.9, "score": 0.21655295164342925}, {"content": "Interesting and novel work. I don't see much application beyond primitives tho. \n\nMy reason for stating this is it invokes the comparitor function more often then the previous version of these algorithms [citation](https://reviews.llvm.org/D118029#3359924). Given the trivial cost of machine-primitive (float, int, etc.) comparisons this is a reasonable trade-off. Nevertheless when your comparitor function is operating on higher level objects/structs... this trade-off maybe less than desirable.\n\nThis fact is backed up in the review comments where they in no uncertain terms state the complex predicates are slower in these algorithms -> [cite-1](https://reviews.llvm.org/D118029#3344301) & [cite-2](https://reviews.llvm.org/D118029#3359924). These algorithsm are (only) invoked behind template meta-programming specialization to ensure they're not ran on non-primitives. \n\n---\n\nI believe the paper hyping this as having massive impacts **THE WORLD OVER** on sorting. As often programmers aren't just sorting lists of integers/floats. \n\nHopefully this approach can be adapted and applied to other difficult problems. Ensuring it could handle SIMD in the future maybe really interesting, specially for stuff like matrix multiplication & FFTs, where just handling raw ints & floats is really all you need.\n\n>Interesting and novel work. I don't see much application beyond primitives tho.\n\nThe work wasn't to find a sorting algorithm. And it also didn't just find a useful sorting algorithm. It also found a hashing algorithm.\n\nThe work was to automate the search for better algorithms.\n\nOne could presumably change the inputs to the meta-algorithm and say \"find me a better algorithm for string sorts\" and it might be able to do that too. If not today, then after the next paper. Or the paper after that.\n\nJudging it based on the breadth of applicability of these particular algorithms is like criticizing a handwriting recognition experiment that only handled postal codes and not also phone numbers. The search for algorithms is the point, not the details of the specific problem statement they offered to the \"search engine\" to show that it works.\n\nAgreed. I hate that we still benchmark algorithms based on primitive types. In part because of the overlap between \u201cworking with primitives\u201d and \u201conly need a partial sort\u201d.\n\nBut what\u2019s been a shadow in my mind for decades now, came to verbalization a good while ago, getting louder every day, is that O(1) access time is a dangerous myth that\u2019s slowing progress of the industry.\n\nPrimitives get closest to maintaining this fiction, so they aren\u2019t simply best case examples, they\u2019re actively harmful to the dialogs we need to be having post-Dennard. There are no saviors coming to let us continue our fictions a little longer.\n\nSerious question: In the scenario of sorting search results, it actually would just be sorting floats, right? Or whatever type their ranking values have. So in this case it would be applicable?\n\n> I believe the paper hyping this as having massive impacts THE WORLD OVER on sorting. As often programmers aren't just sorting lists of integers/floats. \n\niirc aren't there hard limits on sorting?  And we're pretty close to them I thought.\n\nIs schwartzian transform addressed as an approach to this issue?\n\na *lot* of work goes into small, crazy compiler optimizations. things like optimizing the speed of sorting very small lists are more important than you might think. this sort of thing could potentially be used when the compiler is able to infer that a loop should be unrolled into individual statements, for example.\n\nDitto. It seems a lot of people misunderstood the primary purpose of the paper/AlphaDev. Once we have a model, we can chain different ML models together and they can do general or special algorithm optimization.\n\nthe \"hashing algorithm\" is kind of.. not impressive? it looks like they just took the simplest thing that didn't obviously fail any tests and threw it in absl so they could point to it as \"hey look it's doing real work!\"\n\nhttps://news.ycombinator.com/item?id=36229099\n\n> Distribution on real workloads was good, it **almost passed smhasher** and we decided it was good enough to try out in prod. We did not rollback as you can see from abseil :)\n\nyeah the \"hashing algorithm\" definitely doesn't look like it was taken seriously by anyone other than pr\n\nFYI I editted out the part of memory obliviousness. \n\nAfter re-reading the paper I lowered my expectations. This approach doesn't generalize to higher abstraction optimization problems like that. \n\nAt best it'd able to understand SIMD, so maybe faster FFT or matrix multiplication algorithms, _hopefully_.", "id": "jna0gx1", "owner_tier": 0.9, "score": 0.10145357460397508}, {"content": "Great, I can\u2019t wait to be asked to implement it in interviews", "id": "jna3en2", "owner_tier": 0.9, "score": 0.04805695638979531}, {"content": "I have a genuine question: I remeber many *many* years ago from my algorithm analysis lecture that mathematically sort algorithms cannot get better than O(n.logn). What am I missing?\n\nThis is still true, a comparison-based sort will always need at least `n log n` comparisons to sort any list. But the actual implementation in code can still be faster or slower, especially when we consider how complex modern CPUs are with branch prediction and etc.\n\nO(n.logn) is the best possible *asymptotic* complexity of sorting in the *general case*.\n\nSo, 5n.logn and 2n.logn are both O(n.logn), but the latter one is clearly faster.\n\nAdditionally, if you have additional constraints on the data you can apply faster algorithms that are tailore for the specific case. For example, sorting n keys of length w lexicographically can be done in O(w.n) with radix sort. Which is faster than O(n.logn) for w < logn\n\nThey can - see radix sort\n\nIf you compare elements, then asymptotically it can't be better, but there are linear sorts like counting or radix, and you can also have different constants.\n\nWhen you say O(whatever), you're describing how the speed *changes* as the size of the list changes, for sufficiently large lists.\n\nSo if going from sorting 100 elements to 200 elements takes 4 times as long, it's O(n^2).\n\nSo let's say sorting 100 elements takes 1 second, and 200 takes 4 seconds. I come along and I present my new sorting algorithm. It takes 0.01 seconds to sort 100 elements, and 0.04 seconds to sort 200 elements. Still grows by the square of the number of inputs, so it's still O(n^2). But mine is 100x faster.\n\nSaying something is O(n log n) is a useful shortcut to say \"it's probably pretty alright\", but it's just one metric that doesn't come close to fully describing the speed of an algorithm.\n\nthese are sort3 sort4 and sort5\n\nyes..\n\n 3 elements, 4 elements and 5.   thats it.  the ability to do these well is not O(n) notation based.\n\nYou can get O(1) performance with the intelligent design sort: the list has already been perfectly sorted by a divine Sorter according to criteria beyond our mortal comprehension, and reordering the list would be an affront against nature.\n\nIt's worth adding that asymptotic complexity tells you what happens when n tends to infinite, it's meaningless for very low numbers. For example if the exact number of operations is given by the function *5+n\\^2*  this means it's assymtotically worse than *10+10\\*n* but it isn't worse for all n, for n=<10 it's still better.\n\nThis isn\u2019t breaking any Big O barrier, it\u2019s squeezing out better real-world performance through low-level optimization of small-n subproblems.\n\nU can still optimise for constant factor. nlgn might be 1000000nlgn, decreasing that large number would yield speed ups", "id": "jnahj67", "owner_tier": 0.3, "score": 0.07030554731237021}, {"content": "More recently, Google has also merged [Bitsetsort](https://github.com/minjaehwang/bitsetsort) to [LLVM's std::sort() implementation](https://github.com/llvm/llvm-project/commit/4eddbf9f10a6d1881c93d84f4363d6d881daf848), which actually has a more significant performance improvement in general cases (but was developed by hand on top of other optimized sorts, not with machine learning).\n\nOne of the reasons it's so fast is that parts of it are carefully written so LLVM generates SIMD instructions.\n\nThis is one of the things I found most interesting. Cause it's trying to optimize against the real hardware. Current hardware is so complex we may be missing some tricks.", "id": "jnb36nz", "owner_tier": 0.5, "score": 0.008602788487095817}, {"content": "This seems minor but hyped to be huge.\n\nReddit is big and pretty stupid. I see here only 3-4 competent commentators besides me. This cannot be avoided with their approach to communities and commenters.\n\nI don't understand why you are blaming Google here. They have nothing to do with it.", "id": "jnb3ot1", "owner_tier": 0.3, "score": 0.001483239391871848}, {"content": "Now they can sort ads into my search results even faster", "id": "jncympi", "owner_tier": 0.5, "score": 0.0011865915129041827}, {"content": "Coolest research paper I've read this year or possibly ever.", "id": "jnaij8r", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "Applying deep learning to fundamental algorithms is so fucking cool. Deepmind also found a more efficient matrix multiplication algorithm last year, which is obviously HUGE for machine learning.", "id": "jnc4rwn", "owner_tier": 0.1, "score": 0.0008899436339365173}, {"content": "Could someone explain the psuedocode for the original sort3? I am having a hard time following, the way I read it the original value B can never be placed in Memory\\[2\\], and as such will not correctly sort when B is the largest value. I must be misinterpreting something and would really appreciate some clarification, so I can then try and understand the AlphaDev solution.\n\n&#x200B;\n\nPosted a question about this on reddit [here](https://www.reddit.com/r/compsci/comments/143wh6u/understanding_alpha_dev_sorting_algorithm/).\n\nHave the same question, it seems like they are focusing on a specific set of comparisons and leaving off the step comparing B and C, since presumably AlphaDev uses the same approach for this portion.\n\nAlso if you compare the assembly (b) and (c), then you see that they only removed an instruction, nothing else changed except for the comments.", "id": "jncl69d", "owner_tier": 0.1, "score": 0.001483239391871848}, {"content": "Now if only Google would make Google search great again ...\n\nThe whole AI transition comes with a massive reduction in service quality. It has been hugely disappointing so far. Quality goes down, but tons of promo articles babble about the epic future. It's bizarre.", "id": "jnd43nd", "owner_tier": 0.7, "score": 0.000593295754968852}, {"content": "Over a year ago, committed 8.April 2022", "id": "jnddsm1", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "I\u2019m planning to apply it to sort billions of rows using only 32GB of memory. Previously I had completed algorithm for billion-row Distinct, GroupBy, Filter and JoinTable. Sorting is the last one I cannot solve at the moment.", "id": "jne55rc", "owner_tier": 0.1, "score": 0.000593295754968852}, {"content": "https://news.ycombinator.com/item?id=36231147 debunks the hype here.", "id": "jned149", "owner_tier": 0.5, "score": 0.000593295754968852}, {"content": "Imagine spending time optimizing the sorting of lists sized n < 10 in 2023.\n\nThis is coming from a recovering micro-optimization fiend.", "id": "jnd6fsi", "owner_tier": 0.3, "score": 0.00029664787600118655}, {"content": "And so it begins. AI is now enhancing portions of itself.", "id": "jnckj5u", "owner_tier": 0.9, "score": -2.9664787896766537e-12}, {"content": ">It is the first change to these sub-routines in over a decade.\n\nThis shouldn't say much, since C++'s std is still missing a half-century (or century-old) algorithm.", "id": "jndmudh", "owner_tier": 0.1, "score": 0.00029664787600118655}], "link": "https://www.reddit.com/r/programming/comments/143gskm/google_finds_faster_sorting_algorithm_using_deep/", "question": {"content": "", "id": "143gskm", "title": "Google finds faster sorting algorithm using deep reinforcement learning", "traffic_rate": 935.2067938021454}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}, {"answers": [{"content": "Got to be hash slices.\n\n    my @cols = qw[col1 col1 col3];\n\n    my @data;\n    while (<$some_file_handle>) {\n      chomp;\n      my %row;\n      @row{@cols} = split /\\t/;\n      push @data, \\%row;\n    }\n\nI use this all the time. And it's always a big \"wow\" moment when I introduce it on my intermediate level course.\n\nWould \n\n@rows = ('a','b','c');\n\n@hash{@rows} = (1, 2, 3);\n\nMake\n\n$hash{a} == 1\n\n\n?\n\nWorks especially well IMO for stuff like CSV with header rows.\n\n    #!/usr/bin/env perl\n\n    use strict;\n    use warnings;\n \n    use Data::Dumper;\n\n    #read header row. \n    chomp ( my @header = split /,/, <DATA> ); \n\n    my @rows; \n \n    #iterate other rows\n    while ( <DATA> ) {  \n       chomp;\n       my %row;\n       #use hash slice to insert into named values. \n       @row{@header} = split /,/; \n       #push this row into the larger data structure. \n       push @rows, \\%row; \n    }\n\n    print Dumper \\@rows; \n\n    __DATA__\n    field1,field2,field3\n    value,anothervalue,someothervalues\n    fish,bird,carrot\n\n\n\nI'm assuming the opened file is delimited by tabs? \n\nExactly.\n\nOf course, the usual disclaimer applies; if you hand-roll your CSV parsing, one day you'll choke on something you didn't expect, e.g. quoted values or multi-line rows - for instance:\n\n    foo,\"Hi, this is one field\",42\n\nIf you use Text::CSV, (or Text::CSV::XS) then it's a battle-tested solution that won't fall over little gotchas like that, like your code would have.  (If you control the generation as well as the consumer of the CSV then you can be more confident hand-rolling if you really need to, but even then I'd still tend to just use Text::CSV personally.)\n\nIt can also handle the \"give me the data as a hashref\" cleanly for you, e.g. :\n\n    open my $fh, \"<\", $filename or die \"Failed to open $filename - $!\";\n    my $csv = Text::CSV->new;\n    $csv->column_names($csv->getline($fh));\n    while (my $row = $csv->getline_hr($fh)) {\n        # $row is a hashref whose keys are the column names\n    }\n\n\nIn this example, yes. But the concept is easy to use for other file types.\n\nwell that is a massive time saver, thanks for that gem\n\nGranted. But most of the time, I find Text::CSV to be altogether too heavyweight for _most_ usage scenarios - sure, when you're handling quotes and whatnot it's absolutely my go-to. (Although sometimes that's `Text::Parsewords`).\n\nBut _most_ times CSV is not that complicated. \n\nSo this tool is used to create 2D indexing of text files or CSV  files? ", "id": "dnnxv46", "owner_tier": 0.5, "score": 0.9782608693478261}, {"content": "Using `my` in a for^list loop doesn't actually declare a new variable - it just renames `$_` - which means if you alter it you actually alter the value in the list itself.\n\nTry this:\n\n    use strict;\n\n    my @cars = qw( BMW Audi Volvo );\n    for my $car ( @cars ) {\n        $car .= \" drivers suck\";\n    }\n    print( join( \", \", @cars ) );\n\nThis outputs:\n\n    BMW drivers suck, Audi drivers suck, Volvo drivers suck\n\nWhich is not what you'd expect. This bit me several times until I learned about this quirk.\n\nThis has nothing to do with `$_`. This is because `for/foreach` aliases each value, rather than copying it, like subroutines alias their arguments to the elements of `@_`.\n\nIn fact, this is one reason why using `$_` in a for loop can be dangerous even though it localizes the variable; if you call code inside your for loop that modifies `$_` for its own purposes, that will modify whatever the for loop is iterating through!\n\nI've been using Perl for 3 years and you opened my sir!) Thank you.\n\nSorry, I can't see why that is surprising. That's what I thought would happen.\n\nOne of my favourite uses of this is trimming leading and trailing whitespace:\n\n    my @foo = ('     foo  ', ' bar    ');\n    s/^ +//, s/ +$// for @foo;\n\nYeah, 'my' is scope. Same as 'our'.\n\nWhat does the following code do?\n\n    use strict;\n\n    my @cars = qw( BMW Audi Volvo );\n    for ( my $i = 0; $i < scalar @cars; $i++ ) {\n        my $car = $cars[ $i ];\n        $car .= \" drivers rule\";\n    }\n    print( join( \", \", @cars ) );\n\n\nIt's very useful in general for quickly making the same modification to every element of an array.\n\n    my @foo = ('     foo  ', ' bar    ');\n    $_ = join(' ', split(' ', $_)) for @foo;\n\nAlthough it's kind of convoluted, this can be a faster way if you can tolerate (or actually want to) knock all whitespace down to a single space. The ' ' argument to split is a special case condition which automatically trims and flattens whitespace.\n\n    use strict; \n    my @cars = qw( BMW Audi Volvo ); \n     for ( my $i = 0; $i < scalar @cars; $i++ ) { \n             my $car = $cars[ $i ]; \n             $car .= \" drivers rule\"; \n     } \n     print( join( \", \", @cars ) );\n\n\nbut that is completely different. Without trying it I would say that it prints out\n\n     \"BMW, Audi, Volvo\"\n\nNow, I tried it, and that's what it prints. There is no aliasing of the member of @cars, it is copied out in a sub-scope, and nothing changes at the outer scope.\n\n\n\n\nYet in both cases you use `my` to declare a new variable and you only alter this new variable.\n\nBut in the first case even altering this new variable actually changes the list.", "id": "dnnwesd", "owner_tier": 0.5, "score": 0.9999999997826087}, {"content": "I find `//` to be particularly useful. It's not comment, it's a 'defined' test, that works like `||`. \n\nSo you can do:\n\n     print $value // 'default',\"\\n\";\n\nAnd if `$value` is undefined, then you get a default. But _unlike_ `||` it handles empty string and zero differently (which are boolean false, but defined). \n\nYou can also use it for setting defaults with `//=`:\n\n    my $thing //= 'default value here'; \n\n", "id": "dno693s", "owner_tier": 0.9, "score": 0.23913043456521738}, {"content": "I've learned some of the coolest Perl stuff by reading the \"perldelta\" manpages. They contain all the new features, and a list of things that will be obsoleted in the new version of Perl.", "id": "dnnxtww", "owner_tier": 0.7, "score": 0.15217391282608697}, {"content": "I will second /u/davorg about Hash slicing. Particulalry nice when you have a function that takes a lot of arguments, so you use named args... but you also use them a lot so you want to assign them to scalars up front.\n\nThis is a trivially short example of a higher-order function, but you get the idea\n\n    sub process( %args ) {\n        my ( $data, $pred, $proc ) = @args{qw( data pred proc )};\n        return map { $proc->($_) } grep { $pred->($_) } $data->@*\n    }\n\nI'm also loving using signatures and post-deref as you can see.\n\nBut for more classic fave tip, I can't go past `split(' ')`. The `split` function typically takes a regex. Even if you give it a \"string\" it's treated as a regex pattern.\n\nThe exception to this is the ~~single-quoted~~ single space `' '`. This is special syntax that splits a string by any arbitrary whitespace, ingnoring leading and trailing whitespace.\n\n    my $line = \"    oddly    spaced     words  \";\n\n    my @words = split( ' ', $line );\n    # @words = ('oddly', 'spaced', 'words')\n\nI do a lot of text processing and I use this almost every time.\n\nNote that `/ /` does not do the same thing, only `' '`.  \n\nAlso, a word of warning... If you like your regex patterns with insignificant whitespace so much you `use re '/x'`, then this trick doesn't work. The most obvious solution is to do it inside a function that lexically disables  the `re` pragma\n\n    use re '/x';\n\n    sub words( $str, $n = 0 ) {\n        no re;\n        return split( ' ', $str, $n );\n    }\n\n    my $line = \"    oddly    spaced     words  \";\n\n    $line =~ / s p a c e d / && say join( ', ', words($line) );\n\nThe `$n` retains the ability to control the number of times to split.\n\nEdit: Updated to reflect whitespace split works with `split(\" \")`\n\nAgreed on these, just one very minor point: \" \" or a scalar variable containing a single space will have the same effect - i.e.\n\n    perl -e'split \" \"'\n\nworks the same way as your split ' ' example.\n\nUnlike Python, we don't need a (named) function to make a lexical scope. Any block will do.\n\n    $line =~ / s p a c e d / && do {\n        no re;\n        say join ', ', split ' ', $line;\n    }\n\n    if ($line =~ / s p a c e d /) {\n        no re;\n        say join ', ', split ' ', $line;\n    }\n\n\nAhh, you are correct, sir. I could have sworn that double-quoted string behaved differently. Damn false memories.\n\nTrue, but I've used my `words()` sub about 8 times in my current project. I don't wanna repeat myself, and creating lexical block everywhere is not always convenient. For example, I'm using inside a conditional which makes for some nicely readable code.\n\n    if ( words($input) == 1 ) { ... }\n", "id": "dnnzrdw", "owner_tier": 0.3, "score": 0.17391304326086957}, {"content": "dispatch tables \n\n    my $dtable = { \n        something         => \\&BLA::Today::do_something,\n        something_else => \\&BLA::Today::do_something_else,\n    };\n\nsymbol table manipulation : read the source for Class::Data::Inheritable,  saw the light ", "id": "dnofair", "owner_tier": 0.1, "score": 0.10869565195652174}, {"content": "`map` is an awesome function, and you can do a lot of cool things with it. \n\nA pretty standard:\n\n\n    my @things = qw ( fish carrot banana potato );\n    my %is_thing = map { $_ => 1 } @things; \n\n    print \"'fish' is a thing\\n\" if $is_thing{'fish'};\n\nYou can select 'paired' values straight into a hash. \n\nSo given some text like:\n\n    test=woilla\n    test2=gronk\n    fleeg=flirble\n\nYou can hashify this:\n\n    my %value_of = $str =~ m/(\\w+)=(\\w+)/g; \n\n\n\nBut why not just use a for loop and make it easier for whomever is coming behind you to read. I have been writing perl since perl v2 and I can't find a really good reason to use map other than to be perish. I love perl, probably too much, but I hate perl code that is hard to read just because someone wanted to be perlish. I am sure that I have never done that \ud83d\ude1c ... Err not that I would admit to in Reddit \n\nUsing map for this *is* the easy, idiomatic way. \n\nIt's perlish? This **is** Perl. That's like complaining something written in Java is javaish.\n\nIMO, I think that you say that because you're more use to for loops. I prefer map personally.\n\nI didn't grok `map` until I realised it's a list comprehension in the functional programming sense. \n\nBeing able to manipulate whole lists is surprisingly useful. \n\nYes it is idiomatic and there are a lot of Perl idioms that I do like; however, I do not believe in sacrificing readability solely for the sake of being perlish. ", "id": "dno5x4w", "owner_tier": 0.9, "score": 0.17391304326086957}, {"content": "My tip is use [Mojo::DOM](https://metacpan.org/pod/Mojo::DOM) for HTML parsing.\n\nIn the past, I used to use `HTML::TokeParser` then `HTML::TokeParser::Simple` and people recommended all sorts of XLTS and Tree parsers and they all were awkward to use.\n\nSo as far as blowing minds goes, `Mojo::DOM` did that when I first found it. I already knew CSS well at the time, so perhaps that helped the ease of use, but compared to `HTML::TokeParser`, `Mojo::DOM` plays in a whole 'nother league.\n\n-----\n\nBut if you meant core-Perl tip, then I'd say it's that the `/.../` in list context returns the captures and `/.../g` returns the matched substrings if there are no captures in the regex:\n\n    say join \", \", \"foo bar ber\" =~ /\\w+ (\\w+) (\\w+)/; # OUTPUT: bar, ber\n    say join \", \", \"foo bar ber\" =~ /\\w+/g;            # OUTPUT: foo, bar, ber\nNice and concise and no `$1, $2` to deal with.\n    \n\ndo you know what the logic is for /g having that functionality, as opposed to, say, /r ?\n\nOtherwise, wow, TIL. \n\nI ask because /g with capture groups 'carries on going for the whole string'\n\n     my $x = \"cat dog house\";\n     while ($x =~ /(\\w+)/g) {\n\t \tprint \"Word is $1, ends at position \", pos $x, \"\\n\";\n\t }\n\nprints\n\n    Word is cat, ends at position 3\n    Word is dog, ends at position 7\n    Word is house, ends at position 13\n\nand /r is the \"return the result of the match\". \n\n> do you know what the logic is for /g having that functionality, as opposed to, say, /r ?\n\nNope, don't know. I don't think `/r` existed that far back yet.\n\n`/r` applies to `s///`, not `m//`, which has different return values to begin with. There's no `/r` modifier for `m//`.\n\nYour example is running `m//g` in scalar context (whereas Zoffix's example is in list context), which changes both its behavior and return value, something that can be surprising to newcomers but was probably intended to be \"intuitive\".\n\nWell, one thing is completely clear: you *definitely* know what you're talking about. ", "id": "dnog2s7", "owner_tier": 0.3, "score": 0.13043478239130435}, {"content": "The [Schwartzian transform](https://en.wikipedia.org/wiki/Schwartzian_transform). I don't often have a need for it, but it's a very cool way to sort an array by an additional attribute. There's no need for a secondary data structure that keeps using memory after the sort is done, and the attribute is only found/computed once for each element. \n", "id": "dnoi9cn", "owner_tier": 0.7, "score": 0.06521739108695652}, {"content": "    eval { may_die(); 1 } or print \"boom\";\n\nInstead of\n\n    eval { may_die() };\n    print \"boom\" if $@;\n\n\nTo give some more context, this is an important construct for anyone using plain eval instead of Try::Tiny or Syntax::Keyword::Try. Never rely on the value of `$@` to determine if an error occurred; but if an error occurred, eval will always return undef, and you can rely on that.", "id": "dnpe04u", "owner_tier": 0.1, "score": 0.08695652152173913}, {"content": "`say $x` is prettier than `print $x . \"\\n\"`\n\nEmbrace post-dereferencing.\n\n    $a = { a => 1 } ;\n    $b = $a ;\n    $b->{ a } = 2 ;\n    say $a->{a} ; # 2, because references\n    my $c = { $a->%* } ;\n    $c->{ a } = 4 ;\n    say $a->{a} ; # 2, because values\n    say $c->{a} ; # 4, because values\n\nPlus, the Schwartzian Transform is a bit mindbending, but once you understand it, it's **very** powerful.\n\nSurely you'd never type `print $x . \"\\n\"` rather than `print \"$x\\n\"`?", "id": "dno7sac", "owner_tier": 0.7, "score": 0.08695652152173913}, {"content": "Not trick per se, but the first time I ran into lexical post-if:\n\n    my $var = some_stuff() if whatever_expression();\n\nIf you don't know it: it's a horrible bug waiting to happen, because the lexical is only created if the condition is true, but in the scope of the surrounding code, because post-if doesn't create its own scope. So if you have a variable with the same name further up the calling chain, that value will end up being used.\n\nMy tip: Use a linter or a PPI based test to catch them in your code.\n\nPeople deliberately use this to give them persistent variables in subroutines;\n\n    #!/usr/bin/perl\n\n    use strict;\n    use warnings;\n\n    sub foo {\n      my $foo if 0;\n\n      return ++$foo;\n    }\n\n    print foo(), \"\\n\" for 1 .. 4;\n\nThis now gives a deprecation warning and can be replaced by `state`.\n\n    #!/usr/bin/perl\n\n    use strict;\n    use warnings;\n    use feature 'state';\n\n    sub foo {\n      state $foo;\n\n      return ++$foo;\n    }\n\n    print foo(), \"\\n\" for 1 .. 4;\n\n\n\n\nlike the acceptable : \n      my $var = 32 if 0;\n\n\nFWIW there is a [core perlcritic policy](https://metacpan.org/pod/Perl::Critic::Policy::Variables::ProhibitConditionalDeclarations) for this, and I've subclassed it into the [freenode theme](https://metacpan.org/pod/Perl::Critic::Policy::Freenode::ConditionalDeclarations).\n\nAlong with being [deprecated](https://metacpan.org/pod/perldeprecation#Using-my(\\)-in-false-conditional.) for a while now as you said, this functionality seems to have been accidental due to how `my` was implemented with both compile-time and run-time behavior. It's certainly not documented or tested anywhere.", "id": "dnogxpi", "owner_tier": 0.7, "score": 0.26086956499999997}, {"content": "Autovivification, which is such a cool Perl trick that [the wikipedia page](https://en.wikipedia.org/wiki/Autovivification) starts out \"In the Perl programming language...\"\n\nGiven a bunch of lines that have devices and genres (this was from an iTunes example that I just tried -- full example further down)...\n\n    my ($device, $genre) = split(/\\t/, $_); \n    $index->{$device}->{$genre}++; \n \nWe get so used to this, that it's easy to take for granted.  But in most languages you can't do this sort of thing so easily.\n\nYou can go as many levels deep as you want, and you can change up the grouping by just moving around where your variables are in the hash.  \n\nIt's very powerful and is a handy party trick (well okay, a work trick).\n\nHere's the full example, which assumes you have `curl` and `jq`:\n\n    curl -s \"https://itunes.apple.com/search?term=football&country=us&entity=software\"  | jq -r '.results[] | .genres as $genres | .supportedDevices as $devices | $genres[] | . as $genre | $devices[] | [.,$genre] | join(\"\\t\")' |  perl -MData::Dumper -lne '($device, $genre) = split(/\\t/); $index->{$device}->{$genre}++; print Dumper($index) if eof'\n\nAnd, if you don't have `jq` or prefer the pure-Perl version...\n\n    curl -s \"https://itunes.apple.com/search?term=football&country=us&entity=software\"  | perl -MJSON -MData::Dumper -lne 'push(@out, $_); if (eof) {  my $index = {}; my $data = from_json(join(\"\", @out)); for my $result(@{$data->{results}}) { my $genres = $result->{genres}; my $devices = $result->{supportedDevices}; for my $genre(@{$genres}) { for my $device(@{$devices}) { $index->{$device}->{$genre}++ } } } print Dumper($index) } '\n\n**Autovivification**\n\nIn the Perl programming language, autovivification is the automatic creation of new arrays and hashes as required every time an undefined value is dereferenced. Perl autovivification allows a programmer to refer to a structured variable, and arbitrary sub-elements of that structured variable, without expressly declaring the existence of the variable and its complete structure beforehand.\n\nIn contrast, other programming languages either: 1) require a programmer to expressly declare an entire variable structure before using or referring to any part of it; or 2) require a programmer to declare a part of a variable structure before referring to any part of it; or 3) create an assignment to a part of a variable before referring, assigning to or composing an expression that refers to any part of it.\n\nPerl autovivification can be contrasted against languages such as Python, PHP, Ruby, and many of the C style languages, where dereferencing null or undefined values is not generally permitted.\n\n***\n\n^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/perl/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot)   ^]\n^Downvote ^to ^remove ^| ^v0.27\n\nGood bot", "id": "dnp15uf", "owner_tier": 0.7, "score": 0.10869565195652174}, {"content": "In my case, and back when I was learning, the whole array slicing syntax deal was just a delight to learn and use.\n\n\nBut an actual tip would be using -e to check for file existence.\n\n\n\n    if -e './whatevs.txt' ...;\n\nAnd to be sure its a file\n\n-f", "id": "dnntcco", "owner_tier": 0.9, "score": 0.08695652152173913}, {"content": "What about hashref slices?\n\n    my $hr = { qw(one 1 two 2 three 3 four 4) };\n    my @odd = @{$hr}{qw(one three)};\n\nInterpolating *anything* into strings, not only scalars and arrays:\n\n    \"localtime @{[ join '-', localtime ]}\"\n    \"2 + 2 = @{[ 2 + 2 ]}\"\n\nWriting `sort` like functions with `$a` `$b`:\n\n    sub pairgrep(&@) {\n        my $func = shift;\n        my $caller = caller;\n        my @res;\n        while (@_) {\n            my $key = shift;\n            my $val = shift;\n            no strict 'refs';\n            local *{\"$caller\\::a\"} = \\$key;\n            local *{\"$caller\\::b\"} = \\$val;\n            push @res, $key, $val if $func->();\n        }\n        @res;\n    }\n\nAnd then\n\n    my %h = qw(one 1 two 2 three 3 four 4);\n    my %even = pairgrep { $a ~~ [qw(two four)] } %h;\n\nI know I know, it's core in  `List::Util` from Perl 5.20, but I badly wanted it in older versions.\n\nAnd something I learned yesterday. What happens with a hash in scalar context?\n\n    %h = qw(one 1 two 2 three 3 four 4);\n    print scalar %h, \"\\n\";\n    print %h . \"\\n\";\n    3/8\n    3/8\n\nWAT? Number of used/all hash buckets!\n\n> What about hashref slices?\n>\n>     my $hr = { qw(one 1 two 2 three 3 four 4) };\n>     my @odd = @{$hr}{qw(one three)};\n\nAnd now with postfix dereferencing:\n\n    my @odd = $hr->@{qw(one three)};\n\n\nHashes in scalar context now (since 5.26) returns the number of keys, since the hash bucket details are slower to calculate, implementation dependent, and really not relevant to the programmer. If you really needed the implementation details for some reason, you can use [Hash::Util](https://metacpan.org/pod/Hash::Util).\n\n> And now\n\nFrom Perl 5.20 upwards, with\n\n    use feature qw(postderef);\n\nand it's experimental up to 5.22.\n\nI see. So an unintuitive solution was chosen instead of the original unintuitive solution. I expected it to be the number of keys plus values, due to the analogy below:\n\n    %h = qw(one 1 two 2);\n    @a = %h;\n    $s = @a; #4\n\nfeature.pm isn't needed to enable it once it became no longer experimental (5.24+), since it doesn't conflict with existing syntax.\n\nAlso reasonable, but another perspective is that the logical \"size\" of a hash is the number of keys it stores. I am just glad it's something useful (and faster) now.\n\nagreed.", "id": "dno63dq", "owner_tier": 0.1, "score": 0.39130434760869565}, {"content": "references - before their existence, I used any number of old, lame methods to encode some form of data structure into a string, list or associative array. references have me freedom to go wild and shape the structure on the fly the way that I wanted. Wow!!!!\n\nWhile I am not a computer scientist and am ignorant in many if not most languages, I don't know of any language with similar capability.\n\n- edited to correct typos\n\nMost scripting languages have this capability, or the capability of multi-dimensional data structures, in some form; it would be very difficult to do anything complex without it.\n\nFun fact: since perl 4 didn't have references, it had a shortcut to generate fake multidimensional arrays, which still exists in perl 5 today, but you should probably never use it, and it looks confusingly like a hash slice: https://metacpan.org/pod/perldata#Multi-dimensional-array-emulation\n\nYep, used or in the olden days", "id": "dnoxjbf", "owner_tier": 0.1, "score": 0.06521739108695652}, {"content": "The `dbmopen` command which ties a hash to a local on-disk database. Years before sqlite, I believe, and doesn't require a module.\n\nIt's deprecated and there are much better ways to do it, but sooner or later you need some kind of simple, *ad hoc* database and `dbmopen` is the quickest way to do it.\n\nFWIW, this was superceded aaages ago by the core module [DB_File](https://metacpan.org/pod/DB_File). Another one for similar usecases is [DBM::Deep](https://metacpan.org/pod/DBM::Deep). But I tend to prefer SQLite anyway, though I might be [biased](https://metacpan.org/pod/Mojo::SQLite); it's generally going to be much more efficient in the long run if you're doing anything other than reading and writing the entire structure at once.\n\nI'm totally aware it's been superseded, but it's been superseded by more complex things. the dbm thing is instant, and there's practically no syntax to remember. You use one command, and now you have a hash which is saved to disk.", "id": "dnp2jqy", "owner_tier": 0.7, "score": 0.08695652152173913}, {"content": "What I forget again and again and am amazed when seeing in a code is using `values` where keys aren't important, especially as lvalues.\n\n\nE.g.\n\n    $_++ for values %count;\n\nor\n\n    say sum(map $hash{$_}{count}, keys %hash);\n    # versus\n    say sum(map $_->{count}, values %hash);\n", "id": "dnq2myd", "owner_tier": 0.1, "score": 0.021739130217391305}, {"content": "[P](https://stackoverflow.com/a/6163129/1039320)[e](http://world.std.com/~swmcd/steven/perl/pm/xs/intro/index.html)[r](http://www.masteringperl.org/2015/05/computing-excellent-numbers/)[l](http://www.linuxcareer.com/perl-as-a-career-option) [c](http://irclog.perlgeek.de/perl6/2013-10-17#i_7723873)[o](http://www.perl.com/pub/2005/07/14/bestpractices.html)[m](http://www.wall.org/~larry/pm.html)[m](http://interviews.slashdot.org/story/02/09/06/1343222/larry-wall-on-perl-religion-and)[u](http://www.perlmonks.org/bare/?node=Erudil)[n](http://rjbs.manxome.org/rubric/entry/1959)[i](http://blogs.perl.org/users/damian_conway/2012/03/why-i-love-my-job.html)[t](http://perlbuzz.com/2010/11/29/think_for_perls_sake/)[y](http://www.perlmonks.org/?node_id=1180698)...\n\n^ generated via this script:\n\n    #!/usr/bin/perl\n    use strict;\n    use warnings;\n    \n    chomp( my @links = reverse <DATA> );\n    \n    print map /\\s+/    # don't turn whitespace into a link\n      ? $_\n      : \"[$_](\" . pop(@links) . \")\",\n      split //, 'Perl community';\n    \n    print \"...\\n\\n^ generated via this script:\\n\\n\";\n    open my $script, \"<\", $0 or die \"Can't open $0: $!\";\n    print \"    $_\" while <$script>;\n    \n    __DATA__\n    https://stackoverflow.com/a/6163129/1039320\n    http://world.std.com/~swmcd/steven/perl/pm/xs/intro/index.html\n    http://www.masteringperl.org/2015/05/computing-excellent-numbers/\n    http://www.linuxcareer.com/perl-as-a-career-option\n    http://irclog.perlgeek.de/perl6/2013-10-17#i_7723873\n    http://www.perl.com/pub/2005/07/14/bestpractices.html\n    http://www.wall.org/~larry/pm.html\n    http://interviews.slashdot.org/story/02/09/06/1343222/larry-wall-on-perl-religion-and\n    http://www.perlmonks.org/bare/?node=Erudil\n    http://rjbs.manxome.org/rubric/entry/1959\n    http://blogs.perl.org/users/damian_conway/2012/03/why-i-love-my-job.html\n    http://perlbuzz.com/2010/11/29/think_for_perls_sake/\n    http://www.perlmonks.org/?node_id=1180698\n", "id": "dnowi15", "owner_tier": 0.1, "score": -2.1739130302663674e-10}], "link": "https://www.reddit.com/r/perl/comments/735nq2/whats_your_favorite_perl_tip/", "question": {"content": "Anything that blew your mind when you learned it, for example? Whether it's popularly known or not?", "id": "735nq2", "title": "What's your favorite Perl tip?", "traffic_rate": 2.8680186170212765}, "saved_time": "Tue, 16 Jul 2024 03:57:51 GMT", "source": "reddit"}]}