{"result": [{"answers": [{"content": "I think using threading wouldnot be really a bad idea", "id": 77491031, "owner_tier": 0.1, "score": -1.0638297872340426e-10}, {"content": "I have done just a basic test below, but it shows that runpy can be used to solve this issue when you need to have a whole Python script to be faster (you don't want to put any logic in test_server.py). test_server.py test_client.py test_lag.py Testing: Based on this, module is pre-loaded for fast usage.", "id": 74382868, "owner_tier": 0.5, "score": 0.010638297765957446}, {"content": "You can use lazy imports, but it depends on your use case. If it's an application, you can run necessary modules for GUI, then after window is loaded, you can import all your modules. If it's a module and user do not use all the dependencies, you can import inside function. [warning]\nIt's against pep8 i think and it's not recomennded at some places, but all the reason behind this is mostly readability (i may be wrong though...) and some builders (e.g. pyinstaller) bundling (which can be solved with adding missing dependencies param to spec) If you use lazy imports, use comments so user knows that there are extra dependencies. Example: For some specific libraries i think it's necessity. You can also create some let's call it api in __init__.py For example on scikit learn. If you import sklearn and then call some model, it's not found and raise error. You need to be more specific then and import directly submodule. Though it can be unconvenient for users, it's imho good practice and can reduce import times significantly. Usually 10% of imported libraries cost 90% of import time. Very simple tool for analysis is line_profiler This give results 75% of three libraries imports time is matplotlib (this does not mean that it's bad written, it just needs a lot of stuff for grafic output) Note: If you import library in one module, other imports cost nothing, it's globally shared... Another note: If using imports directly from python (e.g pathlib, subprocess etc.) do not use lazy load, python modules import times are close to zero and don't need to be optimized from my experience...", "id": 70496615, "owner_tier": 0.5, "score": 0.04255319138297872}, {"content": "Not an actual answer to the question, but a hint on how to profile the import speed with Python 3.7 and tuna (a small project of mine): ", "id": 51300944, "owner_tier": 0.9, "score": 0.9999999998936171}, {"content": "You can import your modules manually instead, using imp. See documentation here. For example, import numpy as np could probably be written as This will spare python from browsing your entire sys.path to find the desired packages. See also: Manually importing gtk fails: module not found", "id": 36120472, "owner_tier": 0.5, "score": 0.09574468074468084}, {"content": "1.35 seconds isn't long, but I suppose if you're used to half that for a \"quick check\" then perhaps it seems so. Andrea suggests a simple client/server setup, but it seems to me that you could just as easily call a very slight modification of your script and keep it's console window open while you work: I assume your script is identical every time, ie you don't need to give it image stack location or any particular commands each time (but these are easy to do as well!).  Example RAAC's_Script.py: To end the script, close the console window or press ctrl+c. I've made this as simple as possible, but it would require very little extra to handle things like quitting nicely, doing slightly different things based on input, etc.", "id": 25547013, "owner_tier": 0.3, "score": 0.031914893510638295}, {"content": "you could build a simple server/client, the server running continuously making and updating the plot, and the client just communicating the next file to process. I wrote a simple server/client example based on the basic example from the socket module docs: http://docs.python.org/2/library/socket.html#example here is server.py: and client.py: you just run the server: which does the imports, then the client just sends via the socket the filename of the new file to plot: then the server updates the plot. On my machine running your imports take 0.6 seconds, while running client.py 0.03 seconds.", "id": 16430894, "owner_tier": 0.5, "score": 0.29787234031914894}], "link": "https://stackoverflow.com/questions/16373510/improving-speed-of-python-module-import", "question": {"content": "The question of how to speed up importing of Python modules has been asked previously (Speeding up the python \"import\" loader and Python -- Speed Up Imports?) but without specific examples and has not yielded accepted solutions. I will therefore take up the issue again here, but this time with a specific example.  I have a Python script that loads a 3-D image stack from disk, smooths it, and displays it as a movie. I call this script from the system command prompt when I want to quickly view my data. I'm OK with the 700 ms it takes to smooth the data as this is comparable to MATLAB. However, it takes an additional 650 ms to import the modules. So from the user's perspective the Python code runs at half the speed. This is the series of modules I'm importing: Of course, not all modules are equally slow to import. The chief culprits are: I have experimented with using from, but this isn't any faster. Since Matplotlib is the main culprit and it's got a reputation for slow screen updates, I looked for alternatives. One is PyQtGraph, but that takes 550 ms to import.   I am aware of one obvious solution, which is to call my function from an interactive Python session rather than the system command prompt. This is fine but it's too MATLAB-like, I'd prefer the elegance of having my function available from the system prompt.  I'm new to Python and I'm not sure how to proceed at this point. Since I'm new, I'd appreciate links on how to implement proposed solutions. Ideally, I'm looking for a simple solution (aren't we all!) because the code needs to be portable between multiple Mac and Linux machines. ", "id": 16373510, "title": "improving speed of Python module import", "traffic_rate": 15}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "performance", "import", "module"]}, {"answers": [{"content": "You can do a lot better by changing the structure of the core to use a matrix of \"alive\" cells represented by a numpy array of 0/1.  If you do this, you can chop out the double loop you have and let numpy do the heavy lifting of neighbor counting.  You could do some work to index that and get summations, but the 2nd enhancement is to use the process of \"convolution with a kernel\" which is very common in image processing.  Basically you are taking a kernel (in this case a 3x3 kernel to hit the neighbors) and multiplying that by the appropriate grid in your matrix for every member.  scipy has an image processing package that makes this a snap.  Note that you can set up the kernel to ignore out-of-bounds by filling it with zero (zero padding is the term.) I modified your neighbor counting function a bit to separate it from the cell class for comparison, but didn't change any of the guts. In a 500 x 500 structure, your approach takes about 1.3 seconds to calculate the number of living neighbors.  So that looping costs roughly 1 second per frame to do.  Using convolution, I can get it done in 0.004 seconds, around a 1000x speed-up. Note that when I compared results I got a failed comparison because I don't think your modulo arithmetic is accurate for the edges (it is wrapping--not desired.)  You can see it with my code if you print the first neighbor matrix. After you \"neighbor count\" you can use a couple numpy functions to figure out what the alive matrix looks like for the next evolution and just incorporate that in your code.", "id": 76444193, "owner_tier": 0.5, "score": 0.9999999966666667}, {"content": "The simplest fix you can set for yourself is to pre-calculate your cell neighbors rather than running the calculation within an O(n^2) complexity. The modulo operation can be one of the slower operations to perform, so minimizing the number of times you run it will be good. Ultimately you want to minimize or eliminate regular O(n^2) calculations to minimize CPU strain. Given that you\u2019re not doing anything super complicated here, I think this is a simple and quick performance gain you can realize. Remember that Python passes values by reference, so just initialize your neighbors in __init__ rather than passing them to calculateNewState. If you\u2019re running into performance issues, a good helper is dis.dis to see the steps in assembly code. Find your bottlenecks as each step is something your CPU needs to manage. Then, if you can remove it, your code will have fewer steps and run faster. Sometimes without seeing the low level code, it can be hard to understand where your bottlenecks really are.", "id": 76440738, "owner_tier": 0.5, "score": 0.6666666633333334}, {"content": "You could try importing and using threading so that each task that doesn't necessarily need to be completed in order can be done simultaneously and speed things up.\nIt could look something like this", "id": 76440497, "owner_tier": 0.1, "score": -3.333333350082531e-09}], "link": "https://stackoverflow.com/questions/76440408/my-code-run-very-very-slow-how-can-i-make-my-python-code-that-runs-faster", "question": {"content": "please help me. I wrote a program to run \"Game of Life\" with PyQt6 but, it runs very very slow. How can I make it faster? main.py cell.py I want to write code that runs as fast as possible", "id": 76440408, "title": "My code run very very slow. How can I make my python code that runs faster?", "traffic_rate": 290}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "gpu", "cpu", "pyqt6", "conways-game-of-life"]}, {"answers": [{"content": "I think your biggest issue is the lines: You're filtering the entire dataframe while I think you're only actually interested in the KPI and ad_id columns. You could instead create a rolling mask, something like You can then access your subsets something like df[mask][KPI].mean() and df[mask]['ad_id'].values. If you do this, you will avoid copying a huge amount of data on every iteration. I would also be tempted to simplify the code a little, for example I believe vals_lst = list(map(list, itertools.product([True, False], repeat=n))) is the same for each group, so I would probably calculate it once and hold it as a stand alone variable rather than add it to every group; this would clean up the group[0], group[1] and group[0][i] references which were a little hard to track on first reading the code. Looking at the change from iterative filtering to tracking a mask, the mask approach always to perform better, but the gap increases with data size. With 10000 rows the gaps are: with the following test code:", "id": 74158580, "owner_tier": 0.3, "score": 0.9999999900000002}], "link": "https://stackoverflow.com/questions/74158253/what-is-making-this-python-code-so-slow-how-can-i-modify-it-to-run-faster", "question": {"content": "I am writing a program in Python for a data analytics project involving advertisement performance data matched to advertisement characteristics aimed at identifying high performing groups of ads that share n similar characteristics. The dataset I am using has individual ads as rows, and characteristic, summary, and performance data as columns. Below is my current code - the actual dataset I am using has 51 columns, 4 are excluded, so it is running with 47 C 4, or 178365 iterations in the outer loop. Currently, this code takes ~2 hours to execute. I know that nested for loops can be the source of such a problem, but I do not know why it is taking so long to run, and am not sure how I can modify the inner/outer for loops to improve performance. Any feedback on either of these topics would be greatly appreciated. Any insight into why the code is taking such a long time to run, and/or any advice on improving the performance of this code would be extremely helpful.", "id": 74158253, "title": "What is making this Python code so slow? How can I modify it to run faster?", "traffic_rate": 181}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "pandas", "performance", "loops", "data-analysis"]}, {"answers": [{"content": "Mathematics and feasibility aside, here's an O(n^2) approach to optimize your O(n^3) solution.", "id": 71964875, "owner_tier": 0.1, "score": 0.0}], "link": "https://stackoverflow.com/questions/71961571/my-code-run-very-slow-how-can-i-make-it-faster", "question": {"content": "I wrote this code to see which numbers have the condition a**4 + b**4 + c**4 = d**4 But the code I typed is very slow. Do you have any suggestions to make it faster? (I know this code is not suitable for Python language but asking me to optimize this code in the language I know.)", "id": 71961571, "title": "My code run very slow, how can I make it faster", "traffic_rate": 643}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "python-3.x"]}, {"answers": [{"content": "The first thing to do when a program is slow is to identify bottlenecks; in fact, you want to optimize things that take a long time, not things that may actually be fast.  In Python, the most efficient way to do this is with one of the Python profilers, which are dedicated tools for performance analysis.  Here is a quickstart: runs your program and stores profiling information in prof.dat.  Then, runs the profiling information analysis tool pstats.  Important pstat commands include: which sorts functions by the time spent in them, and which you can use with a different key instead of time (cumulative,\u2026). Another important command is which print statistics (or stats 10 to print the first 10 most time-consuming functions).  You can obtain help with ?, or help <command>. The way to optimize your program then consists in dealing with the particular code that causes the bottlenecks.  You can post the timing results and maybe get some more specific help on the sections of the program that could be most usefully optimized.", "id": 8624798, "owner_tier": 0.9, "score": 0.9999999996666666}, {"content": "A good place to start would be in identifying places where the code has to perform many iterations (nested for loops, for example) or process large amounts of data. Place print statements before and after these and you'll be able to determine if that's what's taking ages. Then you can look at why. Or, if it's a small program, just post the entire thing.", "id": 8624793, "owner_tier": 0.9, "score": -3.333333313075097e-10}], "link": "https://stackoverflow.com/questions/8624779/my-python-program-is-very-slow-how-can-i-speed-it-up-am-i-doing-something-wron", "question": {"content": "EDIT: I ran the python profiler and the two most time-consuming things (this is after I decided to comment out the webbrowser portion and Firefox portion of the code, because I knew they were going to be the slowest part...) , the slowest part of my program is re.findall and re.compile and also (len) and (append to list). I don't know if I should post all of my code on here at once because I worked really hard on my program (even if it isn't too good), so for now I'm just going to ask...How do I make my Python program faster? I have 3 suspects right now for it being so slow: Maybe my computer is just slow Maybe my internet is too slow (sometimes my program has to download the html of web pages and then it searches through the html for a specific piece of text) My code is slow (too many loops maybe? something else? I'm new to this so I wouldn't know!) If anyone could offer me advice, I would greatly appreciate it! Thanks! EDIT: My code uses lots of loops I think...also, another thing is that for the program to work you have to be logged in to this website: http://www.locationary.com/", "id": 8624779, "title": "My Python program is very slow! How can I speed it up? Am I doing something wrong?", "traffic_rate": 7}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "performance"]}, {"answers": [{"content": "The comments and Moj's answer give a lot of good advice. I have some experience on signal/image processing with python, and have banged my head against the performance wall repeatedly, and I just want to share a few thoughts about making things faster in general. Maybe these help figuring out possible solutions with slow algorithms. Where is the time spent? Let us assume that you have a great algorithm which is just too slow. The first step is to profile it to see where the time is spent. Sometimes the time is spent doing trivial things in a stupid way. It may be in your own code, or it may even be in the library code. For example, if you want to run a 2D Gaussian filter with a largish kernel, direct convolution is very slow, and even FFT may be slow. Approximating the filter with computationally cheap successive sliding averages may speed things up by a factor of 10 or 100 in some cases and give results which are close enough. If a lot of time is spent in some module/library code, you should check if the algorithm is just a slow algorithm, or if there is something slow with the library. Python is a great programming language, but for pure number crunching operations it is not good, which means most great libraries have some binary libraries doing the heavy lifting. On the other hand, if you can find suitable libraries, the penalty for using python in signal/image processing is often negligible. Thus, rewriting the whole program in C does not usually help much. Writing a good algorithm even in C is not always trivial, and sometimes the performance may vary a lot depending on things like CPU cache. If the data is in the CPU cache, it can be fetched very fast, if it is not, then the algorithm is much slower. This may introduce non-linear steps into the processing time depending on the data size. (Most people know this from the virtual memory swapping, where it is more visible.) Due to this it may be faster to solve 100 problems with 100 000 points than 1 problem with 10 000 000 points. One thing to check is the precision used in the calculation. In some cases float32 is as good as float64 but much faster. In many cases there is no difference. Multi-threading Python - did I mention? - is a great programming language, but one of its shortcomings is that in its basic form it runs a single thread. So, no matter how many cores you have in your system, the wall clock time is always the same. The result is that one of the cores is at 100 %, and the others spend their time idling. Making things parallel and having multiple threads may improve your performance by a factor of, e.g., 3 in a 4-core machine. It is usually a very good idea if you can split your problem into small independent parts. It helps with many performance bottlenecks. And do not expect technology to come to rescue. If the code is not written to be parallel, it is very difficult for a machine to make it parallel. GPUs Your machine may have a great GPU with maybe 1536 number-hungry cores ready to crunch everything you toss at them. The bad news is that making GPU code is a bit different from writing CPU code. There are some slightly generic APIs around (CUDA, OpenCL), but if you are not accustomed to writing parallel code for GPUs, prepare for a steepish learning curve. On the other hand, it is likely someone has already written the library you need, and then you only need to hook to that. With GPUs the sheer number-crunching power is impressive, almost frightening. We may talk about 3 TFLOPS (3 x 10^12 single-precision floating-point ops per second). The problem there is how to get the data to the GPU cores, because the memory bandwidth will become the limiting factor. This means that even though using GPUs is a good idea in many cases, there are a lot of cases where there is no gain. Typically, if you are performing a lot of local operations on the image, the operations are easy to make parallel, and they fit well a GPU. If you are doing global operations, the situation is a bit more complicated. A FFT requires information from all over the image, and thus the standard algorithm does not work well with GPUs. (There are GPU-based algorithms for FFTs, and they sometimes make things much faster.) Also, beware that making your algorithms run on a GPU bind you to that GPU. The portability of your code across OSes or machines suffers. Buy some performance Also, one important thing to consider is if you need to run your algorithm once, once in a while, or in real time. Sometimes the solution is as easy as buying time from a larger computer. For a dollar or two an hour you can buy time from quite fast machines with a lot of resources. It is simpler and often cheaper than you would think. Also GPU capacity can be bought easily for a similar price. One possibly slightly under-advertised property of some cloud services is that in some cases the IO speed of the virtual machines is extremely good compared to physical machines. The difference comes from the fact that there are no spinning platters with the average penalty of half-revolution per data seek. This may be important with data-intensive applications, especially if you work with a large number of files and access them in a non-linear way.", "id": 24317131, "owner_tier": 0.7, "score": 0.9999999966666667}, {"content": "I am afraid you can not speed up your program by just running it on a powerful computer. I had this issue while back. I first used python (very slow), then moved to C(slow) and then had to use other tricks and techniques. for example it is sometimes possible to apply some dimensionality reduction to speed up things while having reasonable accurate result, or as you mentioned using multi processing techniques. Since you are dealing with image processing problem, you do a lot of matrix operations and GPU for sure would be a great help. there are some nice and active cuda wrappers in python that you can easily use, by not knowing too much CUDA. I tried Theano, pycuda and scikit-cuda (there should be more than that since then). ", "id": 24306811, "owner_tier": 0.5, "score": -3.3333333130750966e-09}], "link": "https://stackoverflow.com/questions/24306285/how-to-speed-up-python-code-for-running-on-a-powerful-machine", "question": {"content": "I've completed writing a multiclass classification algorithm that uses boosted classifiers. One of the main calculations consists of weighted least squares regression. \nThe main libraries I've used include: I've developed the algorithm in Python, using Anaconda's Spyder.  I now need to use the algorithm to start training classification models. So I'll be passing approximately 7000-10000 images to this algorithm, each about 50x100, all in gray scale.  Now I've been told that a powerful machine is available in order to speed up the training process. And they asked me \"am I using GPU?\" And a few other questions. To be honest I have no experience in CUDA/GPU, etc. I've only ever heard of them. I didn't develop my code with any such thing in mind. In fact I had the (ignorant) impression that a good machine will automatically run my code faster than a mediocre one, without my having to do anything about it. (Apart from obviously writing regular code efficiently in terms of loops, O(n), etc).  Is it still possible for my code to get speeded up simply by virtue of being on a high performance computer? Or do I need to modify it to make use of a parallel-processing machine?", "id": 24306285, "title": "How to speed up Python code for running on a powerful machine?", "traffic_rate": 7600}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "performance", "numpy", "cuda", "gpu"]}, {"answers": [{"content": "Program efficiency typically falls under the 80/20 rule (or what some people call the 90/10 rule, or even the 95/5 rule). That is, 80% of the time the program is actually running in 20% of the code. In other words, there is a good shot that your code has a \"bottleneck\": a small area of the code that is running slow, while the rest runs very fast. Your goal is to identify that bottleneck (or bottlenecks), then fix it (them) to run faster. The best way to do this is to profile your code. This means you are logging the time of when a specific action occurs with the logging module, use timeit like a commenter suggested, use some of the built-in profilers, or simply print out the current time at very points of the program. Eventually, you will find one part of the code that seems to be taking the most amount of time. Experience will tell you that I/O (stuff like reading from a disk, or accessing resources over the internet) will take longer than in-memory calculations. My guess as to the problem is that you're using 1 HTTP connection to get a list of streamers, and then one HTTP connection to get the status of that streamer. Let's say that there are 10000 streamers: your program will need to make 10001 HTTP connections before it finishes. There would be a few ways to fix this if this is indeed the case:", "id": 15054209, "owner_tier": 0.9, "score": 0.9999999979999998}, {"content": "You are using the wrong tool here to parse the json data returned by your URL. You need to use json library provided by default rather than parsing the data using regex.\nThis will give you a boost in your program's performance Change the regex parser  To json parser", "id": 15054092, "owner_tier": 0.9, "score": -1.999999987845058e-09}], "link": "https://stackoverflow.com/questions/15054008/my-python-program-is-running-really-slow", "question": {"content": "I'm making a program that (at least right now) retrives stream information from TwitchTV (streaming platform). This program is to self educate myself but when i run it, it's taking 2 minutes to print just the name of the streamer. I'm using Python 2.7.3 64bit on Windows7 if that is important in anyway. classes.py: main.py: the program itself works perfectly, just really slow any ideas?", "id": 15054008, "title": "My python program is running really slow", "traffic_rate": 2037}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "performance", "oop", "class"]}, {"answers": [{"content": "I'm surprised no one mentioned ShedSkin: http://code.google.com/p/shedskin/, it automagically converts your python program to C++ and in some benchmarks yields better improvements than psyco in speed. Plus anecdotal stories on the simplicity: http://pyinsci.blogspot.com/2006/12/trying-out-latest-release-of-shedskin.html There are limitations though, please see this", "id": 172794, "owner_tier": 0.5, "score": 0.15217391282608697}, {"content": "There is also Python \u2192 11l \u2192 C++ transpiler, which can be downloaded from here.", "id": 67342288, "owner_tier": 0.3, "score": -2.173913043478261e-10}, {"content": "For an established project I feel the main performance gain will be from making use of python internal lib as much as possible. Some tips are here: http://blog.hackerearth.com/faster-python-code", "id": 41671464, "owner_tier": 0.5, "score": -2.173913043478261e-10}, {"content": "It's often possible to achieve near-C speeds (close enough for any project using Python in the first place!) by replacing explicit algorithms written out longhand in Python with an implicit algorithm using a built-in Python call.  This works because most Python built-ins are written in C anyway. Well, in CPython of course ;-) https://www.python.org/doc/essays/list2str/ ", "id": 247612, "owner_tier": 0.5, "score": 0.08695652152173913}, {"content": "A couple of ways to speed up Python code were introduced after this question was asked:", "id": 18849701, "owner_tier": 0.7, "score": -2.173913043478261e-10}, {"content": "First thing that comes to mind: psyco. It runs only on x86, for the time being. Then, constant binding. That is, make all global references (and global.attr, global.attr.attr\u2026) be local names inside of functions and methods. This isn't always successful, but in general it works. It can be done by hand, but obviously is tedious. You said apart from in-code optimization, so I won't delve into this, but keep your mind open for typical mistakes (for i in range(10000000) comes to mind) that people do.", "id": 172726, "owner_tier": 0.9, "score": 0.36956521717391305}, {"content": "Besides the (great) psyco and the (nice) shedskin, I'd recommend trying cython a great fork of pyrex.  Or, if you are not in a hurry, I recommend to just wait. Newer python virtual machines are coming, and unladen-swallow will find its way into the mainstream.", "id": 863670, "owner_tier": 0.5, "score": 0.021739130217391302}, {"content": "I hope you've read: http://wiki.python.org/moin/PythonSpeed/PerformanceTips Resuming what's already there are usualy 3 principles:", "id": 364373, "owner_tier": 0.3, "score": 0.10869565195652174}, {"content": "The canonical reference to how to improve Python code is here:  PerformanceTips.  I'd recommend against optimizing in C unless you really need to though.  For most applications, you can get the performance you need by following the rules posted in that link.", "id": 247641, "owner_tier": 0.9, "score": 0.04347826065217392}, {"content": "If using psyco, I'd recommend psyco.profile() instead of psyco.full(). For a larger project it will be smarter about the functions that got optimized and use a ton less memory. I would also recommend looking at iterators and generators. If your application is using large data sets this will save you many copies of containers.", "id": 175283, "owner_tier": 0.5, "score": 0.021739130217391302}, {"content": "Rather than just punting to C, I'd suggest: Make your code count. Do more with fewer executions of lines: If all of the above fails for profiled and measured code, then begin thinking about the C-rewrite path.", "id": 173055, "owner_tier": 0.5, "score": 0.6304347823913043}, {"content": "This is the procedure that I try to follow:", "id": 172991, "owner_tier": 0.9, "score": 0.06521739108695652}, {"content": "Regarding \"Secondly: When writing a program from scratch in python, what are some good ways to greatly improve performance?\" Remember the Jackson rules of optimization:  And the Knuth rule: The more useful rules are in the General Rules for Optimization. Don't optimize as you go.  First get it right.  Then get it fast.  Optimizing a wrong program is still wrong. Remember the 80/20 rule. Always run \"before\" and \"after\" benchmarks.  Otherwise, you won't know if you've found the 80%. Use the right algorithms and data structures.  This rule should be first.  Nothing matters as much as algorithm and data structure. Bottom Line You can't prevent or avoid the \"optimize this program\" effort.  It's part of the job.  You have to plan for it and do it carefully, just like the design, code and test activities.", "id": 172784, "owner_tier": 0.9, "score": 0.9999999997826087}, {"content": "Just a note on using psyco: In some cases it can actually produce slower run-times. Especially when trying to use psyco with code that was written in C. I can't remember the the article I read this, but the  map() and reduce() functions were mentioned specifically. Luckily you can tell psyco not to handle specified functions and/or modules.", "id": 172782, "owner_tier": 0.5, "score": 0.06521739108695652}, {"content": "This won't necessarily speed up any of your code, but is critical knowledge when programming in Python if you want to avoid slowing your code down.  The \"Global Interpreter Lock\" (GIL), has the potential to drastically reduce the speed of your multi-threaded program if its behavior is not understood (yes, this bit me ... I had a nice 4 processor machine that wouldn't use more than 1.2 processors at a time).  There's an introductory article with some links to get you started at SmoothSpan.", "id": 172734, "owner_tier": 0.5, "score": 0.08695652152173913}, {"content": "People have given some good advice, but you have to be aware that when high performance is needed, the python model is: punt to c.  Efforts like psyco may in the future help a bit, but python just isn't a fast language, and it isn't designed to be.  Very few languages have the ability to do the dynamic stuff really well and still generate very fast code; at least for the forseeable future (and some of the design works against fast compilation) that will be the case. So, if you really find yourself in this bind, your best bet will be to isolate the parts of your system that are unacceptable slow in (good) python, and design around the idea that you'll rewrite those bits in C.  Sorry.  Good design can help make this less painful.  Prototype it in python first though, then you've easily got a sanity check on your c, as well. This works well enough for things like numpy, after all.  I can't emphasize enough how much good design will help you though.  If you just iteratively poke at your python bits and replace the slowest ones with C, you may end up with a big mess.  Think about exactly where the C bits are needed, and how they can be minimized and encapsulated sensibly.", "id": 172766, "owner_tier": 0.5, "score": 0.08695652152173913}, {"content": "The usual suspects -- profile it, find the most expensive line, figure out what it's doing, fix it. If you haven't done much profiling before, there could be some big fat quadratic loops or string duplication hiding behind otherwise innocuous-looking expressions. In Python, two of the most common causes I've found for non-obvious slowdown are string concatenation and generators. Since Python's strings are immutable, doing something like this: will copy the entire string twice per iteration. This has been well-covered, and the solution is to use \"\".join: Generators are another culprit. They're very easy to use and can simplify some tasks enormously, but a poorly-applied generator will be much slower than simply appending items to a list and returning the list. Finally, don't be afraid to rewrite bits in C! Python, as a dynamic high-level language, is simply not capable of matching C's speed. If there's one function that you can't optimize any more in Python, consider extracting it to an extension module. My favorite technique for this is to maintain both Python and C versions of a module. The Python version is written to be as clear and obvious as possible -- any bugs should be easy to diagnose and fix. Write your tests against this module. Then write the C version, and test it. Its behavior should in all cases equal that of the Python implementation -- if they differ, it should be very easy to figure out which is wrong and correct the problem.", "id": 172744, "owner_tier": 0.9, "score": 0.5434782606521739}, {"content": "Cython and pyrex can be used to generate c code using a python-like syntax. Psyco is also fantastic for appropriate projects (sometimes you'll not notice much speed boost, sometimes it'll be as much as 50x as fast).\nI still reckon the best way is to profile your code (cProfile, etc.) and then just code the bottlenecks as c functions for python.", "id": 172740, "owner_tier": 0.3, "score": 0.19565217369565216}, {"content": "Run your app through the Python profiler.\nFind a serious bottleneck.\nRewrite that bottleneck in C.\nRepeat.", "id": 172737, "owner_tier": 0.5, "score": 0.08695652152173913}], "link": "https://stackoverflow.com/questions/172720/speeding-up-python", "question": {"content": "This is really two questions, but they are so similar, and to keep it simple, I figured I'd just roll them together: Firstly: Given an established python project, what are some decent ways to speed it up beyond just plain in-code optimization? Secondly:  When writing a program from scratch in python, what are some good ways to greatly improve performance? For the first question, imagine you are handed a decently written project and you need to improve performance, but you can't seem to get much of a gain through refactoring/optimization.  What would you do to speed it up in this case short of rewriting it in something like C?", "id": 172720, "title": "Speeding Up Python", "traffic_rate": 5}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "optimization", "performance"]}, {"answers": [{"content": "Please consider trimming down your f_wo_append function: Edit in response to OP's comment \"\"\"This made it a lot worse! The trimmed version takes 4 times more time than the version I have posted above. \"\"\" There is no way that that could take \"4 times more\" (5 times?) ... here is my code, which demonstrates a significant reduction in the \"without append\" case, as I suggested, and also introduces a significant improvement in the \"with append\" case. and here are the results of running it (Python 2.7.1, Windows 7 Pro, \"Intel Core i3 CPU 540 @ 3.07 GHz\"): Edit 3 Why numpy takes longer: and here's why my f_wo_append2 function took 4 times longer: The empirical evidence is that these custom types aren't so fast when used as scalars ... perhaps because they need to reset the floating-point hardware each time they are used. OK for big arrays, not for scalars. Are you using any other numpy functionality? If not, just use the random module. If you have other uses for numpy, you may wish to coerce the numpy.float64 to float during the population setup.", "id": 4679413, "owner_tier": 0.9, "score": 0.799999998}, {"content": "There are many things you can try after optimizing your Python code for speed. If this program doesn't need C extensions, you can run it under PyPy to benefit from its JIT compiler. You can try making a C extension for possibly huge speedups. Shed Skin will even allow you to convert your Python program to a standalone C++ binary. I'm willing to time your program under these different optimization scenarios if you can provide enough code for benchmarking, Edit: First of all, I have to agree with everyone else: are you sure you're measuring the time correctly? The example code runs 100 times in under 0.1 seconds here, so there is a good chance the either the time is wrong or you have a bottleneck (IO?) that isn't present in the code sample. That said, I made it 300000 people so times were consistent. Here's the adapted code, shared by CPython (2.5), PyPy and Shed Skin: Running with PyPy is as simple as running with CPython, you just type 'pypy' instead of 'python'. For Shed Skin, you must convert to C++, compile and run: And here is the Cython-ized code: For building it, it's nice to have a setup.py like this one: You build it with:\n    python setupfaster.py build_ext --inplace Then test:\n    python -c \"import cymakefaster; print cymakefaster.file; cymakefaster.main()\"   Timings were run five times for each version, with Cython being the fastest and easiest of the code generators to use (Shed Skin aims to be simpler, but cryptic error messages and implicit static typing made it harder here). As for best value, PyPy gives impressive speedup in the counter version with no code changes.  Edit: Aaaand more meaningful timings, for 80000 calls with 300 people each: Shed Skin becomes fastest, PyPy surpasses Cython. All three speed things up a lot compared to CPython.", "id": 4654569, "owner_tier": 0.5, "score": 0.9999999980000001}, {"content": "Depending on how often you add new elements to self.people or change person.utility, you could consider sorting self.people by the utility field. Then you could use a bisect function to find the lower index i_pivot where the person[i_pivot].utility >= price condition is met. This would have a lower complexity ( O(log N) ) than your exhaustive loop ( O(N) ) With this information, you could then update your people list if needed : Do you really need to update the utility field each time ? In the sorted case, you could easily deduce this value while iterating : for example, considering your list sorted in incresing order, utility = (index >= i_pivot) Same question with customers and nonCustomers lists. Why do you need them? They could be replaced by slices of the original sorted list : for example, customers = self.people[0:i_pivot] All this would allow you to reduce the complexity of your algorithm, and use more built-in (fast) Python functions, this could speedup your implementation.", "id": 4655176, "owner_tier": 0.5, "score": 0.199999998}, {"content": "This comment rings alarm bells: Aside from the fact that timePd is not used in the function, if you really want just to return the quantity, do just that in the function. Do the \"in addition\" stuff in a separate function.  Then profile again and see which of these two functions you are spending most of your time in. I like to apply SRP to methods as well as classes: it makes them easier to test.", "id": 4657252, "owner_tier": 0.9, "score": 0.199999998}, {"content": "It's surprising that the function shown is such a bottleneck because it's so relatively simple. For that reason, I'd double check my profiling procedure and results. However, if they're correct, the most time consuming part of your function has to be the for loop it contains, of course, so it makes sense to focus on speeding that up. One way to do this is by replacing the if/else with straight-line code. You can also reduce the attribute lookup for the append list method slightly. Here's how both of those things could be accomplished: That said, I must add that it seems a little redundant to have both a customer flag in each person object and also put each of them into a separate list depending on that attribute. Not creating these two lists would of course speed the loop up further.", "id": 4656399, "owner_tier": 0.9, "score": -1.999999987845058e-09}, {"content": "You can eliminate some lookups by using local function aliases:", "id": 4654074, "owner_tier": 0.9, "score": 0.199999998}, {"content": "Some curious things I noted: timePd is passed as a parameter but never used price is an array but you only ever use the last entry - why not pass the value there instead of passing the list? count is initialized and never used self.people contains multiple person objects which are then copied to either self.customers or self.noncustomers as well as having their customer flag set.  Why not skip the copy operation and, on return, just iterate over the list, looking at the customer flag? This would save the expensive append. Alternatively, try using psyco which can speed up pure Python, sometimes considerably.", "id": 4653805, "owner_tier": 0.5, "score": -1.999999987845058e-09}], "link": "https://stackoverflow.com/questions/4653715/increasing-speed-of-python-code", "question": {"content": "I have some python code that has many classes. I used cProfile to find that the total time to run the program is 68 seconds. I found that the following function in a class called Buyers takes about 60 seconds of those 68 seconds. I have to run the program about 100 times, so any increase in speed will help. Can you suggest ways to increase the speed by modifying the code? If you need more information that will help, please let me know. self.people is a list of  person objects. Each person has customer and utility as its attributes.  EDIT - responsed added -------------------------------------  Thanks so much for the suggestions. Here is the\nresponse to some questions and suggestions people have kindly\nmade. I have not tried them all, but will try others and write back later.  (1) @amber - the function is accessed 80,000 times.  (2) @gnibbler and others - self.people is a list of Person objects in memory. Not connected to a database. (3) @Hugh Bothwell  cumtime taken by the original function - 60.8 s (accessed 80000 times) cumtime taken by the new function with local function aliases as suggested - 56.4 s (accessed  80000 times) (4) @rotoglup and @Martin Thomas  I have not tried your solutions yet. I need to check the rest of the code to see the places where I use self.customers before I can make the change of not appending the customers to self.customers list. But I will try this and write back. (5) @TryPyPy - thanks for your kind offer to check the code.  Let me first read a little on the suggestions you have made to see if those will be feasible to use.  EDIT 2\nSome suggested that since I am flagging the customers and noncustomers in the self.people, I should try without creating separate lists of self.customers and self.noncustomers using append. Instead, I should loop over the self.people to find the number of customers. I tried the following code and timed both functions below f_w_append and f_wo_append. I did find that the latter takes less time, but it is still 96% of the time taken by the former. That is, it is a very small increase in the speed.  @TryPyPy - The following piece of code is complete enough to check the bottleneck function, in case your offer is still there to check it with other compilers.  Thanks again to everyone who replied. EDIT 3: It seems numpy is the problem This is in response to what John Machin said below. Below you see two ways of defining Population class. I ran the program below twice, once with each way of creating Population class. One uses numpy and one does not use numpy. The one without numpy takes similar time as John found in his runs. One with numpy takes much longer. What is not clear to me is that the popn instance is created before time recording begins (at least that is what it appears from the code). Then, why is numpy version taking longer. And, I thought numpy was supposed to be more efficient. Anyhow, the problem seems to be with numpy and not so much with the append, even though it does slow down things a little. Can someone please confirm with the code below? Thanks. Edit 4: See the answers by John Machin and TryPyPy Since there have been so many edits and updates here, those who find themselves here for the first time may be a little confused. See the answers by John Machin and TryPyPy. Both of these can help in improving the speed of the code substantially. I am grateful to them and others who alerted me to slowness of append. Since, in this instance I am going to use John Machin's solution and not use numpy for generating utilities, I am accepting his response as an answer. However, I really appreciate the directions pointed out by TryPyPy also.  ", "id": 4653715, "title": "Increasing speed of python code", "traffic_rate": 1882}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "stackoverflow", "tags": ["python", "performance"]}, {"answers": [{"content": "Fire up multiple processes with the multiprocess lib, or fire up multiple copies of the script from different starting points.\n\nDo I have to setup it or importing it is enough ?\n\nYou have to set it up.  Plenty of examples and docs online.", "id": "j342q0x", "owner_tier": 0.3, "score": 0.9999999966666667}], "link": "https://www.reddit.com/r/learnpython/comments/104a5f4/how_can_i_increase_this_python_programs_speed/", "question": {"content": "Hey guys sorry for disturbing, I would like to ask a quick question so I can have a basis to work on.  \n\nActually I am using this python program to scrap tweets for a personal purpose : [https://github.com/achyuthjoism/tweeds](https://github.com/achyuthjoism/tweeds) \n\nThing is I don't get how I could increase the scraping speed of this program as it is really slow. Indeed it only collects  5 tweets per second when another program I was using scraps 4x faster but unfortunately it's not efficient as I am willing it to be. Tweeds is cleaner and collects more datas, but is really slow compared to the other one  Would you mind telling me ? \n\nThanks in advance ! I would like to have an idea where I can work to improve the speed. Have a nice day.", "id": "104a5f4", "title": "How can I increase this python program's speed ?", "traffic_rate": 153.12462962962962}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [], "link": "https://www.reddit.com/r/learnpython/comments/1025bvx/how_can_i_speed_up_my_python_executable_code/", "question": {"content": "I just made my python code executable and It's runs slow, Even if I use \"pyintaller -w main.py\" \n\nHow do I increase the speed of it ??", "id": "1025bvx", "title": "How can I speed up my python executable code", "traffic_rate": 153.12462962962962}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "What\u2019s your memory usage like?  If your virtual memory use is above real memory, you could be swapping.", "id": "fhr51n4", "owner_tier": 0.1, "score": -1.999999987845058e-09}, {"content": "You need to monitor the process' CPU and memory use. If you make use of a database then you need to monitor that too. Run Activity Monitor (comes with Mac) and sort by CPU descending, or sort by process name and look for the Python process.\n\nPython has a tendency to only use a single core, so I suspect that you're really running out of memory. Once you find the process (likely the one at 100% CPU, or close to that) and select it, click the Information button at the top of Activity Monitor. The opening tab is Memory and that's where you can look at the virtual memory in use by the process.\n\nThanks! Thats exactly whats happening.\n\nIs there a method in which I can stop the computer from running out of memory?\n\nDon\u2019t waste memory.  Are you saving all the results at every time step?  You should be throwing most of it away if you have a memory intensive process.\n\nThere are two possibilities (or a combination):\n\n1. Your program is doing too much at once (allocating more memory than available).\n2. There is a bug in your program that allocates more memory than necessary.\n\nI good first step is to try and figure out the quantities you are working with, e.g. return the amount of simulated particles or whatever it is you're working with.\n\nAnother approach is to use a database to hold the data. A database has a certain amount of memory to work with and never goes over this limit. For small amounts of data this can be slower than keeping everything in memory, but as the amount of data gets larger and larger this becomes a better solution. You can use PostgreSQL as it is free and fairly powerful.\n\nFinally, it could be that you are just trying to do too much with too few resources. In this case you might try renting a more powerful machine from a cloud provider to run your simulation on.\n\nCheers! Ill look into using a database. So if I undertand you right, if I use a database it will spit out an error or something instead of making the program unresponsive?\n\nYou need to learn more about databases, they're not just an add-on, but a new layer to setup and manage. This could be out of scope for your project, because of the additional time to learn about DBs though. You'd also need to learn how to use SQL which is the language most relational databases use.\n\nAn alternative is to setup some sort of saving to disk using files to free up memory. But then you need to understand which parts of your are taking up large amounts of memory that you don't need to access right away, and so can be persisted and freed up.\n\nDid you find out how much virtual memory your process is using?", "id": "fht93zh", "owner_tier": 0.3, "score": 0.9999999980000001}], "link": "https://www.reddit.com/r/Python/comments/f4jd7m/why_does_python_severely_slow_my_computer_after/", "question": {"content": "I've written a gravitational simulation for my master degree. So a lot of the time when using it I need to run the code for long periods of time.\n\nIn the first 10 minutes of a long run everything is fine, I can use other applications on my computer without any fault. After around 10 minutes my computer becomes incredibly slow. To the point where its difficult to stop or even close the terminal thats running the python code. Once force quit the code my computer begins to speed up again.\n\nWhy is this and can I do anything to help it?\n\nI'm using python3 and the computer is a 2017 mac btw. The code is slow in both IDEs and terminal as well", "id": "f4jd7m", "title": "Why does Python severely slow my computer after long run times?", "traffic_rate": 207.9365132125644}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "There are programs like Nuitka that compile Python into an executable, but it doesn't usually run much faster. This is mainly because of the dynamic nature of Python.\n\nYou can try Pyston and Pypi, which do JIT (Just-in-time) compilation, and some types of optimisation, which *can* increase the speed of your code in some cases.\n\nAnother option is Cython which uses type hints to produce faster (i.e. more specialised) code.\n\nYou may find that you just need to re-write your code in C or another low-level language in the end anyway, but in many cases, however, it is the *algorithm* that determines the efficiency of processing - and re-coding a slow algorithm in C will just produce a slow C program.\n\nIn fact, in many cases, just writing your code \"Pythonically\" can increase its performance by an awful lot. If you can share your code, we can help you work out if it's the algorithm, or if you need to use one of the above options.\n\ni dont believe its my algorithm, running the terrain generator 17 times runs in 8 seconds which is slow for only 4 chunks (16x16 per chunk).\n\ni tried pyston, i cant, pypy wont work, cython did work but it wont produce an exe file.", "id": "hh4q803", "owner_tier": 0.5, "score": 0.9999999975}, {"content": "> i would like how i could compile my program to a C executable\n\nMaybe instead work out a way to compile just the generation part, but leave the rest in python, sort of how numpy works.  Actually, now that I look at the code, consider if you can use numpy arrays and vectorize.\n\ni tried, it kept crying about alot of things and just yeah, ill try to do this tommorow though.", "id": "hh4s4pv", "owner_tier": 0.1, "score": 0.4999999975}, {"content": "If speed were this important it\u2019s probably easier to just write the program in C, C++, or possibly Java. \ud83e\udd37\u200d\u2642\ufe0f\n\ni cant, im trash at C, C++, and i cant get LWJGL working.", "id": "hh4v8im", "owner_tier": 0.3, "score": -2.4999999848063226e-09}], "link": "https://www.reddit.com/r/learnpython/comments/qaqpn0/improve_python_performance_by_compiling_to_c/", "question": {"content": "Hello, i would like to know how i can improve the performance of my python script, it generates terrain and as you may know that is slow, so i would like how i could compile my program to a C executable.\n\n# INFO\n\nmy OS is Windows 11 x64\n\nmy CPU is intel i7 2600\n\nmy RAM is 8GBs, and its DDR3.\n\nmy GPU is EVGA NVidia GeForce 1650\n\n&#x200B;\n\nmy python script is available [here](https://pastebin.com/y56F5d39)\n\nthe C code made by Cython is available [here also.](https://pastebin.com/yiYVSQKn) (note to self: stop using pastebin so much)\n\n&#x200B;\n\n# GOAL\n\na .exe file that runs faster.\n\n&#x200B;\n\ni do NOT want a frozen executable, also cython + gcc wont work.", "id": "qaqpn0", "title": "Improve python performance by compiling to C", "traffic_rate": 153.12462962962962}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "I love python but i came to hate dynamic typing. Yeah you have the typing module but that's just a band-aid.\n\nAnd it boggles my mind that in 2024 such a mainstream language does not support proper multi-threading. Good thing they are working on this.\n\nThis ship had sailed, hasn't it? Python can't add static typing without fragmenting it and I don't think anyone wants a round 2 of Python 2 vs 3.\n\n[deleted]\n\nWhat exactly is the problem with Python's concurrency? I understand the true threading is impossible because of the GIL, but the `multiprocessing` library is a pretty good job of doing all of the normal and current things that I'm used to in other languages, albeit processes instead of threads. The only real downside I found, is that sometimes objects that I want to send aren't easily `pickle`d\n\nTyping is optional. Isn't that great? Nobody is forced to use it.\nFor basic typing you don't need the typing module btw.\n\nI wish python hadnt decided to reinvent the wheel with its typing module and just straight up followed conventions from typescript. Generics is where this is most apparent.\n\nWe now have efforts like [Mojo](https://docs.modular.com/mojo/why-mojo.html) that change the Python-the-language itself to have static typing and static compilation for extra speed. They try to retain Python compatibility, but it cannot be 100%.\n\nI'd be down for a python-like language with static typing though.\n\nNot downvoting, I'd found not once mistakes in my type annotations. In Java such mistakes won't compile. Annotation helps reading the code, but writing it harder compared to fixed typed languages.\n\nBecause \"duck typing\" was the foundation of the language. \n\nPython prioritized readability over everything else, and that proved to be a winning bet. Now we're going backwards and trying to hack static typing into a language that was explicitly designed against it. \n\nToo many new kids not understanding Chesterton's Fence.\n\nI like it much better too. Couldn't go back to not using mypy frankly. It's far from perfect though. I always come across some issue during a work project that leads me to a mypy issue on GitHub or find an error that points me to a limitation of the type system so have to tack on some workaround or place an inline `ignore: type` comment. Neither of which is ideal.\n\nI suspect it will eventually be flawless, but I think we are quite away aways from that.\n\nPython's type annotations are currently rather limiting. Things like proper support of higher-kinded types would go a long way.\n\nYou can't easily share objects between the processes, it's tedious with copying and queues and stuff.  There's the multiprocessing.shared\\_memory module, but it's quite a bit more limited and complicated than with regular multi threading\n\nThere's also overhead associated with multiprocessing - it takes time to fork your processes and they take quite a bit more memory than a thread would.\n\n>Typing is optional. Isn't that great?\n\nNot when having to update or maintain other developers' code.\n\n>Typing is optional.\n\nIt's not \"optional\" in Python. It's nonexistent. There's a reason why it's called \"annotations\" - it's just that, an arbitrary label. Typing is optional in languages like Haskell which are strongly typed but the language can automatically infer the types  in most cases if you omit them.\n\nTyping in python sucks because it doesnt enforce types.\n\nNo, the cons outweigh the pros. There really aren\u2019t any pros to current typing in python. It\u2019s the equivalent to a docstring imo. Doesn\u2019t stop anyone from using the wrong types with functions that annotate types lol\n\nThis is a stupid argument. Typescript doesn't follow conventions of other typed languages, why should python follow the conventions of typescript? Typescript was barely in it's infancy when python's optional typing was GA'd. This is like complaining that typescript typing isn't like Go's typing. Different languages have different syntax and conventions.\n\nCython is exactly that. It's Python with optional static typing that compiles to C and can be called from other Python scripts.\n\nYou can check out Nim as well.\n\n[deleted]\n\nMojo is trying to do that. You can check it out.\n\nI spent a (very) small time playing with Nim, and its syntax is heavily inspired by Python with static typing.\n\nThere is this very new language called Mojo. It's a superset of Python and by the looks of it, it does support static typing. Personally, I like the idea but I doubt I'll see it anytime soon in the real world.\n\nMojo is basically that. It includes static typing among other improvements for speed.\n\nGDscript for Godot is based on Python and has static typing\n\nHaskell\n\nThen write your own language.\n\nIt definitely helps with debugging, too.\n\n[deleted]\n\nI mean the `pipe` and `queue` constructs work like channels do in other languages. It also has mutexes (Locks) and other sync primitives. What do other languages have that Python doesn't in this context?\n\nThe overhead for starting threads *is* there but you can get around the speed part by launching your threads at the beginning and having them idly wait.\n\nThis is a company code standards problem then. If you don't like the code style where you're at, lobby to have it changed\n\nSo then it's optional in your IDE or your linter.\n\nExcept you can access the annotations by code if they're not defined within comments.\n\nPeople complain there is not runtime check for types. You can create a decorator function for checking for example.\n\nWhat are the cons? Longer lines?\nYes it's equivalent to having types in docstrings. That also does not stop anyone from passing wrong types.\n\nTyping is pure IDE magic in the Python world. So just use an IDE. Better than creating Python4 having strict static types.\n\nI'm not that familiar enough in C/C++. What do they do to achieve variant types? Templates, macros, function overloading. Is it fun?\nI'm thankful that's not required in Python. Readability just wins.\n\nYou can use typedload and validate your input data\u2026 isn't that a pro?\n\nbecause python generic typing confuses its own syntax conventions. When a generic is made as `Generic[Type]` it is indistinguishable from indexing. That's why typescript chose syntax like `Generic<Type>`. Moreover, if you've ever looked under the hood for how the typing module has been implemented you'll see that its a mess of ad hoc bandaid fixes that prevents you from easily implementing missing typing features like `keyof<type>`.  Etc etc. The point is, semantically python gets in its own way when trying to implement advanced generic typing.\n\nYes but as of right now, the interpreter ignores type annotations. Having to unify a 3rd party tool, even if developed by the same group, is a band aid at some level.\n\nAccording to their website, mojo is very specialized.\n\ni mean something like this\n\n```\nC = TypeVar('C', bound=Collection)\n\ndef ints_to_strs(x: C[int]) -> C[str]:\n    ...\n```\n\nWas thinking the same.  \nDon't blame the programming language if a developer codes messy.  \nShould apply to every language.\n\nBut in fact typing isn't always as easy as `x: int|str`.  \nI've recently seen a single type expression weighting \\~1K bytes which would be required to get a function of the `requests` package type hinted correctly.  \nNobody would want it and that's a legit reason, the `requests` project refuses type hinting.\n\n> I'm not that familiar enough in C/C++. What do they do to achieve variant types? Templates, macros, function overloading. Is it fun? I'm thankful that's not required in Python. Readability just wins.\n\nIn Rust variant typing is easy, eg if you want to define a type that\u2019s either an integer or a string:\n\n```\nenum IntOrString {\n    Int(i32),\n    String(String),\n}\n```\n\nIn C++ it\u2019s similarly easy, I believe it\u2019s `variant<int, string> value; `\n\n.NET C# doesn\u2019t have discrimination unions built in (yet) but plenty of good libraries exist like `OneOf<int, string>`\n\nAlso don\u2019t forget most modern languages allow implicit strong typing, so there is literally no extra keypresses!\n\nThe huge advantage in strong typing is error checking and diagnostics. You also get better guidance from AI because there is more intrinsic information from the type system.\n\nEdit: typo, \u201cstrong typing\u201d not \u201cstring typing\u201d\n\nThe cons of dynamic typing?\n\nA fully static python would be WAY faster. A friend of my made a language which has a hard requirement on strongly typed function signatures, and everything else, including class members, can be inferred (it is strongly recommended you statically type class members because it decreases compile times). Full inference was what he was originally shooting for, but it turns out to be an NP-complete problem, and types on function signatures solve make it actually possible. The resulting language was as fast or faster than C (restrict pointers are cool).\n\nThat\u2019s pretty much the same thing as an assert isinstance. It\u2019s a runtime check that doesn\u2019t help you when you\u2019re programming\n\n> because python generic typing confuses its own syntax conventions. When a generic is made as Generic[Type] it is indistinguishable from indexing. That's why typescript chose syntax like Generic<Type>.\n\nWould that syntax have even been possible to bolt on to Python syntax before the PEG parser?\n\nThose were already valid tokens in Python:\n\n    >>> one, two, three = 1, 2, 3\n    >>> one<three>two\n    True\n\n\nSo with `Generic<Type>` it isn't until the parser gets to the end of the `>` token that it realises that `<` token isn't a less than.\n\nIt seems to me that syntax might have required an even bigger pile of hacks to work in the Python world. It's very different adding something to an existing language and implementation than starting from scratch knowing what you want.\n\n[deleted]\n\nTrue. I think they primarily focusing on AI but they also want to be a superset of Python which means you can use all python modules which gives you the flexibility of Python with the strengths of a static typed language.\n\nAs of now, this looks the closest in getting static typing with Pythonic style which OP seeks.\n\n[deleted]\n\nTotally agree! I always appreciate good explanations and neutral discussions. Thank you.\n\nAs said, not good at C++. What I remember is they prefer templates and macros. That probably creates multiple functions at compile time. Of course thats fast and the compiler decides on correct typing here already.\n\nTemplates and macros are harder to read on the opposite side. My personal thing I fear for learning C++. And I like direct memory access.\n\nYes. Please explain your statement:\n\n>No, the cons outweigh the pros. There really aren\u2019t any pros to current typing in python.\n\nIt purely sounds like you've never used typing. Because a lot of programmers including me see a lot of benefits in typing. Why else do they start using it?\n\n> That\u2019s pretty much the same thing as an assert isinstance.\n\nIn the same way a song is just some air moving around\u2026 Technically true but completely misses the point.\n\n> doesn\u2019t help you when you\u2019re programming\n\nIf most security errors weren't caused by something unexpected in the input, you'd be right.\n\nDid not realize mypyc does that. Thanks!\n\ntype(x)(map(str, x))\n\nShould work with most collections. Anyway it's just an example so it's usefulness is not the point. Point is, this would be incredibly useful, for instance, for functional programming libraries. Right now they either go for untyped route like toolz, or overly complicated HKT emulation with mypy plugins.\n\nThanks, I always try and avoid language wars!\n\nUse of those variant types wouldn\u2019t cause duplication of a function that took them as parameters (in Rust, C++ or C#) the function itself isn\u2019t generic (or templated, in C++)\n\nEg perhaps you might have C++\n\n```\ndouble AddOne(variant<int, string, double> value) {\n  // return value plus 1\n}\n```\n\nThe function isn\u2019t generic, there is only ever one. The parameter is tightly packed so the int (4 bytes), string (pointer to heap allocated array) or double (8 bytes) overlap in memory. The overall size of the variant is the size of the single largest member (probably the double).\n\nInstead you could use overloading (3 separate functions) or a templated function which amounts to the same thing.\n\nOne strength of using the variant would be polymorphic containers, an array of the variant could mix all 3 types at random. This is fast and dense in memory, it won\u2019t blow up the cache.\n\nIn Python (I think) any native list is an array of references to heap allocated objects, so would not be cache friendly. Hence the need for optimised libs written in other languages. You can basically express the same thing in those strongly typed languages with:\n\n```\nvector<any> mylist; // C++\nvar mylist = List<dynamic>(); // C#\nlet mylist: Vec<Box<dyn Any>> = Vec::new(); // Rust, I think!\n```\n\nBut it\u2019s so inefficient it\u2019s not something I\u2019ve ever done in real code. There would always be a better way.\n\nMy position is that dynamic typing is bad and no amount of cope that python typing introduces makes dynamic typing less bad. I don\u2019t care if the IDE can statically provide some safety if it doesn\u2019t provide runtime guarantees\n\n>If most security errors weren't caused by something unexpected in the input, you'd be right\n\nBasically true. But it smells like C/C++ buffer overflows, unsecure string function calls, etc. Those mostly don't apply in Python. Except SQL injection if you do it wrong enough of course.\n\nCan you elaborate some examples of \"unexpected input\" especially in Python? Web-Apps? GUI applications? In Python we don't have malicious data which gets passed by system calls right into openssl or other critical system calls, IMHO.\n\n[deleted]\n\nIf you really need runtime guarantees, check the types in your functions. You'll do it anyway if a functions supports multiple data types as input.\n\nTechnically you can create a decorator which checks types at runtime on each call. Yes. Another slowdown of your code.\n\nPython just doesn't care for bad implementations of code.\nPython is still good enough even if it's not the fastest languages and you can't or shouldn't write device drivers ;-)\n\nYou want a json with a list of ints, you instead get a list of ints with 1 element that is a string, now your code will crash.\n\nAgain this is just an example. \n\nMy last job made me really wish we had HKT.\n\nMy apps don't crash on that. My JSON from NoSQL documents and config file options always are optional.\nException handling is the way to go here to not get the app crashed.", "id": "kgvt9nj", "owner_tier": 0.5, "score": 0.9999999999613899}, {"content": "guys who are saying to write code fast instead of doing this should read this PEP https://peps.python.org/pep-0703/\nand read the remarks of all the people working in the industry", "id": "kgx9gyo", "owner_tier": 0.1, "score": 0.06177606173745173}, {"content": "Pandas is kinda magical. I had a one-off task to merge multiple huge csv's by common column names and just did a quick and dirty python script but killed it when my progress meter told me it would take 17 hours to complete.\n\nI didn't have that much time to get it done and wasn't super familiar with pandas but bit the bullet and converted the script. Was shocked that the pandas version took less than 10 minutes. \n\nIt's a little weird to learn at first but does so much, so fast.\n\nHow huge are we talking? I ask as I have something with millions of rows I need to parse thru\n\nI think it was 7 docs that were all close to but under a million. I also had collisions and had to rename but keep the colliding columns and a few other nitpicky requirements but pandas handled it all.", "id": "kgz29be", "owner_tier": 0.7, "score": 0.05405405401544401}, {"content": "Different tools for different tasks. I don\u2019t see anyone complaining the performance of shell scripts. If you need the performance, write a C++ library and call it in python, or even rewrite it in C++, which quite a lot of companies are doing now.\n\n>If you need the performance, write a C++ library and call it in python, or even rewrite it in C++, which quite a lot of companies are doing now.\n\nSo you want to ask [10.1 million](https://distantjob.com/blog/how-many-developers-are-in-the-world/) Python programmers to learn C++ instead of having 6 [Microsoft programmers](https://devblogs.microsoft.com/python/python-311-faster-cpython-team/) apply [60 year old](https://en.wikipedia.org/wiki/Just-in-time_compilation) Interpreter optimization techniques to Python. \n\nWhy would you think that makes sense?\n\nWell Microsoft was willing to hire GvR out of retirement to improve Python performance, so they recognize a need for it.\n\n> I don\u2019t see anyone complaining the performance of shell scripts\n\nbash is slow.\n\nYou're welcome! ;-)\n\nWould you want to ask 750 million excel users to learn python for data analysis? My answer is the same as yours.\n\nWell that is easy, you will still have an interpreter in the end.\n\nBetter performance is nice as long as we don\u2019t give up Python\u2019s ultimate usefulness as a scripting language.   I still maintain it somebody starts a project knowing performance is a key consideration then the use of Python was ill advised.    Part of being a professional is choosing the right tools.  \n\nWhat we really need is a language designed for performance that is as well designed as Python and avoids as much complexity as possible.  This is why I\u2019m keeping an eye on Mojo, Swift and other new comers.  Mojo has the potential to be the Python 4 of the future.  \n\nI\u2019d rather see a new language replace Python 3 than to have the current releases bastardized by continual grafting on of new features that don\u2019t add much as a scripting language.  I can see Python 3 lasting for another decade if care is taken to maintain its good qualities.    The thing is this I like Python as it is.    That is it is the best scripting language that we have at the moment.  If you look at the History of C++, it demonstrates what happens when developers go nuts with language design.  Even Rust is seemingly falling into the same trap.   C++ has exploded complexity wise and that is what I hope we can avoid with Python.\n\nAre you complaining? :D\n\nI think it is great to make Python available to [750 million Excel users](https://support.microsoft.com/en-us/office/get-started-with-python-in-excel-a33fbcbe-065b-41d3-82cf-23d05397f53d). But I am also in favour of the [improvements](https://chandoo.org/wp/what-is-lambda-function/) they have made to the Excel formula language so they can get some of the power of Python without changing languages.\n\nSimilarly, I am happy that people continually make it easier and easier to use other languages from Python. And also happy that Python improves so I don't need to do that most of the time.\n\nThere aren't hard boundaries between the applicability of these languages. If your Excel spreadsheet works 99% perfectly and you just need a lambda, then you should use a Lambda. If it's gotten incredibly complicated and adding a lambda will only make it worse, then you should switch to Python.\n\nIf your Python works 99% perfectly and you just need 30% more speed then you should stick with Python. If you've optimized everything to an inch of its life and your Python doesn't look like Python anymore then you should switch to another language.\n\nChoice is good. I have no idea why people are opposed to choice.\n\nI don't understand what you are trying to say.\n\nDude. None of these optimisations require you to change your python code AT ALL. Guido offers you a free speed up up on billions of lines of already existing Python code and you are complaining? And saying he should invent another language instead?\n\nWhy?\n\nYes.  \n\n\nMy daily cron tasks took *several* milliseconds too long!\n\nBash being slow is very far down the list of reasons it sucks as a programming language. Long before you write a program big enough to be slow, you will run into all of the other problems.\n\nI basically agree :D\n\nIf you want python to remain as useful as it is, you still need a language that is basically an interpreter.   In effect those Microsoft programmers could be spending their time building a higher performance tool that is like Python but doesn't suffer its shortcomings.\n\nDon't get me wrong you can always find ways to speed up a language.   I just don't see us ever having C like performance without giving up a lot of the good stuff in Python.   I'd rather see Python go on like it is and have a new language grow out if it.   This is why I like Mojo, it is development that takes a lot of ideas from Python; just like C++ borrowed from C and the various other languages evolved from C, and C++.\n\nAll those millipennies in wasted electricity!\n\nA just in the time compiler is a compiler with the usability of an interpreter.\n\nThere are many higher performance tools in the world already. Why would Microsoft waste their time making another? The python ecosystem is much more expensive to recreate than a JIT compiler.\n\nPython will never be as fast as C. So what? Who cares?\n\nYou can get paid a million dollars a year to be in the NBA and not be as good as Michael Jordan. Does that make you a failure?\n\nI really don\u2019t understand your point.", "id": "kgwhlz8", "owner_tier": 0.1, "score": 0.20077220073359073}, {"content": "Just go Mojo \ud83d\udd25", "id": "kgwo21m", "owner_tier": 0.1, "score": 0.03861003857142857}, {"content": "C/C++ is slow too, if the programmer does it wrong.\n\nBut Python as same as Java and other languages should just not be compared to machine code level languages. Beyond that you can compile a Python application in various ways to be faster.\n\nPython is not meant to do heavy calculations. And that's not a bug neither a problem because NumPy and similar tools do exist.\n\nPeople should learn or at least understand Python correctly instead of having prejudices.\n\nWhile it may use more memory, Java is known for being wicked fast with a tuned GC. IIUC, jit'ed languages can in theory run faster than aot-compiled languages because jits can analyze and make assumptions about the data aot-compilers can't do\n\n> Python is not meant to do heavy calculations. And that's not a bug neither a problem because NumPy and similar tools do exist.\n\nAgreed, but that doesn\u2019t mean that a faster Python wouldn\u2019t be great. The faster I can call NumPy\u2026\n\nI mean it\u2019s complicated. And you could argue that sure Python should probably not be the only language you know going to lower level languages when performance becomes an issue \n\nBut at the same time writing crap in C/C++ is way more difficult and annoying. (go isn\u2019t too bad but it\u2019s also kinda its own island). Python is a breeze if I could write everything in Python I would. And to be fair it\u2019s pretty rare that the speed ever becomes an issue \n\nBut it would be really amazing if I could write everything in Python maybe with static typing and have it be as fast as something in C. Rapid development time and all the libraries in the world. Getting closer to that is a worthy endeavor and there is likely a lot that can be done to get closer. Hell PHP can be faster these days than Python\n\nI've been designing real-time embedded control systems for nearly 4 decades and seen people absolutely destroy performance in C, even without being I/O bound.  Some people just don't understand algorithms or basic computer science concepts and how they should be applied in different circumstances.\n\nIt is rubbish which people keep repeating. Wrong written  code is not a topic of the discussion. Reasonable quality code is fast in c/Java. Python is nothing like Java, it can be compared to Ruby, lua, JavaScript. \n\n> Python is not meant to do heavy calculations\n\nIt is general programming language, it is not designed for specific purposes. What kind of argument is it? \"if not working - not meant to be\". We are discussing what Python can be used for, if \"not meant to do heavy calculations\"  - ok, whatever reason, marking checkbox off.\n\nBut that's not the point.  \n\n\nIf a programmer does equally bad/good in C++ and python the C++ program will run faster most of the time and probably by a lot.  \n\n\nI mean why else do so many computation-heavy python modules use C++?\n\nI'm currently learning Rust and I heard somewhere that Rust is 80 times faster than Python. Not sure how true but just food for thought.\n\nJava is fast as soon as the runtime is fully loaded.\n\nBy that time many Python scripts are already done.\n\nSince every new Python release offers performance improvements, we're getting there. Even the GIL is adressed, which should improve speed and hopefully not break old code in the future.\n\nFWIW, you've kind of described Cythin and get all the benefits of static typing, but you're also knd of dropping down to C.\n\nIf they do, they would write it in C++.\n\nYou don't do AI in pure Python. You don't implement 3D rendering in C# directly instead of using DirectX APIs.\nIf you choose the plain language for these things, your doing it wrong. And it's slow then. Same rule for Python.\n\nOften when a Python program is slow, someone is using it like it's C++. Wrong use of lists for example. It's not the fault of the language itself.\n\nSo if you have \"heavy calculations\" in any language, use the CPU cycles for that directly. It's not cheating.\n\nBecause of the nature of Python, everything being an object. Not plain memory addresses and CPU instructions.\n\nAndroid and probably Java too are using native compiled modules.\nPHP the same. They won't do crypto stuff in pure PHP for example.\n\nMy point is not comparing cars and bicycles and then complaining bikes are slower.\nFair is comparing machine level languages to each other, but jit languages seperately.\n\nBut it's easier and faster to write correct code in Python. That's like the whole point of high-level languages.\n\nIt's probably faster than Python, but it heavily depends.\n\nBad rust code can be slower than good Python code on a specific task.\n\nDon't forget that Python calls natively compiled functions when computing power is needed.\n\nYou use your car to get into the plane at the airport. Fast enough in sum to reach your destination. Nobody should blame the car until you want to do a really long road trip.\n\nI can\u2019t speak about Java, but I\u2019ve done some comparisons between C# and Rust, for computationally bound work (SHA256 hashing files). In .NET 8 binaries can be \u201cahead of time\u201d compiled, and the performance was impressive. Only 5-10% slower than native Rust.\n\nI believe Java collections use boxing even for intrinsic types, so the performance there might be lower?\n\nYou realize there are people on this planet in software development who have other use cases than just running a python script that takes 2 seconds to finish, right?\n\nNow I'm curious about the differences between Java and Python (in performance). They're technically comparable, aren't they?\n\nMay be both languages could learn from each other?\n\nAddressing the GIL makes python slower, not faster. It's why they hadn't fixed it until now.\n\nPython will never be \"there\". Improvements are impressive, but there are limits.\n\nI can't tell if this is sarcasm or not.  Because, I can't tell if you're being serious.  Why would I rewrite a C application in C++, to fix performance issues?  \n\nIf you're referring to the OP's post, your response might have merit, except I'd still need to look at what the performance issue is and why python could be the wrong solution, before I'd agree with that statement.\n\nSure. Never said there aren't. I'm amongst them actually. \n\nI was just explaining that there are different kinds of speed.\n\nAnd while Java is generally much better at running speed, Python is better at startup speed.\n\nObviously those aren't the only criteria. And both languages can get used for everything, just with a different strengths and weaknesses.\n\nYou have a bad attitude!\n\nIn java if you write a+b, the compiler knows a is an int, b is an int and a+b can be done by the add instruction of the CPU.\n\nIn python a might be anything, so at runtime dictionaries are explored to find the __add__ function of that specific object, then call it. So it really can't directly be done with an addition in the CPU, since it might be anything.\n\nWhen the Java runtime is up and running it's fast. So if you have a big, big long running task and you can't or don't want to optimize crucial parts into a C/C++/Rust module for Python then Java will give you much better long term performance.\n\nBut for short tasks a Python script will be done so quickly that Java is still busy with startup while Python is done already.\n\nIn theory Python could learn JIT compiling from Java and there have been several attempts to do that. But so far none of them are ready to be a drop in replacement. Every faster implementation of Python suffers from lacking full compatibility or being able to just run any established modules from the repo, as CPython can.\n\nAt the end of the day the 2 languages don't have the same target audience and thus different strengths and weaknesses.\n\nI meant avoiding the GIL in the future of course.\n\nGood enough. Someone told some JIT compilers can optimize to be better than a plain C/C++. If true, a limit has been crossed.\n\nI'm working on a video cutter software which just uses caches and then should be faster than classic software. Still just a simple Python programm which does not need to be super optimized to reach the goal.\n\nQuite odd to say that python will never get \u201cthere\u201d without defining \u201cthere.\u201d\n\nIt's been a working point in recent years, so we have GraalVM that starts up in like 1/10th of a second and Spring Boot 3.0 has support for it so the startup time is a non issue anymore. \n\nJava fixed a lot, and i mean a lot of it's issues and shortcomings in the last decade.\n\nTracing JIT largely solves this problem. You put a type guard earlier in the function to ensure the types are always integers and then you use the CPU add in your inner loop. If the type guard ever triggers then you run slower code to figure out what is going on.\n\nAre you building estimation of Python state based on \"someone told? Python doesn't compete with C, if it gets close to nodejs, it will be a good outcome. Same thing for jit, I don't believe it will be faster than node's.\n\nMaybe I replied  to a wrong comment. It was about Python is not fast as c/c++, but getting there because of improvements with each release.\n\nYes, but it will require extra instructions and in general can never be as fast as a typed language.\n\nYou can get [close enough](https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/javascript.html) that performance is mostly dominated by programmer skill rather than language.\n\nAlso, startup time tends to be better for Python and Node than Java and .NET.\n\nIf you include the end to end time, JITted code will often win, even putting aside programmer time/effort.\n\nWell I wrote a pure python module that manages to beat in some benchmarks a similar module written in rust\u2026 I know skill trumps everything. But at equal skills, in the end more CPU instructions are slower than less.", "id": "kgvjaoo", "owner_tier": 0.1, "score": 0.5637065636679537}, {"content": "The tricky part with python, especially with how I've seen it used in Data Science is that most implementations are in the cloud, most who provide near infinite computing power for little to almost no cost.\n\nI can never seem to incentivise my peers to write efficient code.\n\nIf you have \"near infinite computing power for little to almost no cost\" then what is the problem? Why are you trying to convince your peers to do something that has no business value?\n\nI have certainly hit performance limits in Python and will be glad of the changes. But for apps where it doesn't matter, it doesn't matter. Why nag people about something that doesn't matter?", "id": "kgwaqy4", "owner_tier": 0.1, "score": 0.05019305015444015}, {"content": "I'm betting on mojo to become a superset of python ala typescript.", "id": "kgy2r6i", "owner_tier": 0.1, "score": 0.03474903471042471}, {"content": "I really like that people begin to talk more and more about Python performance. All of this suggests that there is demand for making Python fast, and it admires me. This is the competition for performance that will make the whole industry benefit.", "id": "kgyh742", "owner_tier": 0.5, "score": 0.03474903471042471}, {"content": "If performance times are a big need for a certain service, wouldn't you just write a Microservice in a language with better performance metrics? \n\nWhat is the use-case for a performance-oriented Python fork vs a microservice written in C?\n\nAm I missing something here?\n\nSo you\u2019ve written a micro service in Python.\n\nYou are offered two options:\n\nOption 1. Guido makes it faster for you and your cloud hosting bills go down, your electricity wastage goes down and you don\u2019t need to do anything at all.\n\nOption 2. You continue to pay the higher cloud hosting bills, waste electricity,destroy the environment and also do nothing. \n\nWhich option do you prefer?", "id": "kgznf5r", "owner_tier": 0.1, "score": 0.03861003857142857}, {"content": "From a pure didactic point of view, an \"inefficent\" language helps you optimize better your code", "id": "kh167a3", "owner_tier": 0.1, "score": 0.03474903471042471}, {"content": "How to make it faster: don\u2019t write shit code.\n\nFfs, you solved it!  My code is perfect now!!! /s\n\nwow! why didn't the scientists such as Heinrich Kuttler, Manuel Kroiss, Pawe\u0142 Jurgielewicz, Allen Goodman, Olivier Grisel, Ralf Gommers, Zachary DeVito didn't think of that?\n\nI use python because I want an objective completed quickly, not because I need code to run fast. When I need more performance, I switch to something like C#. \n\nThat said, isn\u2019t mojo solving the perf issue? I haven\u2019t messed with it yet. I\u2019m surprised I haven\u2019t seen more hype around it.\n\nI'm glad you found my advice useful.\n\n\ud83e\udd37\n\nDanny DeVito did\u2026 you shoulda listened bud\u2026\n\nWhy is it so difficult to understand that this was meant to be a humorous comment?\n\nare you upset that i didn\u2019t respond with another joke? i\u2019m not even arguing with you. in fact, the reply wasn\u2019t even directed at you.\n\ni layered commentary on a relatively high comment. relax.\n\nBecause of Poe\u2019s Law\n\nYou seem the one who's butthurt. I merely pointed out the obvious without any particular feeling.\n\nMy ass is aching\n\nWe can tell", "id": "kgvemjq", "owner_tier": 0.5, "score": 0.25096525092664096}, {"content": "Optimization goes a long way, learn how generators and inline if else statements works, also think about unescesary variables.", "id": "kgwuapv", "owner_tier": 0.3, "score": 0.04247104243243243}, {"content": "Perhaps, don't write code in a way that would make it slow to begin with? Then just simply compile it to C code using a compiler (nuitka) and then run it. \n\n\n\nEdit: __Not to be taken seriously__, it's sad that I even have to say this....what is wrong with you people. This comment is the equivalent of saying \"Lol, just write fast code then.\" xD\n\nPerhaps you need to learn the principles first:  make it work, make it work correctly, make it fast.\n\nYou are kinda proposing premature optimization.  Don't.  And before *any* (manual) optimization, profile, don't guess.\n\nI have seen 0 performance gains using nuitka on my scripts. What\u2019s your experience?\n\n>nuitka\n\nInteresting, haven't heard of this project before and I spent a lot of time working on a single-node Python project that has a big CPU bottleneck. Thanks for bringing this up.\n\nI assume even though it's compiled to C, it still doesn't enable multithreading?\n\nBut why use python than anyway, if you end up doing all in C?  \n\n\nMaybe I could just use C#, Javascript, Java, Go, that are 1/2 speed of C++ but still a lot easier to program with (compile times suck for C/C++)\n\nNuitka doesn\u2019t do heavy optimizations, it\u2019s more for packaging than anything. The generated C code is very slow still\n\nwhy didn't the scientists such as Heinrich Kuttler, Manuel Kroiss, Pawe\u0142 Jurgielewicz, Allen Goodman, Olivier Grisel, Ralf Gommers, Zachary DeVito didn't think of that?\n\n>Perhaps you need to learn the principles first: make it work, make it work correctly, make it fast.\n\nActual High performance code is so wildly different to \"just get it done\" code that it's basically a different application at that point. e.g If you have a wire format you control you can do a lot just using bit operators or you can give estimates of the values present in a memory page using a header. Or likewise you can use knowledge of data sharding characteristics to distributed load across threads efficiently to keep data locality. If it was as easy as people said on the internet then scaling problems would be a thing of the past.\n\nThere is a lot of stuff in critical applications where there's actual money/lives on the line where you can't just bolt it on later and even trying would likely cost you more than a rewrite. The problem is the the person saying \"premature optimisation\" usually never sticks around long enough to see the problems play out.\n\nthis mindset is why all software is so fucking slow and so fucking shit. nobody actually eveer makes it work correctly anyway!\n\nHaven't seen any significant gains with Nuitka, that isn't really what it is for.\n\nNumba however I have made some code with numerical calculations and image processing nearly as fast as C++.\n\nIt can\u2019t change how your code works. If it did, you would get bugs from race conditions\n\nIf you mean nogil multi threading, then no, it still follows the python spec, which mean it\u2019ll work with threading in a similar way as cpython\n\nIkr, it's so simple. Just write fast code lol\n\n> Actual High performance code\n\n\n\n> critical applications where there's actual money/lives on the line\n\n\n\n\nIs this kind of stuff really written in python? I like and use python, but I don't want my critical software (even file manager and text editor) written in it.\n\nThat's a budget problem.\n\nGood to know! I've asked someone who used numba in his project for the performance gain he got. Didn't get an answer. He probably never checked.\n\nWe used nuitka with PySide2 on a Raspberry Pi Zero. I can't remember any more if it had noticable performance improvements.\n\n> \n> \n> And it boggles my mind that in 2024 such a mainstream language does not support proper multi-threading. Good thing they are working on this.\n\nI've had really good improvements with Cython as well - it's becoming my go-to for any codebase more involved than scripting.\n\nNuitka is first and foremost a performance tool actually, but the creator, Kay, hasn\u2019t had a lot of time to work on that part, he will soon.\n\nPython is absolutely used in business-critical applications. Basically ever major tech company is using Python. Instagram is built on top of Django, for example.\n\nWhich is not what you're replying to.  Note the \"lives on the line\".\n\n\"actual money on the line\" is literally in the comment", "id": "kgvfxml", "owner_tier": 0.5, "score": 0.28185328181467184}, {"content": "Use C or Rust instead of Python /s", "id": "kgvtnho", "owner_tier": 0.5, "score": 0.0038610038223938228}, {"content": "Step one: Rewrite the whole program in C", "id": "kgx54ay", "owner_tier": 0.7, "score": 0.019305019266409266}, {"content": "I love python but i came to hate dynamic typing. Yeah you have the typing module but that's just a band-aid.\n\nAnd it boggles my mind that in 2024 such a mainstream language does not support proper multi-threading. Good thing they are working on this.", "id": "kgvtaee", "owner_tier": 0.5, "score": -3.861003837538722e-11}, {"content": "Python runs at C speed. python or python3 executable is a compiled c program. Just that every line of python code needs tens to hundreds of lines of C code to run for it. \n\nIt also means, single line of python code abstracts several 10s of lines of equivalent C code. Just like how single line of C code could abstract 10s lines of assembly machine code.\n\nChoose wisely between Python, C, Assembly as much as you need for your use case.", "id": "kgxm3sf", "owner_tier": 0.1, "score": 0.023166023127413126}, {"content": "[Brandt Bucher \u2013 A JIT Compiler for CPython](https://www.youtube.com/watch?v=HxSHIpEQRjs)\n\nIt will take some time for the JIT work to show benefits and I'm not sure what to expect with this simplified JIT approach. It isn't merged yet either. Python (cpython) is currently slower to run with the JIT enabled.", "id": "kgxysap", "owner_tier": 0.3, "score": 0.03088803084942085}, {"content": "No one mentioned \"Pythran\" project here. It's an AOT compiler that just needs 1 line of code to compile a function.", "id": "kgy4oal", "owner_tier": 0.1, "score": 0.03088803084942085}, {"content": "The links on your third and fifth bullet are identical (tweet)", "id": "kgzz8ey", "owner_tier": 0.7, "score": 0.03088803084942085}, {"content": "looking for PEP 703", "id": "kh00job", "owner_tier": 0.1, "score": 0.03088803084942085}, {"content": "Nice post", "id": "kh4ylhr", "owner_tier": 0.1, "score": 0.03088803084942085}], "link": "https://www.reddit.com/r/Python/comments/191gmtm/why_python_is_slow_and_how_to_make_it_faster/", "question": {"content": "As there was a recent discussion on Python's speed, here is a collection of some good articles discussing about Python's speed and why it poses extra challenges to be fast as CPU instructions/executed code.\n\n* Pycon talk: [Anthony Shaw - Why Python is slow](https://www.youtube.com/watch?v=I4nkgJdVZFA)\n* Pycon talk: [Mark Shannon - How we are making CPython faster](https://www.youtube.com/watch?v=wyty6sFMWI0)\n* [Python 3.13 will ship with --enable-jit, --disable-gil](https://twitter.com/anthonypjshaw/status/1744144186478375373)\n* [Python performance: it\u2019s not just the interpreter](https://blog.kevmod.com/2020/05/19/python-performance-its-not-just-the-interpreter/)\n* [Cinder: Instagram's performance-oriented Python fork](https://twitter.com/anthonypjshaw/status/1744144186478375373)\n\nAlso remember, the raw CPU speed rarely matters, as many workloads are IO-bound, network-bound, or a performance question is irrelevant... or: Python trades some software development cost for increased hardware cost. In these cases, Python extensions and specialised libraries can do the heavy lifting outside the interpreter (PyArrow, Polards, Pandas, Numba, etc.).", "id": "191gmtm", "title": "Why Python is slow and how to make it faster", "traffic_rate": 207.93734419145753}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "Python has an abstraction level on top of C, so it will be slower than C, whatever you do. If you rewrite a C program in pure python, it will be much slower than in C. However there are three things that make python interesting:\n\n- what actually matters is life cycle cost for a software. It includes developer time, running time, debugging time and cost of resources. Python is much more flexible than C and therefore faster/easier to develop with (but with great power comes great responsibility). So if you need to write a small script that will run for a few seconds every day, maybe it is not worth spending more time writing it in C to save maybe a minute of runtime every year.\n\n- CPU limitation is just an element of your code speed. When you are dealing with network access, or even file system access, a lot of you execution time is waiting for these operations to finish. You won't gain a lot by speeding up the code itself, unless you have enough operations to run things in parallel.\n\n- a lot of time in software, there are just a few bottlenecks in your code. Since python is capable of executing C libraries, you can code these in C , or even assembly if C is too slow, and you will have addressed 80% of your bottlenecks. That's basically the model used in ML: data preparation, model definition are the parts that can change a lot every time so keeping them in python saves development time. And also they are not the most CPU intensive task overall so no need to optimise them to death.\n\n> what actually matters is life cycle cost for a software. It includes developer time, running time, debugging time and cost of resources\n\nTo put it another way, there are better languages than Python for making things work quickly. Python is a language for making things work, *quickly*.\n\n[deleted]\n\nTo cap it off, Python's undergone such a huge amount of development in the last 10 years, that if you want that quick solution in development/deployment/production, 90% of the time you can just drop it into an existing system where everything just works. Containerization and cloud development has only made this a more compelling architecture.\n\nThat's a very Pythonic way of saying that.\n\nFor these reasons why i made Nim my go to language.\n\nI've also been feeling this more and more as I've gotten more experience developing software. There are entire classes of bugs that just don't exist in a statically typed language, just like there are entire classes of bugs that don't exist in memory safe languages like Python (and unlike C).\n\n> For everything significant I prefer statically typed languages.\n\nYep, exactly why TypeScript was made for JavaScript. I think you are a certain level of insane if you take on a large project without typing and the IDE yelling at you before you run the code when you are using the wrong type.\n\n>Containerization and cloud development has only made this a more compelling architecture.\n\nBe warned that python is even slower than normal on a container, due to libseccomp screwing you over (I think with Spectre/Meltdown mitigations).\n\n[deleted]\n\nTry Nim", "id": "hnfsszh", "owner_tier": 0.5, "score": 0.9999999999898888}, {"content": "> I work as ML Engineer\n\nThen you should know that the ML libraries and any library with heavy math that Python uses are mainly written in C/C++/Fortran/any other fast compiled language, not Python, Python is mainly used for calling functions from those languages. \n\nThat's why you \"never felt like Python is slow\", cause you were really running C/C++ that Python just calls, if those libraries were written in pure Python, they would be 100-1000 times slower.\n\nIt's a good combo, fast but inflexible language to do the \"heavy lifting\" part, slow but flexible language to do the \"management\" part, best of both worlds, and works surprisingly well.\n\nOf course that ends once you stop using and start writing a \"Python\" math heavy library, then Python is not an option anymore, you will have to use another language, at least for the heavy parts.\n\nHere's a very crude example of this at work. Consider adding `1` to every entry of a huge array of numbers. In python you could just use a big ol' list of lists, or, if you're smart, you'd use `numpy`. That latter is much faster:\n\n    import numpy as np\n\nfrom timeit import default_timer as timer\n\nSIZE = 10000\n\n\nprint(\"Starting list array manipulations\")\nrow = [0] * SIZE\nlist_array = [row] * SIZE\nstart = timer()\nfor x in list_array:\n    for y in x:\n        y += 1\nend = timer()\nprint(end - start)\n\nprint(\"Starting numpy array manipulations\")\na = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\nstart = timer()\na += 1\nend = timer()\nprint(end - start)\n\n\nOn my laptop:\n\n    Starting list array manipulations\n4.841244551000273\nStarting numpy array manipulations\n0.40086442599931615\n\nThat's one of the beauties of python, it was designed to be really easy to leverage new or existing binary libraries. So while it is maybe not pure python, it is part of what python was designed to do.\n\n[deleted]\n\nFormatted edition:\n\n----\n\nThat latter is much faster:\n\n    import numpy as np\n\n    from timeit import default_timer as timer\n\n    SIZE = 10000\n\n\n    print(\"Starting list array manipulations\")\n    row = [0] * SIZE\n    list_array = [row] * SIZE\n    start = timer()\n    for x in list_array:\n        for y in x:\n            y += 1\n    end = timer()\n    print(end - start)\n\n    print(\"Starting numpy array manipulations\")\n    a = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\n    start = timer()\n    a += 1\n    end = timer()\n    print(end - start)\n\n\nOn my laptop:\n\n    Starting list array manipulations\n    4.841244551000273\n    Starting numpy array manipulations\n    0.40086442599931615\n\nIf someone knows how to make the markdown editor actually accommodate code blocks sensibly, please fix this mess.\n\nEdit: disregard my conclusions here, per the responses to this comment. Leaving the comment up so people can follow the discussion.\n\n~~Iterating through every item of the every list is not necessary. Instead, one could use the python built-in \"map\" and it would go much faster. Faster than using numpy, in fact. The numpy code is easier to read, of course, but not faster.~~\n\n    import numpy as np\n    from timeit import default_timer as timer\n    \n    SIZE = 10000\n    \n    print(\"Starting list array manipulations\")\n    row = [0] * SIZE\n    list_array = [row] * SIZE\n    start = timer()\n    # for x in list_array:\n    #     for y in x:\n    #         y += 1\n    list_array = map(lambda y: list(map(lambda x: x+1, y)), list_array)\n    end = timer()\n    print(end - start)\n    \n    print(\"Starting numpy array manipulations\")\n    a = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\n    start = timer()\n    a += 1\n    end = timer()\n    print(end - start)\n\nOn my 10-year-old desktop:\n\n    Starting list array manipulations\n    2.6170164346694946e-06\n    Starting numpy array manipulations\n    0.6843039114028215\n\nEvery programming language has a foreign function interface that can speak to the C ABI, it's a requirement for communicating with the OS via syscalls (without which you will not have a very useful programming language).\n\nHaving such an ABI does not make Python particularly special, and I would argue CPython's ABI is not particularly good. It's actually a very nasty hairball with a lot of unintuitive dead ends and legacy cruft. NodeJS is probably the market leader on this today for interpreted languages, and obviously compiled languages like D/Rust/Go/etc can use C headers and C code rather trivially.\n\nHaving python as a bridge layer isn't a bad practice. Serving models directly from python tends to be really slow (depending of course on the library and model itself, but I'm assuming some level of deep learning here) compared to using an actual inference engine (nvidia's Triton server has been great), so I would definitely not recommend that, but Python makes for great API code. Most of the deploys I've done have included python on the user interaction layer with the inference pipeline being built with heavier systems.\n\nJust prepend every line with four spaces and it works (triple backticks does NOT work on old reddit).\n\nIt's easiest to do this by just copying it into a code editor (like vim or vscode) and indenting all of the code once, then paste it into the reddit box.\n\nUnless you're running Python 2, this comparison is not at all the same, `map` returns a generator and not a list -- you're timing how long it takes to create a generator object, not how long it takes to construct the list. If you want an equal comparison, you need to wrap `map` calls with `list` -- just like you did with the inner `map`.\n\nIt is **much** slower.\n\n    \n    >>> from timeit import default_timer as timer\n    >>> \n    >>> SIZE = 10000\n    >>> \n    >>> def mapped():\n    ...     print(\"Starting map timing\")\n    ...     row = [0] * SIZE\n    ...     list_array = [row] * SIZE\n    ...     start = timer()\n    ...     # for x in list_array:\n    ...     #     for y in x:\n    ...     #         y += 1\n    ...     list_array = map(lambda y: list(map(lambda x: x+1, y)), list_array)\n    ...     end = timer()\n    ...     print(end - start)\n    ... \n    >>> def nomapped():\n    ...     print(\"Starting list timing\")\n    ...     row = [0] * SIZE\n    ...     list_array = [row] * SIZE\n    ...     start = timer()\n    ...     # for x in list_array:\n    ...     #     for y in x:\n    ...     #         y += 1\n    ...     list_array = list(map(lambda y: list(map(lambda x: x+1, y)), list_array))\n    ...     end = timer()\n    ...     print(end - start)\n    ... \n    >>> mapped()\n    Starting map timing\n    5.516994860954583e-06\n    >>> nomapped()\n    Starting list timing\n    5.158517336007208\n\nJust using `map` is only faster in some situations -- situations where you only need to iterate over a set once. If you're using numpy, you presumably are going to be reusing your arrays (well, dataframes) across multiple operations.", "id": "hngad3z", "owner_tier": 0.7, "score": 0.3761375126289181}, {"content": "Depends on how your program is written.\n\nIf you are \"vectorizing\" your code and calling fast libraries like Numpy or Pandas (which are itself written in Fortran or C) your code can be very fast - often faster than \"hand-written\" solutions in other languages. Same for JIT-compiled code with Numba.\n\nBut if you are writing large loops (>> 10k iterations) in pure (C-)Python it is very slow - often a factor of 100 slower than in fast compiled languages.\n\nCython tries (reasonably successfully) to make up for the gap\n\nAnd Spyder is the best ide for *cythonising*\n\nWhich is faster at adding big integers:perl or python?\n\n> But if you are writing large loops (>> 10k iterations) in pure (C-)Python it is very slow - often a factor of 100 slower than in fast compiled languages.\n\nIt's even worse than that:\n\nThe loop does not need to be large. It just needs to be sufficiently hot.\n\nAnd 100x is just a single threaded penalty. Multi-threaded, multiply it by the number of cores available: so you'd get 800x to 3000x penalty\n\n[deleted]\n\nAn issue with Cython is that it gets slow again if you are calling Python functions from within. Thus, for good speed you need to make sure to use only C (or other compiled) libraries inside critical loops. \n\nAs a toy example I tried to write a Monte-Carlo pricer in Cython (and other languages). The issue with the Cython version was the nomal distributed random number generator:\n\n1. using the default Python one was slow\n\n2. I could not find a fast C library for it (I am sure there exists one, but search and integration effort is significant)\n\n3.  writing your own normal distributed random number generator based on \"standard\" algorithms gives you rather poor performance compared to optimized algorithms\n\n>Cython tries (reasonably successfully) to make up for the gap\n\nIt's fine if you Cythonize 10 lines of code. If you realize most of a module of 1000+ lines of code is slow, it's really a pain.\n\nHow does Spyder help with cythonizing?\n\nI have no experience with Perl therefore I cannot answer your question.\n\nBigIntegers are slow in any language because they are not a native machine type. Consider using Int64 or Float64 instead.\n\nI can't comment on performance per se but python handles big integer seamlessly compared to other languages. It's a+b or a%b vs say Java BigInteger.add, etc. So shifting from math to code is a lot nicer in python.\n\nOne thing to keep in mind is that native exponentiation (**) has some limits. You'll want to use a fast exponential algorithm or similar. I just wrote my own but I'd be shocked if there isn't a good version in standard libs.\n\nI'm taking a masters level cryptography course and have implemented all the number and group theory in python,  going to Java when doing more standardized tasks because of Javas excellent crypto algorithm support.\n\nWhile I don't have your specific answer, there seems to be a toss-up of which language is better, based on submissions to the Benchmark Game site.\n\nhttps://benchmarksgame-team.pages.debian.net/benchmarksgame/q6600/fastest/perl-python3.html", "id": "hnfrb9q", "owner_tier": 0.7, "score": 0.1799797775429727}, {"content": "As others have said, \"too slow\" is a question of context, but figured I'd give an example from my day job and what we did about it.\n\nThere's a system I work on where one of its features is that it lets users download CSV reports. These reports are generated on the fly from data in the database. We did performance test most of the system before going live, so we identified most of the performance issues, but we overlooked the CSV report download feature. After go-live, users complained about slow download speeds (around 2mbit/s). When we profiled the code, we discovered that the bottleneck was the CSV library we were using which was written in pure Python (we weren't using the standard library one, which is well optimized and written in C, for reasons that I won't go into). Rewriting the bits of the CSV library that we needed in Cython proved to be enough to get download speeds up to the point where it was faster than most users internet connection speeds.\n\nSo the requirements are your context. Sometimes the requirements are not explicit, and you only discover them when you get them wrong (we never had any requirements for download speed, until users started using the system), but \"too slow\" always depends on how fast you need it to be.", "id": "hng0pt1", "owner_tier": 0.7, "score": 0.04550050555106168}, {"content": "Rarely is your python the bottleneck in most domains where python is popular. When it is, you can pretty much always do something about it.\n\nQ. When do you know your Python is really \"too slow\"?  \nA. When you profile it.\n\nMinor nitpick: you discover that your code is too slow when you benchmark it or performance test it (perf testing and benchmarking are essentially the same thing, but different names are used in different contexts). What profiling tells you is _why_ your code is as slow as it is.\n\nThe idea is that you don't know if it's actually your code that's slow before you profile it; it can be any layer in the request cycle.\n\nBut if performance is your problem, then ultimately it's all \"your code\", even if the bottleneck turns out to be an external call to a third party service. It's then your job to find a way to make that faster (maybe you can cache the result of the third party call? Or there's some way you can optimise the query you're making? Or you can find a way to avoid the call entirely?)\n\nBut this is about *python being \"too slow\"*. It's not about \"your application being too slow\". \n\nYou need to bring out a profiler (or enable/make some performance logging) to know whether it's your code or any of your dependencies that is the problem.\n\nBenchmarking/performance testing won't give you any insight into whether it's your code (the Python part) that's being slow (which is what OP is referring to).", "id": "hnfrnnk", "owner_tier": 0.5, "score": 0.08190091000000001}, {"content": "> Is Python really 'too slow'?\n\nNo. Is it slower than other languages? yes.\n\nIf you find it slow or expensive in your usecase, rewrite the high-cost section in rust or c (for example) and call in to it with python bindings (similar to numpy, tensorflow and the rest).", "id": "hnfwr4y", "owner_tier": 0.3, "score": 0.022244691597573304}, {"content": "As an ML engineer, you probably use pretty little Python in fact, as your work will leverage Numpy, Pandas, Dask, PySpark or whatever. You don't actually create and iterate over Python-specific data structures using Python.\n\nPandas, Numpy leverage a lot of C code for the more CPU-complex tasks. You use Python simply to orchestrate those tasks.\n\nThat said, your experience is a testament of the fact that computers today are really fast, and for the most part you shouldn't care if your program is 60-200 times slower than if it were written in C. This is linear performance anyway, and most performance issues that I've seen are based in the fact that developers chose O(n^(2)) algorithms or worse, when an O(log n) could have been used.\n\nThe real world situations where Python isn't fast enough, are really few and hard to find. Maybe if you have some code that manages a huge amount of data, using pure Python, due to a custom logic, then you might feel like it's really slow, and actually impacting your business.\n\nWhen you get to that level of optimisation, you'll see people complain about latency spikes when .NET Garbage Collection is triggered, or other nitty-gritty details about pure performance.\n\nYou won't be building a new database using Python, that's for sure.\n\nBut if you use Python to glue stuff together, and let the real performance-intensive stuff to be done by systems designed for performance, then you'll be Fiiiiiiine.\n\n\n>The real world situations where Python isn't fast enough, are really few and hard to find. \n\nDon't want to bash Python, I'm a big fan... but every single videogame out there is written in C++ (mainly) or other compiled language. Not really hard to find situations where Python is just no no.\n\nThe other place the time effect matters is in high-frequency trading where firms compete over offices that are physically closer by meters to the NYSE mainframe.\n\nYea, a couple of years ago I actually wrote a 2d vector type game (visually similar to asteroids), which as far as games go is pretty simple of course. I wrote the vector collision detection in Python, actually a couple of different naive implementations, basically porting over similar C routines from the available literature.\n\nI benchmarked the collision detection by adding a lot of actors on the screen and found it was adequate for my specific case but if the game I had designed had been busier, slower computers would have struggled pretty soon already.\n\nNow of course there's two things I could have done: first, optimize the routine - probably very much possible to do, but that would have probably taken way more effort than writing all the rest of the game, and also, if I had written the game in C++ in the first place, optimization would have not been necessary at all, the naive implementation would have been fast enough.\n\nSecond is the age old \"just implement the critical parts in C!\"... Yeah, if this had been for work, sure, whatever, but since I was doing this for fun in my spare time, no, absolutely I will not just do that.\n\nC# for Unity.\n\nYeah, of course, large user base software will try to optimize things a lot, which is why OS, browsers, hand engines, are written on the platform that has the least performance cost.\n\nThe Unity engine is almost entirely C++. It then loads Mono as a scripting runtime, with the C# API using binding to the C++ stuff.\n\nC# is at least an order of magnitude faster than Python either way.\n\nWell, when you boil it down to it, CPython is entirely C, and Python is just a scripting runtime, with the Python API using binding to the C stuff.\n\n> C# is at least an order of magnitude faster than Python either way.\n\nDepends on what you do and how you do it. C# is just C#. Python can be so many things. It can be CPython, PyPy, CPython with `@numba.jit`, CPython with C libraries like Numpy, etc. Its native types are more powerful than in .NET, which allow fast operations on data types for which you have to install 3rd party libraries on .NET. Used correctly, it can surpass the performance of .NET.\n\nFor instance, how difficult, and how fast will be your best attempt at determining the 1 millionth Fibonacci number?\n\nIn Python, on my desktop, it's 5.21 seconds using a 5-line function. Or 0.15 seconds using a 4-line long function and an import (numpy).\n\nFor 10 million'th number, it took 5.63 seconds to compute the number.\n\nNow I can do that in Python because it's fast, and it's good at numbers. It took this code to calculate the n'th Fibonacci number:\n\n    import time\n    import sys\n    import numpy as np\n    import math\n\n\n    def fibfast(n):\n        base = np.matrix([[1, 1], [1, 0]], dtype=object)\n        result = np.linalg.matrix_power(base, n)\n        return result[0, 0]\n\n    if __name__ == \"__main__\":\n        n = int(sys.argv[1])\n        start = time.monotonic()\n        val = fibfast(n)\n        end = time.monotonic()\n        print(f\"{end - start:.4f} seconds\")\n        print(f\"{math.ceil(math.log10(val))} digits\")\n\n\nShow me how you can get comparable results in C#.\n\nAnd it's not just this damn Fibonacci problem. In many places, Python is just really fast, with its infrastructure of really fast stuff.\n\nWhat's slow is the Python code itself, and I have only 3 lines in a function that takes up 99% of the time.", "id": "hng9te6", "owner_tier": 0.7, "score": 0.05763397370070778}, {"content": "\"Too slow\" is of course relative and it may not be too slow for you - but \"slow\" is absolute and in the grand scheme of things Python is indeed terribly slow.\n\nI've also very succesfully written applications that people probably wouldn't think would work out in Python (Like real time image processing) - but I also ran into the case where Python was too slow (and speeding it up a bigger hassle than just going to a faster language) (happens quite often in numerical simulations like Monte Carlo simulations). It's usually quite easy to write simple code in a fast language that outperforms well written and potentially complicated Python.\n\nAnd it should also be said that a lot of the fancier dynamic stuff in python kinda pushes you away from high performance.\n\n[deleted]\n\n> For nearly all analytics, AI, ML, scientific and similar workloads, no.\n\nI'd argue that *pure* Python *is* too slow for these workloads.  Thankfully, the intensive portions are implemented in C/Fortran and then made accessible to Python.  So, from an \"end-user\" standpoint they're using Python, but in reality they're actually using a lot of C if they're using Cython.  \n\nAnd I do think this is an important distinction to make, as there are some commercial codes with Jython APIs.  Because Jython is built on Java, it is incompatible with NumPy and SciPy, which are written in C/C++/Fortran.  You wouldn't want to write a pure-Python linear system solver in Jython rather than use one of the solvers provided by SciPy, because it would be too slow.", "id": "hnfrs11", "owner_tier": 0.7, "score": 0.03741152678463094}, {"content": "When you code in python, you need to be aware that plain python is very slow & shouldn't be used for large loops.  Instead you should either use libraries that have code written in other languages, or if that isn't possible, then use something like numba to JIT compile the code into much faster code.\n\nFor example, if you were to write code to fill a 2000x2000 python list of lists with random integers & then sum the values, this would be very slow:\n\n    def fill_data(data: list[list[int]]):\n        for i in range(0,2000):\n            data_row = []\n            for j in range(0,2000):\n                data_row.append(random.randint(0,1000))\n            data.append(data_row)\n\n    def sum_data(data: list[list[int]]):\n        total = 0\n        for i in range(0,2000):\n            #total = total + sum(data[i])\n            for j in range(0,2000):\n                total = total + data[i][j]\n        return total\n\n    data1 = []\n    t0 = time()\n    fill_data(data1)\n    t1 = time()\n    total = sum_data(data1)\n    t2 = time()\n    print(total)\n    print(f'fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\nfill:22.5902s, sum:2.57115s\n\nIf you were to do the same thing using numpy and the built-in randint and sum functions:\n\n    t0 = time()\n    data2 = numpy.random.randint(0,1000,(2000,2000))\n    t1 = time()\n    total = data2.sum()\n    t2 = time()\n    print(total)\n    print(f'fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\nfill:0.0209434s, sum:0.00199556s\n\nThat's over 1000x speedup (from 22.59 seconds to 0.02 seconds) to populate the 2000x2000 with random data and over 1200x speedup (from 2.57 seconds to 0.002 seconds) to sum all the data.\n\nIf there is no function that does exactly what you want and you want to write your code in python, then one way to make it faster is to use numba. It can JIT compile your code into a form that runs much closer to native code speed:\n\n    @numba.jit(nopython = True)\n    def fill_data2_jit(data):\n        for i in range(0,2000):\n            for j in range(0,2000):\n                data[i,j] = random.randint(0,1000)\n\n    @numba.jit(nopython = True)\n    def sum_data2_jit(data):\n        sum = 0\n        for i in range(0,2000):\n            for j in range(0,2000):\n                sum = sum + data[i,j]\n        return sum\n\n    t0 = time()\n    data2 = numpy.zeros((2000,2000),dtype=numpy.int32)\n    fill_data2_jit(data2)\n    t1 = time()\n    total = sum_data2_jit(data2)\n    t2 = time()\n    print(f'total: {total}')\n    print(f'[numba] fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\n[numba(first time)] fill:0.707387s, sum:0.174748s\n\n[numba(all other times)] fill:0.0239353s, sum:0.000998497s\n\nThe first time it runs it needs to compile the code, so it takes much longer, but all subsequent runs are very fast.\n\nIf you use numpy arrays but don't make use of the built-in functions (or numba), it appears to be no faster than native python code with lists:\n\n    def fill_data2(data):\n        for i in range(0,2000):\n            for j in range(0,2000):\n                data[i,j] = random.randint(0,1000)\n\n    def sum_data2(data):\n        sum = 0\n        for i in range(0,2000):\n            for j in range(0,2000):\n                sum = sum + data[i,j]\n        return sum\n\n    t0 = time()\n    data2 = numpy.zeros((2000,2000),dtype=numpy.int32)\n    fill_data2(data2)\n    t1 = time()\n    total = sum_data2(data2)\n    t2 = time()\n    print(f'total: {total}')\n    print(f'[numpy (no builtin)] fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\n[python] fill:20.5389s, sum:2.26132s\n\nwow thats cool, didn't know about numba", "id": "hngpzfh", "owner_tier": 0.3, "score": 0.010111223447927198}, {"content": "Generally the models you're writing for data science are using modules that are written in C with an API in Python, so these won't be affected by Python's speed. \nFor general purpose Python really isn't all that bad at all - you're more likely to run into IO problems, especially with web APIs. \nAs you begin to scale in say a larger application the speed can become noticeable. But it really depends on what you're doing! Choose the right tool for the right job.", "id": "hnfrkgg", "owner_tier": 0.3, "score": 0.007077856410515672}, {"content": "Bad python > bad C\n\nIt doesn't matter if python is a little slower, it's still more reliable to write code in and faster to write code in than C. Speed of development trumps speed of performance for almost all applications. I'm an engineering manager at a robotics company and literally none of our server software needs to be fast. The robot software is real time, the server software just needs to be well written. \n\nAnd also people ask about speed but we are talking just CPU speed and not a lot slower but like 10%-20% more to get the same task done, so slow in terms of usage rather than slow in terms of time. I can still write my server software to answer complex queries in the same amount of time.\n\nAny big loop will give you factor of 100 slowdown compared to any compiled language tho.\n\nBut that's where anyone who is experienced in python at all will say, do those in a thread or have a thread pool to do those, or if it's even bigger fan them out to other processes for the work, then it doesn't even have to be in Python for that specific task. For instance ZeroMQ has push/pull behaviour which is designed for fanning out tasks to multiple workers, then have the workers send the results back. There are multiple better approaches than just doing something in a loop and stopping there if you REALLY want performance\n\nWhich is not necessarily relevant.\n\nOn my laptop it takes about 35 ms to start-up a python program, but then there's little difference between a simple loop of 100 iterations vs one of 100,000 iterations.\n\nSometimes performance matters - like when I'm processing tens of billions of records a day - and just want to keep it economical.  But I'd guess that 95% of what typical programmers write on a daily basis doesn't need the absolute fastest performance.\n\nThis isn't a trivial approach, I don't think it is really intended for an industrial working on a project, and more for enterprise who need to somehow scale their python codebase. I only say that limits of performance can be reached very easily in some real world tasks, which isn't really the case with compiled languages.\n\n> This isn't a trivial approach\n\nErr have you seen the code for ZeroMQ usage? It's 100 lines of copy paste code for the most part. \n\nThe overall point I'd make is there are 100 ways around Python's slowness, there aren't any ways around C/C++...etc when it comes to ease of development. You can implement your performance critical piece in C if you want and just wait for the results in Python, there are a million ways to fix this but ease of use trumps everything. \n\nI've had this argument 1000 times since I started using Python. If you have enough Python devs you don't need NodeJS (for the most part), you don't need C/C++ (for the most part) you just need Python and some glue and you will be able to do everything. It doesn't address speed well but as a general purpose language it's good enough for everything and everywhere but the very most perf sensitive applications. I wouldn't be timing rocket stages with it but I'd very much say it could be used for the basic rocket internals other than control for example.", "id": "hnfymkx", "owner_tier": 0.9, "score": 0.009100101102123356}, {"content": "I am working with Python for the last 4 years and I am building physical simulations and optimizations which we expose as web APIs. Python is wonderful for this:\n\n* FastAPI and Flask are terrific web frameworks,\n* nothing comes even close to Numpy and SciPy for linear algebra in other languages (except Matlab and Mathematica, but they are a non-starter for general development)\n* Python is beautiful, productive, and easy to read and maintain.\n\nIf you are talking about the web and API things, then I've found that the performance hit you get from using Python is largely irrelevant. You have to get to be serving millions of requests per day to be affected by the comparative slowness of Python.\n\nIf you are talking about compute-heavy things, Numpy does a very good job at abstracting most of the heavy loads to other languages. If you then find then that something takes a long time, then you have an option of Cythonizing parts of your code. Or you can use Numba to compile pieces of code just-in-time. Or you can use an alternative compiler like PyPy to try to speed it up that way. But there are limitations to how far you can get and it's a pain because you have to navigate the minefield of incompatibilities between compilers, libraries and Python versions.\n\nThe solution that I am currently trying out is to use Rust to write the custom compute-heavy stuff and have Python do the web API and glue everything together. I'm finding that Rust is MUCH faster than Python+Numpy. Like 10x to 100x easily. It actually allows me to do things that I would not even be thinking about if I was using just pure Python. But you need quite more lines of code to do the same thing in Rust than you would need in Python. So it's about finding the optimum between how fast something HAS to run, how much time we have to develop it and how often it's going to be run.", "id": "hnggvhb", "owner_tier": 0.3, "score": 0.002022244681496461}, {"content": "> I haven't developed very large-scale apps\n\nThis is probably why. Try writing an API that's going to receive 1 million QPS and has a 100 ms latency budget. You'll quickly find Python isn't a good choice for this problem.", "id": "hni4x05", "owner_tier": 0.9, "score": 0.0030333670273003034}, {"content": "Most of ML libraries are written in low level languages like C, C++ etc. So in your case you use python to control flow, where most heavy calculations are done in heavy optimized low level code. \n\nSo I do not think that you could meet a scenario where using python may significantly impact on performance.", "id": "hng9o58", "owner_tier": 0.3, "score": 0.0010111223356926188}, {"content": "Depends on your metric.  It's fast to develop in and that's generally the most expensive time.  When realtime performance is required, you don't generally reach for Python in the first place (rightfully so).\n\nAs such, you don't tend to see issues.", "id": "hng3m23", "owner_tier": 0.5, "score": -1.0111223396587755e-11}, {"content": "What I often need is a language which makes it simple to deploy my tools. This is the main reason why I'm more into Go lately. It's not the slowness of Python. Although Python is not the fastest language under the sun, it's fast enough for most use cases. And there are lots of optimized C libs for Python, especially when it comes to ML (so I don't wonder that the OP doesn't suffer too much here).\n\nWhat I'd like to see in Python is a flawless possibility to create self-containing executables. I know there're 3rd party libs and tools providing that feature but either there're not open source or they are  behind of the latest Python releases.\n\nSoftware deployment is one of the bigger issues with Python, not speed.", "id": "hnhrab0", "owner_tier": 0.3, "score": -1.0111223396587755e-11}, {"content": "Do you find it to be too slow?\n\nIf you do find it to be too slow, then it's too slow.\n\nIf you don't find it to be too slow, then it's not too slow.", "id": "hnhwnpn", "owner_tier": 0.7, "score": -1.0111223396587755e-11}], "link": "https://www.reddit.com/r/Python/comments/ra2aqh/is_python_really_too_slow/", "question": {"content": "I work as ML Engineer and have been using Python for the last 2.5 years. I think I am proficient enough about language, but there are well-known discussions in the community which still doesn't fully make sense for me - such as Python being slow.\n\nI have developed dozens of models, wrote hundreds of APIs and developed probably a dozen back-ends using Python, but never felt like Python is slow for my goal. I get that even 1 microsecond latency can make a huge difference in massive or time-critical apps, but for most of the applications we are developing, these kind of performance issues goes unnoticed.\n\nI understand why and how Python is slow in CS level, but I really have never seen a real-life disadvantage of it. This might be because of 2 reasons: 1) I haven't developed very large-scale apps 2) My experience in faster languages such as Java and C# is very limited. \n\nTherefore I would like to know if any of you have encountered performance-related issue in your experience.", "id": "ra2aqh", "title": "Is Python really 'too slow'?", "traffic_rate": 207.942496260595}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "Because:\n\n1. It executes fast enough for most things.\n\n2. It\u2019s fast to write.\n\n3. Spending 3-5x the development time in order to shave off 5-10% from execution time often isn\u2019t worth the time investment. When it is actually worth the effort, don\u2019t use Python for task.\n\nThe performance gain with other languages is actually more in the realm of 5-20x. But it doesn't really matter if my program takes 400ms to execute in Python or 20ms in C - it's near-instant to me either way.\n\n>1. Spending 3-5x the development time in order to shave off 5-10% from execution time\n\nYou're not wrong, but you're understating how slow Python is\n\n[Semi relevant XKCD](https://xkcd.com/1205/) taking something written in python and redoing it in C won't fit the boxes listed in the majority of cases\n\nnow tell that to the dev I had in my team that wanted to write everything in elixir because whatsapp or whatever could handle 200k requests per minute with it\n\neven though our application would have 100 requests per minute if all our users decided to use it at the same time\n\nIt also has a pretty decent startup time compared to languages and runtimes which execute the actual computation faster, which is sometimes desirable\n\n&#x200B;\n\nFor many applications it is more like 1ms in c vs 20 ms in python and 100-500 ms in I/O.\n\nWhen the python part takes seconds to run and the tensor multiplications in c take hours, days, weeks or months, we're talking about a performance gain of less than a small fraction of a percent.\n\nI am not sure thats true so for an hpc profiler I am doing I use python and I may need to switch out because waiting a minute instead of 3 seconds is a big deal.\n\nHowever...\n\nJava is in most benchmarks 10-20x faster than Python, yet most apps written in Java is so painfully slow and memory hungry...\n\nThis 100%! People are underestimating how slow Python really is.", "id": "kjth7bb", "owner_tier": 0.7, "score": 0.9999999999875467}, {"content": "It's because computers are really, really, really fast. With Python, they're only really fast, but that's still OK for many tasks.\n\n300 ms of extra electrical costs is usually much cheaper than the extra hours of paying a programmer.\u00a0\n\nOf the several correct answers, this is my favorite.\n\nExactly, it was a different story when Python came out 30 years ago. Modern computers can handle all the layers of abstraction much better\n\nbest comment. 300ms of code execution vs extra 15h to code the same thing.\n\nOne has to choose\n\nyour dismissing the execution cost over time and at scale. There are times when it matters, but for most of pythons use cases it doesn't.\n\nOnce you go beyond a certain scale the equation changes and its more efficient to write services in C++. the dev time is a drop in the ocean compared to operating costs.\n\nTBH the dev time in other languages isn't that bad IF you already know exactly what needs to be done. In 90% of the projects, you don't.", "id": "kjtfkld", "owner_tier": 0.7, "score": 0.3985056039726027}, {"content": "Yes, and the other point to make is that time spent programming and debugging is just as real as time spent waiting for software to run and there are plenty of applications where sacrificing a little run time speed to spend less time programming and optimizing for speed is the overall most efficient approach, because not everything is a video game.\n\nAlso development time is active, while execution time can be passive", "id": "kjte7iq", "owner_tier": 0.3, "score": 0.09339975092154422}, {"content": "I love Python, but let\u2019s be real, Python doesn\u2019t get used in areas where performance matters.\n\n- You don\u2019t see AAA games in Python.\n- You don\u2019t see fast frequency trading in Python.\n- You don\u2019t see real time embedded in Python.\n\nI work in a 5kk LOC python codebase and it is starting to show. Our parsers ever so slow, our AWS bill ever so high. We\u2019re moving from Python to other languages but it will take time.\n\nAnd yet you do see it used widely in AI/ML/scientific computing where those are literally about doing massive amounts of computation.\n\nBecause the slow bits are not being done in Python itself.\n\n> You don\u2019t see real time embedded in Python.\n\nMicroPython's actually pretty decent - and it running on an ESP32 is faster than C running on an ATMega / Arduino board, which nobody ever said was too slow to do decent things with...\n\nDumb question: what number is 5kk? Is that 5*1000\\*1000=5,000,000?\n\nThe cloud bill is so fucking true... When using Python for web development usually it works because on cloud it's just a matter of scaling up and down, but when you start getting high traffic your bill start increasing real quickly.\n\nIf you look after benchmarks (like TechEmpower) one, an API + db (single query) scenario you get 70k req/s with FastAPI while [ASP.NET](https://ASP.NET) Core is handling around 300k+ req/s, that's 4x more requests with the same machine.\n\n> Because the slow bits are not being done in Python itself.\n\nPython is just a glue language. My argument still stands.\n\n> MicroPython's actually pretty decent\n\nMicroPython has no use in **real time embedded** mostly because it inherits the garbage collector from Python. Running microPython without the garbage collector in an embedded device with an event loop is asking for memory leak and segmetation.\n\n> running on an ESP32 is faster than C running on an ATMega / Arduino board\n\nI'm not sure where you got your benchmark from, but regardless, C will **always** be faster than micropython on the same board.\n\n> nobody ever said was too slow to do decent things with... \n\nMost serious applications can't use Python because it doesn't pass safety standards. Sure, I'd love to do a quick elevator control board in micropython, but it won't pass any safety checks because of potential issues at runtime.\n\npython in AI is just a wrapper around C/C++ code or code that runs on your GPU.\n\n> 5kk\n\n1kk = 1M, yes.\n\nIs 252 million requests per hour not enough? Even if you have 252 requests per user, that's 1 million users per hour.\n\n> Python is just a glue language. My argument still stands.\n\nYes, and? Glueing libraries or APIs together is like 90% of programming these days...\n\n> I'm not sure where you got your benchmark from, but regardless, C will always be faster than micropython on the same board.\n\nI didn't say on the same hardware. A lot of embedded stuff is using PIC/AVR cores and the people who program them (in C/assembly) say that there's no way Micropython can be performant enough. But on the hardware it's typically used on, which costs about the same per chip as well, it's faster than a PIC/AVR running C.\n\n> MicroPython has no use in real time embedded mostly because it inherits the garbage collector from Python.\n\nI've looked into this - on the bare metal ports, hardware interrupts are not blocked by garbage collection.\n\nThe upper bound for the GC can be calculated, so you can schedule it when there's time.\n\nIn the critical timing sections, you lock the GC (provided you don't allocate any new memory) and unlock when leaving. Or set everything up, lock the GC, then never unlock it.\n\n> Sure, I'd love to do a quick elevator control board in micropython, but it won't pass any safety checks because of potential issues at runtime.\n\nMy experience with this is that it instead gets written in C, and ends up with memory corruption issues that get blamed on hardware... but it gets the boxes ticked because hey, we didn't use Python...\n\nyou could argue all of python is just a wrapper around c code\n\nWhat do you mean? I'm not saying Python can't handle API requests, I'm saying it's not efficient on that - and that translates to expensive bills...\n\nWe live in the era of cloud and distributed systems, we can scale a service up to dozens and hundreds of instances, of course a Python API will support millions of requests, the runtime here doesn't even matter - the question is: how much would you benefit from moving it from Python to .NET (or Java, Rust, etc)? Cost wise, probably A LOT. But of course, not everything is about money and maybe for you use case Python is the best lang to use.\n\n\\* Based on TechEmpower benchmarks: [https://www.techempower.com/benchmarks/#hw=ph&test=db&section=data-r22](https://www.techempower.com/benchmarks/#hw=ph&test=db&section=data-r22)", "id": "kjus29t", "owner_tier": 0.3, "score": 0.08717310085927772}, {"content": "Its because it is a good beginners language and  its syntax looks like a pseudocode and you can do much more with fewer lines of code.\n\nEven programmers from other languages like Java prefer to use Python for coding interviews as it is less verbose.\n\nIt has good ecosystem of libraries which again help to do more with less. You will find library or package for most of the use case you could imagine .\n\nResearches use it as many scientific computation libraries are available in python which are written in C for faster execution.\n\nIts versatile and can be used for variety applications like:\n\n* IoT and  Automation using Raspberry Pie\n* OS level automation for file handling, parsing documents.\n* Web scrapping , browser Automation, web crawling with Selenium, B4S,  scrapy\n* Full stack web development with Django\n* Data Analyses with Pandas and Numpy with jupyter notebook for experimentation\n* Rest API framework like FastApi and Flask for backend\n* Machine Learning, AI, NLP.  Deep learning libraries  pytorch, TensorFlow are used in generative AI.\n\nFor many of above tasks there is no alternatives in other programming languages.\n\nAnd where speed is required it can be achieved by using C and having python wrappers. For other applications like web and backend speed is not a big issue as network and database have more bottlenecks.\n\nDon't forget MicroPython and Circuit Python on the current crop of microcontrollers, RP2040s like the Pi Pico or ESP-32 based chips let you do many prototypes in python complete with an REPL!", "id": "kjtevfp", "owner_tier": 0.3, "score": 0.042341220410958905}, {"content": "You're very much preaching to the converted here. It's not wrong to say the Python is slow compared to something like C. That's _very_ different to saying you shouldn't use Python because it's slow: in many cases it's an excellent choice.", "id": "kjtiqi4", "owner_tier": 0.5, "score": 0.029887920286425902}, {"content": "Python is simply a tool in the toolbox - sometimes Python makes sense, sometimes the speed of  C++ is needed.\n\nor C, or rust, or assembler, or...\n\nMet a climate scientist recently that was crunching numbers with Fortran for the speed.\n\nNow I'm imagining a bunch of climate coders sitting around joking about how slow C is.", "id": "kjtm8cd", "owner_tier": 0.5, "score": 0.052303860510585304}, {"content": "This is a really bad take and also a bad faith argument.\n\n-If i'm building a go binary simply calling C programs would you call this a C program or a Go program?\n\n-If i'm calling FFMPEG from python does python also becomes amazing at video processing?\n\nNo. \nPython is a slow language through and through and if it's \"secret sauce is calling \"super-optimizted C lib\" then what happens if tomorrow people writes the same C binding in JS, Ruby, Go or any \"user friendly\" language ?\nWill python still be used ? probably yes because this is not the real value of python.\n\n---\n\nYou missed the real value of python which is : \nit's a beginner friendly scripting language with a strong community and a plethora of libs for anything and everything.\n\nIt's amazing for prototyping and scripting and it has really good support for things like HTTP thanks to libs like requests.\nIt's really pleasant to discover an API using python because you can dynamically adjust your payload, read response, jump with IPDB etc...\n\n---\n\nYou are also not mentioning why being slow is an issue beyond user's perspective.\n\nCompute is expensive, when python is recorded being 10 or 100x slower than Rust, Go or C++ this means that my compute bill in the cloud also increases when running workloads slower than with other languages.\nThis might not being of your concern but for startups and small companies this can mean a lot.\n\nBeing single threaded is also expensive. i can't scale vertically as much as I'd like. with languages like Go if i have a 6 CPU machine it's not all going to waste.\n\nNot being strongly type is a good feature for scripted language but the drawback is that you can make really unsafe code. i've worked on python for one of the biggest bank in the world and i'm glad it was not for stuff related with bank accounts.\n\n---\n\nI like python. it's a cool language. but it has really limited cases where it's really good and a billion of them where it's absolutely terrible (let's not even talk about package management).\n\nFacts\n\n> Not being strongly type is a good feature for scripted language but the drawback is that you can make really unsafe code.\n\nPython is a strongly typed and dynamically typed. There's no implicit conversion, so it's not weakly typed. What python isn't is a statically typed language but type hints exist and can be enforced with mypy\n\nMost of the rest makes sense, just wanted to correct this one mistake\n\nYou can use all cpus with joblib: [https://joblib.readthedocs.io/en/stable/](https://joblib.readthedocs.io/en/stable/)\n\nUsing all your cores is not hard at all in Python.\n\ngood point, i meant statically typed.\n\nusing them efficiently is. \n\ni'm gonna use the most obvious contender for python in a really common usecase : the webserver space (flask, django etc): Golang.\n\nGolang requires only a fraction of the CPU consumption and automatically scales across cores as needed.\n\nAlso not hard to use all your memory and max out those cores.\n\ndefinitely recommend you set up mypy then with strict rules on type hints. It'll give you 90% of the benefits of static typing\n\nUntil you import some libraries with no type hints\n\ni kinda stopped using python except for one shot scripts where types are much less important.\n\nlast time i used it was for some automation with OpenWeather and to explore an REST API in Ipython before implementing the client in Go.", "id": "kjtyj0q", "owner_tier": 0.1, "score": 0.0311332502988792}, {"content": "Because most of the time, CPU power is cheaper than engineers.\n\nRewriting the right things in languages that waste less resources than Python can still save the monthly hosting costs of multiple engineer salaries which can make it economically sound to rewrite code some times.\n\nYou don't necessarily even have to have a particularly large systems, serving something like a few hundred of thousands of users per day can some times get pretty expensive with Python.\n\nAt that point it is not so much about software developers being more expensive as it is about prioritizing use of the developers time. Often fixing business logic bugs, directly improving business metrics or working on visionary features is most important but some times operational matters as reducing monthly running cost might also be among the top priorities.", "id": "kjtfwbr", "owner_tier": 0.7, "score": 0.0199252801867995}, {"content": "It's not C. It's native in general. Large parts of numpy and SciPy are written in C++. Pydantic v2 is written in Rust.\n\nIt's pydantic, okay?\n\nThese details matter.\n\n1. Numpy it's write in C with BLAS, OpenBlas & LAPACK.\n2. Scipy it's Made with Cython & FORTRAN\n\nWhich part of numpy is written in C++ again? And which part of scipy is written in C++ again?\n\nArgh, autocorrect strikes again. Thanks for the correction.\n\nAlso: nice pun.\n\n[https://github.com/numpy/numpy/tree/main/numpy](https://github.com/numpy/numpy/tree/main/numpy)\n\n[https://github.com/scipy/scipy/tree/main/scipy](https://github.com/scipy/scipy/tree/main/scipy)\n\nYou might want to check that again, that is all C, not C++. There is a very small amount that is mostly interfacing and to save some code during code generation so that you don\u2019t have to implement sparse multiplication for every numpy-float data type for example, hence why you at best see templates and nothing else. and the rest is essentially C and Fortran. That\u2019s why I was asking. I know the repos. Saying \u201ethey are written in C++\u201c is by all means an overstatement.", "id": "kjtj25z", "owner_tier": 0.7, "score": 0.0498132004856787}, {"content": "Good job, OP! You sure destroyed that straw man!", "id": "kjtpc8s", "owner_tier": 0.5, "score": 0.0087173100747198}, {"content": "Python advantage is ergonomics. C is my favourite language and I am good enough with it to write pretty performant code with SIMD and other fancy techniques. But why bother to use C to parse my json configuration files? Why should I get crazy to make plots and visualise the results? The 2 can be combined with almost 0 effort! That\u2019s the strength of Python. It\u2019s slow if you use it for the wrong task.\n\nSame boat.  Primarily C++ developer but a big Python fan.  I use Python just about any time I can because it's so much quicker and easier to get up and running.  Less boilerplate, even on levels that are so mundane almost no one ever calls them out like not needing \"#include <map>\" or a type declaration before stuffing stuff into a dict.  One less command line to run between compiling and then executing the generated output.  So many of these are just tiny, seemingly inconsequential things, but they do add up to better productivity if the raw speed is not something you need, which it very often is not.\n\n>But why bother to use C to parse my json configuration files?\n\nHere\u2019s the thing about performance in Python that I haven\u2019t seen mentioned nearly enough in this comment section:  it\u2019s very easy to get the raw speed of C for something like parsing JSON in Python.  Python has a fantastic array of FFI tools for calling C/C++/etc libraries.  Most popular libraries already have Python bindings written for them, and if they don\u2019t, you can generate them with [cffi](https://cffi.readthedocs.io/en/stable/) or use [ctypes](https://docs.python.org/3/library/ctypes.html) or [Cython](https://cython.org).  \n\nUsing the JSON example, there\u2019s a Python wrapper around the very fast [simdjson](https://github.com/simdjson/simdjson) library called [cysimdjson](https://github.com/TeskaLabs/cysimdjson). \n\nCython is another great tool for C/Python integration, especially with type hints becoming common in Python 3.  Cython can translate type-hinted Python into C and then compile it, which can result in huge speedups.  There are also other Python compilation projects like Numba, Nuikta, and codon.  Alternatively, using PyPy instead of CPython can often give a significant speed boost via JIT.  \n\nMost heavy lifting in Python in numerical/scientific computing and data analysis fields is already handled by C/C++/FORTRAN libraries with Python interfaces (numpy, scipy, ML/AI libraries, etc.)\n\nCython generates faster code than Nuitka in my experience. Anyway, I agree with you.", "id": "kjtlbp3", "owner_tier": 0.5, "score": 0.0174346201618929}, {"content": "https://instagram-engineering.com/dismissing-python-garbage-collection-at-instagram-4dca40b29172\n\nThis is an interesting one to read. TL;DR is that instagram basically found totally deactivating the garbage collector in CPython and instead just restarting workers when they ran out of memory made their systems run up to 10% more efficiently.\n\nIn a big company, that can be the difference of thousands of dollars per month.\n\n>In a big company, that can be the difference of thousands of dollars per month.\n\nI would imagine 10% would equate to more than thousands per month. saving a few % in one of those companies usually justifies an entire teams salarys", "id": "kju3heu", "owner_tier": 0.7, "score": 0.006226650049813201}, {"content": "The speed isn't much of an issue but the lack of real concurrency is a big limitation.\n\nPython concurrency is super easy for many cases: [https://joblib.readthedocs.io/en/stable/](https://joblib.readthedocs.io/en/stable/)\n\nMultiprocessing is an easy way to use concurrency in Python.\n\nThat's not concurrency either.\n\nThat's specifically not concurrency.\n\nUsing multiple processes instead of threads comes with many limitations and overheads. This is not specific to Python, but the fact that concurrency cannot be achieved using threads is. In languages where you can use both (basically every other major language), people very rarely use processes instead of threads.\n\nWhat is it?", "id": "kjtsnr7", "owner_tier": 0.7, "score": 0.0024906600124533}, {"content": "I think it's fair to say that python is slow. for many use cases it doesn't matter as you say, and for some it does and python just bridges out to C. it will be used in those cases, and even if it outgrows its purpose on those it's fine, you can optimise. \n\n&#x200B;\n\nI think the biggest thing for python developers (those developing python, not those writing code in python) is that go is similar in complexity (slightly more complex), but has massive speed benefits when it comes to web services. \n\nSo improving speed makes total sense to keep python in that niche.", "id": "kjtuxdw", "owner_tier": 0.5, "score": 0.0024906600124533}, {"content": "The right tool for the job.\n\nI will never write a 3D driver in Python. It is way to slow and you need speed.\n\nWriting a web-app on the other hand. Most of you latency is the network and DB access. The few milliseconds you will save using a compiled language vs your dev time is not worth it.", "id": "kjtkk6z", "owner_tier": 0.5, "score": 0.00124533}, {"content": "I work on analytic software written in Python that handles a fair amount of data and it runs just fine. I would rather parallelize it to scale up performance than re-write everything in Rust or C/C++.\n\nDoesn't always help. I scaled up an MCTS algo on Python to 90 core server and 340GB ram, and then beat it easily with a Raspberry Pi 4 in Golang.\n\nPython's Multiprocessing has a lot of overhead, and doesn't scale linearly.\n\nNot every problem is embarrassingly parallel, some are even inherently serial. I generally agree with you, though in regards to parallelization. Often times when a program gets complex enough to warrant more than straightforward parallelism I'd still want to rewrite it in Rust, Elixir etc. because getting the correct results is even more important than speed.", "id": "kjtl2zu", "owner_tier": 0.3, "score": 0.0037359900249066005}, {"content": "Realtime audio in Python is unbearably slow, making it not worth it at all in many cases", "id": "kjtr0vl", "owner_tier": 0.3, "score": 0.00124533}, {"content": "I recently did a fairly big and complex GUI project in python using nothing but tkinter where I had more than a dozen loops running parallely in the background with varying time interval from 100-250 ms, of course each of those parallel loops had several lines of logical code which was barely taking 0.7-5 ms to complete before it repeats itself on set conditions, I was really taken aback with such a raw performance from slower python !!!! I only felt slowness when I was calling some API over the internet or dumping huge chunks of data for processing, I haven't got to the point where I am coding some really complex AI/ML shit but for regular folks like me python is more than powerful to run anything you throw at it without breaking a sweat.", "id": "kjtfkg8", "owner_tier": 0.3, "score": -1.2453300048848432e-11}], "link": "https://www.reddit.com/r/Python/comments/1acd5j0/why_pythons_slowness_is_not_slowing_anyone_down/", "question": {"content": "Ever wondered why, despite all the grumbles about Python being slow, it\u2019s still everywhere? Especially when folks dive into coding, one of the first things you hear is \u201cPython\u2019s slow.\u201d But, if it\u2019s such a snail, why do so many people use it for all sorts of heavy-duty stuff?\n\nHere\u2019s the deal: Yes, Python isn\u2019t the Usain Bolt of programming languages when it comes to raw speed. We\u2019re talking basic stuff like loops and if statements. But let\u2019s be real, how often are we in a situation where the speed of a for-loop is the make-or-break of our project?\n\nThe secret sauce of Python isn\u2019t in beating speed records. It\u2019s in its knack for playing nice with super-optimized C libraries. These libraries are the muscle doing the heavy lifting, while Python\u2019s more like the friendly coach guiding the process. So, your Python code might take a tiny bit longer to run a loop, but when it calls on these C libraries to do the real work, they zip through tasks at lightning speed.\n\nSo, next time you hear someone knocking Python for being slow, maybe toss this thought their way. Python\u2019s not just about the speed of typing out code; it\u2019s about the overall speed and ease of getting stuff done, thanks to all those optimized libraries it wraps around so neatly.", "id": "1acd5j0", "title": "Why Python\u2019s \u201cslowness\u201d is not slowing anyone down", "traffic_rate": 207.942496260595}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "The second example with map is not correct at all: the `map()` function does **not** return a list since python 3.\n\nMeaning that the \"list\" is not indexable (`newlist\\[0\\]`), nor iterable more than once\n\nYou get a generator.\nComprehension might be better or you can do:\nlist(map(\n\n`[*map(...)]` is actually faster than `list(map(...))` in many cases!\n\nWait, really? Oh wow, do you know why that is?\n\nHere's an example:\n\n    C:\\Windows\\System32\n    \u276f python -m timeit \"[*map(str.upper, 'hello world')]\"\n    500000 loops, best of 5: 583 nsec per loop\n    \n    C:\\Windows\\System32\n    \u276f python -m timeit \"list(map(str.upper, 'hello world'))\"\n    500000 loops, best of 5: 665 nsec per loop\n\n\nHonestly, I'm out of my league when trying to interpret bytecode, but here's the `dis`. Maybe... fewer instructions, fewer global lookups?\n\n\n    In [1]: import dis\n    \n    In [2]: def unpack():\n       ...:     return [*map(str.upper, \"hello world\")]\n       ...:\n    \n    In [3]: def cast():\n       ...:     return list(map(str.upper, \"hello world\"))\n       ...:\n    \n    In [4]: dis.dis(unpack)\n      2           0 LOAD_GLOBAL              0 (map)\n                  2 LOAD_GLOBAL              1 (str)\n                  4 LOAD_ATTR                2 (upper)\n                  6 LOAD_CONST               1 ('hello world')\n                  8 CALL_FUNCTION            2\n                 10 BUILD_LIST_UNPACK        1\n                 12 RETURN_VALUE\n    \n    In [5]: dis.dis(cast)\n      2           0 LOAD_GLOBAL              0 (list)\n                  2 LOAD_GLOBAL              1 (map)\n                  4 LOAD_GLOBAL              2 (str)\n                  6 LOAD_ATTR                3 (upper)\n                  8 LOAD_CONST               1 ('hello world')\n                 10 CALL_FUNCTION            2\n                 12 CALL_FUNCTION            1\n                 14 RETURN_VALUE\n\nYeah, pretty much. There's one more layer of indirection with list() - it's just a regular function call, while the unpack is baked into the language semantics. \n\nCalling a function in Python is just doing things to a variable, and variables are effectively just keys in a dictionary - builtins are simply variables that are pre-loaded for you when you start the interpreter.\n\nSo, to run list(), Python does a dict lookup for the key 'list' and returns the actual code to run as the value, then runs it. A dict lookup requires hashing+modulo+data structure bookkeeping, so it has some extra overhead, while the unpack goes straight to the put-stuff-in-a-list logic.\n\nCALL_FUNCTION is nearly always the most expensive operation.  Remove as many of those as you can for improved performance on tight loops...", "id": "gvb1fy8", "owner_tier": 0.1, "score": 0.9999999999230769}, {"content": "While generally correct, point 6 is a bit of an anti pattern in that it encourages namespace pollution. 5 is true, you will get better results yet if you predefined the size of the list (or numpy array) in advance, which is the real tip. 4 can be better worded to something like \u201cmove calculations outside of for loops if possible\u201d, both examples use a single for loop. Item 2 needs a reword too, because list.append is no less a built in function than map.  Also, for 1, I think OP meant dictionary and not directory as a data structure. \n\nWith the exception of the 1, a better written 2, better written 4, better written 5, these are all micro optimizations. Hell, I\u2019d argue with 8 and say just use fstrings. \n\nWe should all agree optimization is also often done in the second pass of writing code. So profiling is important, understanding a refactor process (like including tests) and don\u2019t sacrifice readability for a few extra ms. If speed is your primary concern at that level then invoke the C, C++ or even the FORTRAN libraries directly, tensor flow is a good example of that architecturally.\n\nAbsolutely, point 6 is horrible advice. It encourages wildcard imports which are wayyyy slower than a dot lookup.\n\nI want to see how much time #5 can really save because at the end of the day it looks more like a fancy 1 liner than an optimization for performance. \n\nI agree readability > a few ms everytime. :)\n\nHow can you predefine a list\u2019s size?\n\nWhy does it have to be a wildcard?  Can't you just import the functions that you intend to use?  I just tested this on my machine and it was about 20% faster not using the dot.\n\nNot only that, but different libraries often define same-named functions, or collide with standard library names.  I maintain a somewhat large project in Python, about 50,000 lines of code, and not too long ago I eliminated all wildcard imports and feel this was a big improvement.   \n\n\nPerhaps there could be some way to \"freeze\" an import, saying effectively, \"OK, no one is going to modify this import\" and it would allow the function calls to be made static.  I don't know much of anything about the internals of Python, but I would bet that this is an optimization that might make sense, or that something like it already exists.\n\nI don't like and don't use wildcard imports, but why would they be \"slow\"?\n\n\\#6 is not wildcard importing it's importing a module which only runs the module's  \\_\\_init\\_\\_.py. \n\n`from math import sqrt`\n\ncost more than\n\n`import math`\n\nI ran the code 3 times using:\n\nimport time\n\nstart_time = time.time()\n\ncode (except with a range of 1 -100,000) \n\nprint(\"%s seconds\" % (time.time() - start_time))\n\nThe traditional for loop (the first code) took an average of 0.0086552302042643 seconds\n\nThe suggested format took:\n0.0049918492635091 seconds.\n\nSo there is a speed improvement. But you'll need to be looping through a lot of data to really see it.\n\nSpeaking from personal experience, list comprehension isn\u2019t the speed up, but there are some fun itertools that can help, but the pre allocation has taken functions I had from seconds to milliseconds. So those were huge. Naive loops are inefficient because they\u2019re super generic.\n\n\u2018Preallocatedlist = [] * 1000\u2019\n\nThough to be fair, generators are a good alternative too, it depends on your use case. When Numpy gets involved you\u2019ll be doing preallocating a lot because there the vectorized operations are much more optimized.", "id": "gvbca6t", "owner_tier": 0.5, "score": 0.9076923076153847}, {"content": "While all these points are correct, the most important point is to avoid large Python loops at all - and with large I mean > 10,000 iterations in the innermost loop.\n\nMethods to archieve this:\n\n* Use vectorization or other techniques to move the looping from Python to a faster language, e.g. with Numpy, Pandas, etc.\n* Compile the most critical parts of your Python code, e.g. with Numba.\n\nIf your loops are your performance bottleneck, this could often give you speed-ups by a factor of 100.\n\nCython is also very good!\n\n> move the looping from Python to a faster language\n\nThat is the thing. You want fast Python - use other language. I am writing a regular Java code and getting a reasonably good performance just by using standard features. I am working with Python and need to think about carrying around monsters like pandas, conda... and something else to utilize multiple CPUs (dask?) :(\n\nThere is certainly something to be said for only using one language in a codebase. It might not be the best language at anything, but is OK.\n\nBut there's also something to be said for using multiple languages where each is used in the domain in which it shines. You add the complexity of an extra language, but also the code written in each language is natural, and people coding it feel like they're using the right tool for the job.\n\nOn a small project perhaps the overhead of multiple languages is excessive. \n\nOn anything else, the overhead of multiple languages is lost in the noise.\n\nWe'll put. I would also say that using numpy or pandas is not \"another language\"\n\nMaybe I have not explained myself clearly. \"each is used in the domain in which it shines\" - that is exactly my point. Performance is not Python's domain (I hope it is not for discussion). We are trying to streatch it to every domain. Pandas weren't designed for big data processing (as I understand its main purpose is matrix manipulation) - but it is still very convenient for small - medium size data manipulations. Now we have Dask which is handling big data. Pandas has no special optimization like DB indexing or partitioning as I understand it is inherited by Dask. \nThere is the point we cross: we add another nice feature to a tool, then another ... and at some point it turns into monster. It is not a language anymore, but set of tools. \nPython becomes a language which hard to learn. It got loaded with type annotation, lazy evaluation by default, asynchronous syntax, pattern matching (each isn't bad by itself). With that many basic problems, like package dependency management never resolved. \n\nIf standard language syntax doesn't give you performance you okay with, it is probably time to look another options. I don't want to think about syntax tricks all the time when code (that goes against opposite Python's philosophy). I am not married to Python it is just a tool for a job.\n\nNumpy is C and as result pandas. And it is not a standard Python extension it is more like API. Any conversion to Python native types cost performance loss. And even so pandas still not using multiple CPUs.\n\nI am surely missing your point but that can absolutely be me rather that you not explaining clearly. \n\nYou've mentioned lots of things (Pandas, Dask, annotations, async) as *problems* and I'm not sure I even recognise that. When I call a language feature in Python I don't inherently care how it is implemented. Is string.find() implemented in pure Python or via C? C surely, but *I don't care, not do I have to care*. I can generally take it for given that the implementation will be a good one. \n\nIn 20 years of working with Python the number of times I've cared about Python performance can be counted on (tbh) two (human) hands. When we profiled, 95-98% of our time was spent in C/C++. Even if we optimised our Python code by a factor of 10 nobody else would have noticed. \n\nIMHO most of the suggestions given can be useful, but if it is really necessary to bear them in mind and use them all the time perhaps it's time to rewrite in a faster language. Although you may find that that involves shunting the 5% of your code that takes 95% of your runtime over to another language and using FFI. \n\ntl;dr: I'm not married to Python either, but its speed has never been a reason to leave it.\n\nLast 7 years I have started to work with data. The face performance problems each time I need to process more than 1G.", "id": "gvall7y", "owner_tier": 0.7, "score": 0.5923076922307693}, {"content": "This reads like a clickbait blogpost with lots of content copied from other sources without a lot of actual \"meat\" to it.\n\nHis whole recent post history is just reposting release notes and clickbait titles with not fully correct code inside.\n\nI'm really confused how this is not relegated to /r/learnpython, the majority of comments are people picking apart the \"tutorial.\"\n\nCould be a bot that doesn't detect code blocks correctly.", "id": "gvbx8qg", "owner_tier": 0.7, "score": 0.36153846146153845}, {"content": "The formatting is messed up on old reddit. There is too much side scrolling inside code blocks. Either use linebreaks or take out the explanations from code blocks.\n\nI agree.\n\nRoughly 1/3 of users prefer the widescreen layout to the new.reddit.com layout. While /u/SolaceInfotech made a good post, it's close to unreadable for many of us. =(\n\n*edit* took a look at the new reddit formatting, and it's better, but it's actually side scrolling there too.\n\nOP needs to remove the non-code from the code blocks.\n\nThe side scroll looks weird on new reddit too\n\nReddit (old and new) has the worst markdown processing I've ever seen. \n\nWhat are they using? Their own implementation of John Gruber's markdown from scratch or a real Markdown flavor?\n\nIt is time to let old redit go\n\nI look forward to seeing the site that those users will migrate to.", "id": "gvb0t2e", "owner_tier": 0.3, "score": 0.18461538453846152}, {"content": ">The second code is faster than the first code because library function map() has been used. These functions are easy to use for beginners too.\n\nI agree with all the points but in the second point,\n\nthe second code is faster because `map` returns a generator object. It is more memory efficient but not faster. If you cast it into a list, it would probably take the same amount of time.\n\nOP is still thinking about python 2, which can safely be ignored when giving general advice at this point. Python 2 is a niche use case.\n\n`map(my_fn, my_list)` is obviously a lot faster than doing a for loop because it doesn't actually do any iteration or execute any code until you start pulling values from it. It's worth noting that list comprehensions are actually faster, though, because the interpreter python can pre-allocate the size of the list.\n\nIt's also faster because the two examples are not doing the same things. His first example is appending elements to a new list, the second is modifying an existing list in place.\n\nlist comprehension might be faster than for in loops but are not faster than map\n\n    elapsed 5.496323823928833 # for in\n    elapsed 0.0005249977111816406 # map\n    elapsed 3.3215620517730713 # list comp\n\n> `map(my_fn, my_list)` is obviously a lot faster than doing a for loop because it doesn't actually do any iteration or execute any code until you start pulling values from it.\n\n\\- the post you're replying to\n\nAs it happens, in the test I did (doing `str.upper` as in OP's example) `list(map())` was faster than the list comprehension, but not by 4 orders of magnitude.\n\nyead i miss read your reply, I assumed you meant list comp > map but it was meant that list comp was faster than for in, which it is", "id": "gvaqqkm", "owner_tier": 0.1, "score": 0.24615384607692306}, {"content": "    #code1 \n    newlist = [] \n    for word in oldlist:     \n    newlist.append(word.upper())\n    \n    #code2\n    newlist = map(str.upper, oldlist)\n\nThose two are not equivalent. One creates a list, the other creates an iterator\n\nWrap it in a list()", "id": "gvbnneu", "owner_tier": 0.1, "score": 0.04615384607692308}, {"content": "Use numpy, especially its arrays if you're using a lot of python lists and are iterating through them in your code.\n\nCan you point me toward a few examples of this?\n\nThere are a lot of numpy tutorials out there, especially on YouTube.\n\nAs for the performance side of things, https://towardsdatascience.com/how-fast-numpy-really-is-e9111df44347\n\n\nThis article shows that depending on the number of elements, numpy arrays can be up to a 100 times faster than python lists.", "id": "gvarkob", "owner_tier": 0.5, "score": 0.09230769223076922}, {"content": ">2. Use Built In Functions And Libraries-  \n>  \n>Python includes lots of library functions and modules written by expert  developers and have been tested thoroughly. Hence, these functions are  efficient...\n\nWhilst I do agree that they've been vetted by experts and will likely be faster than anything I write on my own, it's worth noting that for a lot of modules they're C-backed and this is normally the reason for the big speed ups. For example, take numpy. It's a C-backed library, hence why it is orders of magnitude faster than a pure-python implementation", "id": "gvbdrzu", "owner_tier": 0.5, "score": 0.030769230692307692}, {"content": "~~the~~ python code", "id": "gvb3cdg", "owner_tier": 0.5, "score": 0.023076923}, {"content": "In 8, str.join is slower than `str.__add__`, at least on small cases: https://t.me/pythonetc/650\n\nThe most important rule is to value readability over performance. If you think something is slow, prove it first.", "id": "gvc2hcj", "owner_tier": 0.1, "score": 0.015384615307692308}, {"content": "As lots of people have pointed out, this is spammy outdated clickbait, but I don\u2019t think anyone has yet pointed out that #4 is wrong: `re.search` caches the compiled regex automatically so calling it repeatedly is no more costly than variable lookups. (It is tidier, and would be true for other examples, but not in this instance.)", "id": "gvcu35o", "owner_tier": 0.5, "score": 0.030769230692307692}, {"content": "In each example, the second code should do \\_exactly\\_ what the first example does, otherwise they're not very helpful.\n\nAlso, it should be noted that `xrange` does not exist in Python 3, as it is not needed.", "id": "gvc308z", "owner_tier": 0.7, "score": 0.015384615307692308}, {"content": "Some days ago I found PyPy it promises speed times comparable to C, in theory it uses a just in time compiler and it's compatible with most common libraries", "id": "gvbt25l", "owner_tier": 0.3, "score": 0.0076923076153846155}, {"content": "None of these tips are actually going to help. Profile your code, use the output to identify and understand what makes it slow, and come up with a lower cost solution.", "id": "gvcajkm", "owner_tier": 0.1, "score": 0.0076923076153846155}, {"content": "A lot of things here I either disagree with or just doubt;\n\n- 2 is a strange comparison - you are not returning the same type of item.\n- I'm pretty sure the two examples in 4 are equivalent, with only a small variation in what functions from `re` you call and what you store as an intermediate. \n- List comprehension (5) is good for several reasons, but I wouldn't say that you should use it \"when the syntax becomes too big\"; you should honestly probably be less inclined to use it if you have a lengthy statement, for readability's sake.\n- For the import example (6), I think those are close to equivalent in time since you have to parse the module to find the function to import anyway. I still prefer importing the object, but I wouldn't use time as an argument here.\n- It's pretty odd to suddenly bring in a Python2 example, seeing as it's already sunset (10).\n\nThere are things in here I agree with, but I would probably recommend looking at a different resource for advice on how to improve.", "id": "gvck1qf", "owner_tier": 0.7, "score": 0.0076923076153846155}, {"content": "Isn't the for loop faster than the while loop because of it range function, which uses C?\n\nThat\u2019s correct.", "id": "gvb5qvd", "owner_tier": 0.1, "score": 0.023076923}, {"content": "Use pypy.", "id": "gvc3nq3", "owner_tier": 0.3, "score": -7.692307645557915e-11}], "link": "https://www.reddit.com/r/Python/comments/mv8z4p/how_to_speed_up_the_python_code/", "question": {"content": " \n\nWe all know that Python is one of the most popular programming all over the world. These days, it is being used in competitive programming because of its simple syntax and rich libraries. You can almost do anything with Python from data science, machine learning, signal processing to\u00a0 data visualization. But many people claim that python is a bit slow while solving grave problems. Time to execute a program depends on the code that you write. Knowing some tips to optimize the code will help you to speed up the python code. Let\u2019s see the top 10 tips to speed up python code.\n\n## Top 10 Tips To Speed Up Python Code \u2013\n\n \n\n# 1. Use Proper Data Structure-\n\nUse of proper data structures has significant effect on runtime. Python includes tuple, list, set and directory as built-in data structures. Most of the people use the list in all cases, but it is not a good choice. Basically use of proper data structures depends on your task. You can use tuple instead of list, because iterating over tuple is easier than iterating over a list.\n\n# 2. Use Built In Functions And Libraries-\n\nPython includes lots of library functions and modules written by expert developers and have been tested thoroughly. Hence, these functions are efficient and able to speed up the code-no need to write the code if the function is already available in the library. Let us see a simple example-\n\n \n\n    #code1 \n    newlist = [] \n    for word in oldlist:     \n    newlist.append(word.upper())\n    \n    #code2\n    newlist = map(str.upper, oldlist)\n    \n    The second code is faster than the first code because library function map() has been used. These functions are easy to use for beginners too. \n    \n\n# 3. Do Not Use Global Variables-\n\n    Python has global keyword to declare global variables. Global variable takes higher time during operation than local variable. Using few of them save form unnecessary memory usage. Also, Python scoops up a local variable more rapidly than a global one. While navigating external variables, Python is genuinely slow. A few other programming languages oppose the unplanned use of global variables. The counter is because of side effects like higher runtime. Hence, try to use a local variable rather than a global one whatever possible. Also, you can make a local copy before use it in a loop, saving time.\n    \n\n# 4. Try To Minimize The Use Of For Loop-\n\n    It is hard to  avoid the use of for loop. But whenever you can avoid it, do so. For loop is dynamic in Python and its runtime is more than a while loop. Nested for loop is more time consuming, two nested loops will take the square of the time in a single for loop.\n    code1\n    for i in big_it:\n        m = re.search(r'\\d{2}-\\d{2}-\\d{4}', i)\n        if m:\n            \u2026\n    \n    date_regex = re.compile(r'\\d{2}-\\d{2}-\\d{4}')\n    \n    for i in big_it:\n        m = date_regex.search(i)\n        if m:\n            ...\n    In such cases, it will be better to use a suitable replacement. Also, if for loops are inevitable, move the calculation outside the loop. It will save more time. We can see it in example as given. Here the second code is faster than first code since the calculation has been done outside the loop. \n    \n\n# 5. Use List Comprehension-\n\n    List comprehension offers a shorter syntax. It is handful when another list is made based on an existing list. Loop is important in any code. Sometimes the syntax in a loop becomes large. In such cases, you can use list comprehension.\n    L = []\n    for i in range (1, 1000):\n        if i%3 == 0:\n            L.append (i)\n    \n    Using list comprehension, it would be:\n    L = [i for i in range (1, 1000) if i%3 == 0]\n    \n    Here, the second code requires less time than the first code. Approach to list comprehension is short and more precise. Mostly there is not more difference in small codes. But in an extensive development, it can save time. \n    \n\n# 6. Do Not Use Dot Operation-\n\n    Try to avoid dot operation. To know it in detail, let us see the below program-\n    import math\n    val = math.sqrt(60)\n    \n    Rather than writing a code like this, write code like below-\n    from math import sqrt\n    val = sqrt(60)\n    \n    This is because when you call a function with . (dot), it first calls   __getattribute()__ or __getattr()__ which then uses dictionary operation that takes time. Hence try to use from module import function.\n    \n\n# 7. Make Use Of Generators-\n\n    In python, generator is a function that returns an iterator when keyword yield is called. Generators optimize memory. At a time they return single item rather than all at a time. If your list includes lots of data and you need to use one data at a time, use generators. Generators compute data in pieces, so function can return a result when called upon and retain its state. Generators stores the function state by stopping code after the caller generates the value, and it continues to run from where it is stopped. As the generators access and compute the on-demand value, a significant part of data does not need to be saved completely in memory. This results in memory savings, and speeding up the code.\n    \n\n# 8. Concatenate Strings With Join-\n\n    Concatenation is common when you work with strings. Mostly, in python, we concatenate with \u2018+\u2019. In every step, the \u201c+\u201d operation creates a new string and copies old material. It takes more time and hence is inefficient. To speed up Python code, you have to use join() to concatenate strings strings.\n    \n    code1\n    x = \"My\" + \"name\" + \"is\" + \"john\" \n    print(x)\n    \n    code2\n    x = \" \".join([\"My\", \"name\", \"is\", \"john\"])\n    print(x)\n    \n    Here, the first code prints \u201cMynameisjohn\u201d and second code prints \u201cMy name is john\u201d. The join() operation is more faster and efficient than \u2018+\u2019. It also keeps the code clean. So if you want a cleaner and faster code, start using join() rather than \u2018+\u2019 to concatenate strings. \n    \n\n# 9. Use The Latest Release Of Python-\n\n    Python is updated and upgraded regularly, and every release is faster and more optimized. Hence use the latest version of python. \n    \n\n# 10. Replace range() with xrange()-\n\n    Note that this is applicable for python 2 users.\n    \n    These functions are used to iterate anything in for loop. In case of range(), it saves all the numbers in the range in memory. But xrange() just saves the range of numbers that should be displayed. Return type of range() is a list, and that of xrange() is an object. As xrange() takes less memory, it takes less time. Hence use xrange() rather than range() whenever possible. \n    \n\n# Wrap Up-\n\n    The value of python is increasing day by day and these are some of the tips that reduce the runtime of python code. There can be few others too.", "id": "mv8z4p", "title": "How To Speed Up The Python Code?", "traffic_rate": 207.942496260595}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "a few performance tricks:\n1) dump objects in favor of simple lists or arrays for your board representation. Board pieces should be values that are OR'd together, i.e. BLACK=1, WHITE=2, KING=4, QUEEN=8, etc.\n2) don't copy the board state -- your move generator (that generates possible board states for your minimax function) should be able to undo moves that were just made so that you only need one copy of the board in memory.\n3) Find an evaluation function and minimax code that you already know is fast. Why reinvent the wheel here?\n\nI've written a program called [Raven Checkers](http://code.google.com/p/raven-checkers/) that works with these ideas.  \n\nGood luck.\n\nwrt 1) always keep it in simple NumPy arrays when possible. makes moving to a C backend much easier  \nwrt 3) scipy probably has something. their *optimizers* code is quite good", "id": "c0ib6fd", "owner_tier": 0.1, "score": 0.9999999989999999}], "link": "https://www.reddit.com/r/Python/comments/allhf/help_a_newbie_speed_up_a_python_program/", "question": {"content": "I've been messing around with a little chess program that I started to write a while ago. It's your run-of-the-mill minimax program.\n\nIt works, but I knew from the start that I'd have performance problems. I've been messing with cProfile trying to speed things up a bit, but to no avail. I can only run the program with a maximum search depth of 2 and even so it's still slow as hell.\n\nBefore dumping this little project in the garbage, I'd like to know if you guys can give me a couple of tips on how to improve overall performance or even the design.\n\nThe code is on http://bitbucket.org/itaborai83/chess/\n\nTo run the tests, \"python run_tests.py\"\n\nTo see the minimax executing, just run \"python minmax_game.py\" and it'll show the board as well as the minimax score found for each of the initial moves for each player.\n\nI'd appreciate your help.", "id": "allhf", "title": "Help a newbie speed up a Python program", "traffic_rate": 207.97041715140435}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "Did you compile them to exe?\n\nDo you have any antivirus products installed?\n\nWhat do the apps do besides display a GUI window?\n\nFir the tkinter app I used autoPyToExe and for the flet one I used the included command (flet pack) which I think is using pyInstaller. \n\nOne app takes paths to excel files, reads the excels files, does some calculations, creats some graphs and is exporting a pdf. \n\nThe second one is reading a csv file after sending http-get request and displays some values after calculations. \n\nBoth take some text input.\n\nYou didn't say anything about antivirus software installed on your machine...?\n\nSince you are converting it to exe, its possible the slowdown at start is because of the antivirus sandboxing and scanning the application to make sure its safe before allowing it to run.\n\nTest by disabling the antivirus runtime scans, or, not converting to an exe...\n\nI dont think I have any. Just windows defender I guess. \n\nBut thank you very much for your answers. \n\nSo it is possible to create quicker python apps and I just need to find a way to do so.\n\nWindows defender is an antivirus, that is known for slowing down startup of applications it doesn't recognise.\n\nAdd an exclusion for your application.\n\nhttps://support.microsoft.com/en-us/windows/add-an-exclusion-to-windows-security-811816c0-4dfd-af4a-47e4-c301afe13b26\n\nAwesome, thank you!", "id": "kt4bvd5", "owner_tier": 0.7, "score": 0.999999999375}, {"content": "When I was using PyInstaller and packaging to an AIO executable, startup time was slow.\n\nThis was fixed by packaging to a directory, and using InnoSetup to make a single exe to install for easy distribution.\n\nIm gonna look into that, thank you!", "id": "kt4fhoi", "owner_tier": 0.3, "score": 0.124999999375}, {"content": "Try codon for tkinter app and tell me , codon converts the code into native machine code.\n\nIm gonna try it, thanks!\n\nDont forget tell me the results , because i used codon in small code, but never tried in gui apps.", "id": "kt4h0lk", "owner_tier": 0.1, "score": 0.062499999375000004}, {"content": "From what I understand converting to an exe basically packages Python in it completely. You can make it smaller by just importing the parts of modules you are using to try and make it open faster but I just learned to accept the slow open speed for the convenience of just having a GUI.\n\nI even thought of learning c# just because it annoys me\n\nThat's a great idea! The right tool for the right job and all that :)\n\nYeah but I love the simple way of python and it works great until I want my coworkers to benefit from my apps as well. Maybe I will learn to like c# as well lol", "id": "kt4gpw4", "owner_tier": 0.3, "score": 0.249999999375}, {"content": "When you run the code from the editor, does it also take a long time? If it does, then the problem is with how you wrote the program, not what you used to compile it.\n\nOnly the first time, but this is even the case when I am not using any additional libs. \n\nBut then it takes like 5 secs not the up to 20.\n\nI think you might have a module or modules that loads or processes something during the start of the program. During the start of your program, you ideally should not allow any large objects to be processed.", "id": "kt6cb53", "owner_tier": 0.3, "score": -6.249999962015806e-10}], "link": "https://www.reddit.com/r/learnpython/comments/1b5b7ia/how_can_i_speed_up_the_starting_time_of_my_python/", "question": {"content": "Basicly I made myself two gui desktop apps (.exe for windows) and they take 10-20 sek to start up. I made the first with tkinter and the second with flet. As these are fairly simple apps I dont see why they should take this long to start. Is this just a python thing or did I just choose the slowest ways of making python apps? Once open, the apps are snappy and fast. \n\nThanks for your answers!", "id": "1b5b7ia", "title": "How can I speed up the starting time of my python desktop-apps?", "traffic_rate": 153.1714814814815}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "reddit"}, {"answers": [{"content": "\r\n    General answer: when you think performance; optimization, speed up ...\r\nThe first thing to do is to think profiler.\n26.4. The Python Profilers \u2014 Python 2.7.14 documentation[^]\npython - How can you profile a script? - Stack Overflow[^]\r\nThe profiler is a tool that will show you the time spend in your code and where it is spent. It also tell you how many time a given part is executed.\r\nThe place that consume the most time is the first place to look at for optimizing.\r\nAnd everywhere, you have to ask yourself:\r\n- Why am I doing this?\r\n- Is it really necessary?\r\n- Can I do it another way which is less resource hungry?\n\r\nYou have a file of ~500k words, this file is more or less constant. This mean that any work that can be done early and saved in the file will be a huge saving.\nPython\n\r\nletters.find(char.upper())\r\nFor example if you ensure that the file of words and the letters are using the same letter case, you don't have to worry when you process the file.\r\nThis code scans letters once for every letter of every word, but at this point, letters is constant and it is easy to know if a given letter is part of letters or not.\n\r\nAdvice: do not thread when you optimize, if processing the file is too long, use a reduced one for testing purpose.\r\n", "id": "2_1223050_1", "owner_tier": 0.5, "score": 3.0}], "link": "https://www.codeproject.com/Questions/1222962/Help-me-speed-up-my-Python-code-algorithm", "question": {"content": "\r\n\t\t\t    I'm new both to the language, and performance-sensitive algorithms as well, so it's been really challenging so far, which is a good thing I guess.\n\r\nAnyway, the task was the following: \n\r\nI have x amount (24 in this case) of letters, and a list of words. \r\nI'm supposed to find all the combinations of words(\"sentences\") that use up the given letters with no letter remaining.\n\nUpdate: A word can appear more than once in a sentence, for example \"the the the the\" is perfectly fine if the letters allow it.\n\r\nFor example:\n\ninput:\n\r\ngiven letters:\n\n\r\nIFSNIHLSTA\r\nAnd the list of words:\n\n\r\nnice\r\nguys\r\nfinish\r\nlast\n(The max amount of words we're looking for is irrelevant in this case.)\n\r\nThe output is:\n\n\r\nfinish last\r\nlast finish\n\r\nI'm pretty sure there are some existing algorithms I could use, but I don't know any at the moment, so I decided to write one from scratch, and as you could guess, it ended up being so slow that it's unusable.\n\nWhat I have tried:\n\r\nThe very first thing to do was obviously to cut down the amount of words first, so I filtered those out that aren't needed for sure.\n\r\nThen the first main idea was to take a word, use the letters up for it, then take a next word, and use the remaining letters, and so on until all the letters are used up, or no more words can be created. If the letters are all used up, then hurray we got a sentence.\n\r\nRepeating the above process for all the words, and again for each word(after a valid one) will in the end get us all the combinations, right?\n\r\nThe problem with it is the speed, finding sentences containing a single word will require iterating through the list once, however two word sentences will need an another iteration after each word in the worst case, and if we're looking for 6 word sentences, then oh boy... you get the idea.\n\r\nSo I looked into ways to make it efficient enough to be usable (if I had been successful, I wouldn't be writing this):\n\r\nI figured that since I'm only looking for the combinations, the order doesn't matter, so I have far fewer words to start with (if I'm looking for sentences of 6 words, then I only have to look at the 1/6 of the words for the first iteration).\n\nThe implementation is really a spaghetti code, as a day ago I didn't even know what python looked like, sorry about it.\n\r\nAnyway it follows the logic written above, the difference being that it splits the words up, and starts a process for each cpu core.\n\r\nSince it is around 200 lines, I put it on pastebin instead of pasting it here:\nCode on pastebin[^]\n\nI have 2 kinds of questions:\n\nTheoretical:\r\nHow to improve the algorithm, or what other algorithms to use, that could be more efficient? (and also what am I missing?)\n\nUpdate: There still some duplicates in the output, and I'll need to figure a way out to remove them.\n\nAbout performance itself:\r\nEven though each process runs on an own cpu thread, and there's no disk i/o going on, the CPUs isn't 100% used, why could that be?\n\r\nAlso would I get further with CUDA or openCL, if yes, how?\n\r\nThanks in advance\r\n\t\t    ", "id": "1222962", "title": "Help me speed up my Python code/algorithm", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["Python2.7"]}, {"answers": [{"content": "\r\n    Without being able to look at your code I can only guess at what might be causing the slowdown which is what you need to know in order to make an effective change. \r\nThere are however 3 outline possibilities I can think of.\n\r\n1 Your calculation simply takes that long.\r\n2 Your non calculation e.g UI code is taking up the processor time.\r\n3 Your application is spending a lot of time thrashing between threads.\n\r\nYou have excluded option 1 by isolating your calculation code. Now you need to run your application without performing the calculation and use Windows Task Manager or SysInternals Process Explorer to determine what the processor usage of the rest of the application is.\r\nIf neither part of the application is consuming excessive proessor time when run without the other then you may need to look at any data that is shared between threads and how access to this is being synchronised. Is your process actually wasting most of the time waiting for access to data locked by a thread that is not currently executing?\r\n", "id": "2_571486_2", "owner_tier": 0.3, "score": 3.0}], "link": "https://www.codeproject.com/Questions/571452/Howplustoplusspeedplusuppluswindowsplusforms", "question": {"content": "\r\n\t\t\t    I have a large program which does a lot of counting and calculation behind the scenes. When it enters this calculation stage, the windows form only has this to do, but I've set it as a background task \"background worker\" functions etc.\n\r\nThe calculating takes a large amount of time and I've done reading about how to minimise this. Possible (and followed through tips) were:\n\r\n- decreasing module sizes\r\n- passing data by reference/pointers\r\n- not using gradient colors\r\n- rendering allocated memory space free after use\r\n- not using fancy graphics\n\r\nNow, the problem is the main calculation is the only thing actually occuring once selected, and the user has to sit looking at a screen saying \"calculating\". Not even a moving \"...\"\n\r\nTo put it into perspective:\r\nI have the same program in a seperate c++ file which is not windows forms - standard c++ that just does the calculation. It finished in about 5-10 mins depending on what the user inputs. In windows forms, it's been 2 hours and still running. I've checked the files it is writing to and I've found it's averaging at a shockingly low rate of calculation. Each result takes about 20 seconds, or more, whereas in the std c++ it's too fast to count per second! :D\n\r\nAny ideas what I could check for, or any feedback on what you've found has helped with your program speed would be much appreciated!\r\n\t\t    ", "id": "571452", "title": "How to speed up windows forms", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["C++/CLI"]}, {"answers": [{"content": "\r\n    You answered your own question:\nQuote:My goal is to train an AI model, predict with it, and store it without any computer lagging while running the computer with the required storage space exceeding the (GPU memory + DRAM + swap memory).\n\r\nHow on earth are you going to execute code on a machine that does not meet the requirements of the code? If your code is demanding 64GB of RAM to execute with any speed and your machine only has 8GB, you're going to run into severe performance problems. You simple have no way around that, besides adding more RAM to the machine!\n\r\nYour machine slows down because it's swapping memory to page file, a very slow process compared to the speed of memory. You cannot possibly expect a millions of page swap operations to not slow the machine down.\r\n", "id": "2_5382806_1", "owner_tier": 0.7, "score": 2.5}], "link": "https://www.codeproject.com/Questions/5382799/My-linux-computer-will-become-very-slow-or-even-ha", "question": {"content": "\r\n\t\t\t    In PyTorch, I use `torch.cuda.memory.CUDAPluggableAllocator` and `cudaMallocManaged` methods to allocate (GPU memory + DRAM + swap memory).\n\r\nWhen I do this, my computer becomes very slow with extremely high `iowait` values and will quickly hang up after using up all the GPU memory and DRAM, but the `CPU usage` values appear to be normal.\n\r\nI am using *Ubuntu Server 22.04* *Anaconda3* and *Docker*. Since Linux automatically adjusts and uses swap memory once all the DRAM is used up. I want to train an AI model, predict with it, and\u00a0store it without any computer lagging while running the computer with the required storage space exceeding the (GPU memory + DRAM + swap memory).\n\r\n################################################## \n\r\nReference:\r\n- Using custom memory allocators for CUDA `[https://pytorch.org/docs/stable/notes/cuda.html](https://stackoverflow.com)`\r\n- Introduction swap memory `[https://blogs.oracle.com/linux/post/understanding-linux-kernel-memory-statistics](https://stackoverflow.com)`\n\nWhat I have tried:\n\r\nWhat did you try and what were you expecting?\r\nDescribe what you tried, what you expected to happen, and what actually resulted. Minimum 20 characters.\n\r\nMy goal is to train an AI model, predict with it, and\u00a0store it without any computer lagging while running the computer with the required storage space exceeding the (GPU memory + DRAM + swap memory).\n\r\nIn order to achieve my goal, I have tried the following three methods or a combination of them:\r\n- Force a program to\u00a0use\u00a0swap memory directly before DRAM runs out.\r\n- Use PyTorch's built-in functions to accomplish my objectives.\r\n- Employ a software control program to prevent the computer from lagging and continue\u00a0the\u00a0training\u00a0of\u00a0the AI model, predict with it, and store it.\n\r\nI have tried `cgroup v2`, Docker (including Nvidia Docker runtime), Linux preloading `vm.swappiness` functions, PyTorch `fbgemm` UVM tensor, and `torch.cuda.memory.CUDAPluggableAllocator` but could not achieve my goal.\n\r\n---\r\nThe following command line is expected to implement the following 2 methods:\r\n- Force a program to\u00a0use\u00a0swap memory directly before DRAM runs out.\r\n- Employ a software control program to prevent the computer from lagging and continue the training of the AI model, predict with it, and store it.\n\r\n`cgroup\u00a0v2` is used to limit DRAM\u00a0use.\r\nThe command line trying to achieve my goal is:\r\n```\r\necho 42949672960 > /path/to/the/location/memory.high\r\n```\r\n---\r\nThe following segment of a command line is expected to implement the following 2 methods:\r\n- Force a program to\u00a0use\u00a0swap memory directly before DRAM runs out.\r\n- Employ a software control program to prevent the computer from lagging and continue the training of the AI model, predict with it, and store it.\n\r\nDocker is used to limit DRAM or Swap Devices\u00a0use\u00a0too.\r\nThe segment of this command line is:\r\n```\r\ndocker run ... \\\r\n--memory=10g \\\r\n--memory-swap=3789g \\\r\n...\r\n```\r\nand [EDITTED ON 2024-05-25]\r\n```\r\ndocker run ... \\\r\n--device-write-bps=/path/to/device:1500mb \\\r\n--device-read-iops=/path/to/device:1500gb \\\r\n...\r\n```\r\n---\r\nThe following source code is expected to implement the following 2 methods:\r\n- Use PyTorch's built-in functions to accomplish my objectives.\r\n- Employ a software control program to prevent the computer from lagging and continue\u00a0the\u00a0training\u00a0of\u00a0the AI model, predict with it, and store it.\n\r\n`torch.cuda.memory.CUDAPluggableAllocator` method is called from alloc.so which is compiled from the following alloc.cc source code:\r\n```\r\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda-11.8/include -shared -fPIC\r\n#include <sys types.h=\"\">\r\n#include <cuda_runtime_api.h>\r\n#include <iostream>\r\nextern \"C\" {\r\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream)\r\n{\r\n\tvoid *ptr;\r\n\tcudaMallocManaged(&ptr, size);\r\n\treturn ptr;\r\n}\r\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream)\r\n{\r\n\tcudaFree(ptr);\r\n}\r\n}\r\n```\r\nand\r\n```\r\n#include <sys types.h=\"\">\r\n#include <cuda_runtime_api.h>\r\n#include <iostream>\r\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda-11.8/include -shared -fPIC\r\nextern \"C\" {\r\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream) {\r\n   void *ptr;\r\n   cudaSetDeviceFlags(cudaDeviceMapHost);\r\n   cudaHostAlloc(&ptr,size,cudaHostAllocMapped);\r\n   return ptr;\r\n}\r\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\r\n   cudaFreeHost(ptr);\r\n}\r\n}\r\n```\r\n---\r\nI also tried `cpulimit`, `prlimit` and `nice` but it still doesn't work. [EDITTED ON 2024-05-26]\r\n`cpulimit` command line\r\n```\r\ncpulimit --pid $(processs_pid) --limit=15 --lazy --background\r\n```\r\nIt is limited to <100, but the process is lagging and auto-killed.\r\n`nice` command line\r\n```\r\nnice -n 19 python /path/to/file.py\r\n```\r\nThis command does not solve the lagging problem.\r\nAnd `prlimit`\r\nThe command line\r\n```\r\nprlimit -m=42949672960 python3 /path/to/file.py\r\n```\r\nProcess limit status\r\n```\r\nLimit                     Soft Limit           Hard Limit           Units     \r\nMax cpu time              unlimited            unlimited            seconds   \r\nMax file size             unlimited            unlimited            bytes     \r\nMax data size             unlimited            unlimited            bytes     \r\nMax stack size            8388608              unlimited            bytes     \r\nMax core file size        0                    unlimited            bytes     \r\nMax resident set          42949672960          42949672960          bytes     \r\nMax processes             256496               256496               processes \r\nMax open files            1024                 1048576              files     \r\nMax locked memory         8419708928           8419708928           bytes     \r\nMax address space         unlimited            unlimited            bytes     \r\nMax file locks            unlimited            unlimited            locks     \r\nMax pending signals       256496               256496               signals   \r\nMax msgqueue size         819200               819200               bytes     \r\nMax nice priority         0                    0                    \r\nMax realtime priority     0                    0                    \r\nMax realtime timeout      unlimited            unlimited            us\r\n```\r\nThis command does not solve the lagging problem too.\r\n---\r\nI have tried to use PyTorch `fbgemm`  library, but the result is similar to employing `torch.cuda.memory.CUDAPluggableAllocator`.\n\r\nWould there be any other possible methods to achieve my goal?\n\r\n##################################################\n\r\nComplement 1:\r\nI have already used M.2. SSD nvme PCIe 3.0 x 2 to swap memory.\r\nAs I know, A PCIe 3.0 has a maximum bandwidth of around 3.x Gbps, I have 2 M.2. SSD.\n\r\n##################################################\n\r\nReference:\r\n- swappiness `[https://phoenixnap.com/kb/swappiness](https://stackoverflow.com)`\r\n- Use RAM after GPU memory is not enough [`https://stackoverflow.com/questions/27035851/use-ram-after-gpu-memory-is-not-enough`](https://stackoverflow.com)\r\n-\u00a0What is the maximum read and write speed for pcie 3.0 x4 m.2 slots? [`https://pcpartpicker.com/forums/topic/391989-what-is-the-maximum-read-and-write-speed-for-pcie-30-x4-m2-slots?__cf_chl_tk=ytRz0fL2zwxqVfoQBY0G6gwWnFHiwRBSpcV9dbbeSEU-1716647279-0.0.1.1-1791`](https://stackoverflow.com)\r\n\t\t    ", "id": "5382799", "title": "My linux computer will become very slow or even hang up when GPU memory and DRAM are used up after using pytorch to run cudamallocmanaged", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["Ubuntu", "CUDA", "docker", "Python3.6"]}, {"answers": [{"content": "\r\n    OK, so you want to see your CPU beg for mercy using a Parallel.For?\n\r\nHere you go:\nC#\n\r\npublic void TestMethod()\r\n{\r\n    ParallelLoopResult result = Parallel.For(0, 50, x =>\r\n    {\r\n        Random random = new Random();\r\n        double result = 1.0;\r\n\r\n        for (int i = 1; i < 1000000000; i++)\r\n        {\r\n            result += random.NextDouble();\r\n        }\r\n\r\n        Console.WriteLine($\"{result}\");\r\n    });\r\n}\n", "id": "2_5317501_1", "owner_tier": 0.7, "score": 0}, {"content": "\r\n    This is definitely a response to an old question...but, I didn't seem to see an actual answer to the OP question with some saying this was all ok. I'm going to come at this from a slightly different viewpoint. I don't use Parallel.For in C# or as a matter a fact in C++ (what I almost solely program in). I worked with multithreaded code before built in parallelization was provided by IDEs like Visual Studio such that you had to manage it yourself.\n\r\nWhen you spawn a thread in C++ you can specify the thread's priority. Changing a program's thread priority in Task Manager doesn't always change the thread priority of the spawned threads within the program, it only changes the program's primary thread priority (in which case if spawned threads use the same priority as the main thread then it will propagate down, but this isn't guaranteed because you can specify a threads priority upon creation).\n\r\nIf Parallel.For has the ability to specify the thread priority (which I didn't see in any code above), that would be the very first thing to look at since otherwise you are definitely at the mercy of Parallel.For. If you find that Parallel.For uses a below average thread priority and cannot override/set this...your only choice then is to code your own multithreaded function. But, based on the simpler code providing 100% CPU usage, I would assume that Parallel.For uses at least an average priority thread and therefore the problem lies with the code differences between your examples.\n\r\nThe second problem I see is with memory allocation/destruction which others pointed out. If you plan to use any kind of memory in an optimized multithreaded function...YOU should manage that memory beforehand (that means don't use lists, arrays, vectors...basically anything that allows you to grow your memory shouldn't be used because they have a LOT of overhead). That means you need to allocate your own memory (before using the parallel function) and then reuse it within your function and only destroy it once your parallel function is completely done.\n\r\nIf Windows requires a new memory allocation when memory is requested, a page fault occurs. Page faults WILL slow down your code especially in a multithreaded function where you could be doing a page fault every pass (which will directly result in something you are seeing - I've experienced the same thing myself). This is because a lot goes on in a Page Fault to memory allocation and it is preferable to minimize this everywhere you can (growing a List<> causes Page Faults because of the reallocation of memory). Using a List<int> variable means pulling in everything a List brings with it instead of just creating a massive integer array at the beginning and splitting it up as needed in the function. Since I saw no mention of what the List<int> variable was being used for, this is something the OP needs to think about...\n\r\nIn the end I would think it is obvious that the culprit is the List<int> creation/destruction in your Parallel.For function since it results in a massive slow down in CPU usage when present and not when not present. I'm pretty sure you will find that figuring out a way to not do any creating or destroying of memory within your parallel function will bring your CPU usage back up close to 100%.\r\n", "id": "2_5383940_1", "owner_tier": 0.1, "score": 0}], "link": "https://www.codeproject.com/Questions/5317493/Throttling-Why-is-application-not-using-all-availa", "question": {"content": "\r\n\t\t\t    I have A LOT of calculations to perform so I wrote a quick program to do them for me.\n\r\nFor some reason when I can't seem to use all of my CPU, it seems like the system is throttling it. I wrote a simple application to demonstrate this behavior. See the below code:\n\nC#\n\r\nParallel.For(0, 10000, x =>\r\n{\r\n    int i = 0;\r\n    while (true)\r\n    {\r\n        i++;\r\n    }\r\n});\n\r\nRunning the above code in a console app, it maxes out all of my processors and my CPU utilization is at 100% (as I would expect).\n\r\nIn my calculation, I am creating several Lists and arrays so I added that to the above code and that is when I saw it behave just as my actual program was (where it would use less than 100% of the CPU). See example code below:\n\nC#\n\r\nParallel.For(0, 10000, x =>\r\n{\r\n    int i = 0;\r\n    while (true)\r\n    {\r\n        i++;\r\n        List<int> vs = new List<int>();\r\n    }\r\n});\n\r\nOn my computer, which has 12 logical processors, the CPU utilization only reaches 70-80%. When running on a higher-performance PC I have, which has 32 logical processors, the behavior is even more dramatic - the highest the CPU utilization goes is 50%. And my real application that is performing the calculations only uses 20% of the total CPUs.\n\r\nAnd as a follow-up, in Task Manager I see that it is using all logical cores more or less evenly - it isn't maxing out some and others sitting idle.\n\r\nI am curious why this is happening, and more importantly, how can I make my program use all of the processor?\n\nWhat I have tried:\n\r\nI tried monitoring garbage collection but that didn't offer any clues (and also wouldn't make sense because I would expect garbage collection to show up as CPU usage).\n\r\nI tried increasing the process priority from normal to \"High\" and even \"Real-Time\" but that didn't change anything.\r\n\t\t    ", "id": "5317493", "title": "Throttling. Why is application not using all available processing power.", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["C#", ".NET"]}, {"answers": [{"content": "\r\n    Creating 400 threads does nothing but slow your workload down. The system can only execute the same number of threads as you have cores in your CPU, usually 8 or less unless you have a higher-end CPU. So, creating 400 threads is pointless.\n\r\nThere is no way to speed up the threads other than rewriting and optimizing the code the thread is running, and this is assuming the bottleneck in your code is CPU-bound.\r\n", "id": "2_5308529_2", "owner_tier": 0.7, "score": 0}, {"content": "\r\n    Python is limited by the Global Interpreter Lock (GIL), which only allows one thread to execute at a given instance.  This means that threading does not reduce program run times, regardless of how many cores your system has.  There's a Stack Overflow discussion here: multithreading - python multi-threading slower than serial? - Stack Overflow[^] Note that this is quite old, and Python3 may change things. Google for slow python threads.  There are plenty of resources that might help you choose another direction to improve your programs.\r\n", "id": "2_5308530_1", "owner_tier": 0.3, "score": 0}], "link": "https://www.codeproject.com/Questions/5308525/How-do-I-make-a-faster-thread-in-Python", "question": {"content": "\r\n\t\t\t    Thread is slower, timeout is set to 13 seconds\n\nWhat I have tried:\n\n\n\r\nfor i in range(400):\r\n\ttry:\r\n\t\tthread=consumer(quee)\r\n\t\tthread.setDaemon(True)\r\n\t\tthread.start()\r\n               thread.excepthook()\r\n\texcept:\r\n\t\tprint \"Working only with %s threads\"%i\r\n\t\tbreak\n", "id": "5308525", "title": "How do I make a faster thread in Python", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["threads", "multi-threading"]}, {"answers": [{"content": "\r\n    Looping through the processes may be slow and as you have done you have to be prepared for exceptions, which again cause delay.\n\r\nFrom your code it's not clear what you're looping (the For is missing) but what if you would use GetProcesses[^] to get all the processes before entering the loop and to avoid constantly searching for the processes.\n\r\nAnother(small) enhancement could be the usage of a With[^] block, for example for blist.\n\r\nAnother thing is that since you can't make this lightning fast, why not show a 'splash' screen for the user and tell about the progress. You could have a background worker searching for the processes and the UI would show what happens all the time. This doesn't take away the duration but it gives a better user experience when you see that the program isn't hung.\r\n", "id": "2_1033648_2", "owner_tier": 0.5, "score": 0}, {"content": "\r\n    @Mika Well, I changed my code just a bit and for some reason, it's faster now. So far, it doesn't make sense to me. Maybe it will come to me later. Here's the code after I changed it. The ListView windows only takes 2 seconds to load and I can live with that.\n\n\n\r\nblist.Clear() 'clear listbox. In case listbox window has been closed then reopened\r\n\r\nimgList.ImageSize = New Size(30, 30)\r\n\r\nFor Each p As Process In Process.GetProcesses\r\n\r\n    Try\r\n\r\n        strName = Process.GetProcessesByName(p.ProcessName)(0).MainModule.FileName.ToString()\r\n\r\n        If strName.ToLower <> \"idle\" Or strName.ToLower <> \"system\" Then\r\n\r\n            exeIcon = Icon.ExtractAssociatedIcon(strName)\r\n\r\n            imgList.Images.Add(exeIcon)\r\n\r\n            blist.Items.Add(imgList.Images(i))\r\n            blist.Items.Add(strName)\r\n            blist.Items.Add(p.Id)\r\n            i += 1\r\n        End If\r\n\r\n        'MsgBox(p.Id)\r\n    Catch\r\n        Continue For\r\n    End Try\r\nNext\r\n\n\r\nI moved all my code inside of the \"try\" block. It works now. Maybe someone will tell me how :) Thanks for the help Mika\r\n", "id": "2_1033697_2", "owner_tier": 0.1, "score": 1.0}], "link": "https://www.codeproject.com/Questions/1033644/how-do-I-speed-up-the-load-time-of-a-form-Im-loopi", "question": {"content": "\r\n\t\t\t    As stated, my form loads slow while looping through processes. I'm catching all the \"access denied(5) exceptions\" and I think that's what is slowing things down. I'm even using a \"continue for\" (which I've never used before) to see if that would speed it up. I know it helps though. Here's my code: \n\r\n    Try\r\n        strName = Process.GetProcessesByName(p.ProcessName)(0).MainModule.FileName.ToString()\r\n    Catch\r\n        Continue For\r\n    End Try\r\n\r\n\r\n    exeIcon = Icon.ExtractAssociatedIcon(strName)\r\n\r\n    If strName.ToLower <> \"idle\" Or strName.ToLower <> \"system\" Then\r\n\r\n        imgList.Images.Add(exeIcon)\r\n\r\n        blist.Items.Add(imgList.Images(i))\r\n        blist.Items.Add(strName)\r\n        blist.Items.Add(p.Id)\r\n        i += 1\r\n\r\n    End If\r\n\r\nNext\r\n\n\r\nDoes anyone see where I can optimize this code? Is there a faster way to handle the exceptions? Maybe there's a way to block more of the exceptions? Thanks for looking.\r\n\t\t    ", "id": "1033644", "title": "how do I speed up the load time of a form I'm looping through running process before the form loads vb.net", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["VB"]}, {"answers": [{"content": "\r\n    Maybe your threads run on a single core. See, for instance multithreading - C++ run threads on different cores - Stack Overflow[^].\r\n", "id": "2_5311326_1", "owner_tier": 0.5, "score": 4.0}, {"content": "\r\n    We can't help you based on so little information: but from what I read, std::Vector is thread safe* - which probably means that it has internal locking which would seem to confirm the results you are getting. If the methods include internal locking of the whole vector, then multiple threads will just slow things down since only one of them can be active at any time.\r\nAdditionally, the number of threads created can slow the machine as well - if it exceeds the \"free cores\" then the thread switching overhead can make a marked difference.\n\r\nI'd think about what you are processing, and see if you could separate it into smaller Vectors each of which could be locked independently and test that to see if it confirms the problem.\n\n* Using Data Structures Safely with Threads[^] \r\n* c++ - Is std::vector or boost::vector thread safe? - Stack Overflow[^] \n", "id": "2_5311327_1", "owner_tier": 0.9, "score": 0}, {"content": "\r\n    Every thread needs some overhead and system resources, so it also slows down performance. Another problem is the so called \"thread explosion\" when MORE thread are created than cores are on the system. And some waiting threads for the end of other threads is the worst idea for multi threading. \n\r\nThe best strategy is to create some long running threads, on which some complete work load is enqueed and the results get dequeed. I would start with an UI thread and a work thread for the vector computation.\n\r\ntip: check that you compile for the multithreading in the compiler and linker settings\r\n", "id": "2_5311349_1", "owner_tier": 0.3, "score": 0}], "link": "https://www.codeproject.com/Questions/5311325/Cplusplus-why-my-multi-threads-program-is-slower-t", "question": {"content": "\n\n\r\nI have to deal with a vector containing around 30 million elements, as a result it will return another vector containing around 55 million elements. \r\n\r\nI decide to go with multithreading. I created a few threads and each thread will handle a portion of the vector, store its sub-result into a temp vector. Finally when all threads finish, combine all the temp vector into a big vector, that is the final result. There is no mutex/lock used at all. Also I am very careful with the vector. I use reserve whever possible to avoid the reallocation.\r\n\r\nI have tried run the program with 1 thread, 2 threads, 4 threads and 8 threads. 1 thread gives the best result. I am so confused. Can anyone tell me why?\r\n\r\nBtw, all the operations happens in the ram and no IO. I am running the program on my laptop with 4 cores.\n\nWhat I have tried:\n\r\nI have tried different number of threads. none of them show an increase of speed. 1 thread looks the fastest.\r\n\t\t    ", "id": "5311325", "title": "C++: why my multi-threads program is slower than single thread?", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["multi-threading", "C++11"]}, {"answers": [{"content": "\r\n    Creating \"cold\" tasks and managing the threads yourself seems like a bad idea. Why not create \"hot\" tasks and let the thread pool take care of scheduling them? Or use Parallel.For or Parallel.ForEach instead?\n\r\nWith your current code, unless every task takes exactly the same length of time to complete, you will be \"wasting\" cores. Consider: You have ten tasks to run. You start five tasks. Four of them complete in one minute; the fifth takes three minutes. For two minutes, only one task is running; the other five pending tasks have not started.\r\n", "id": "2_5356237_1", "owner_tier": 0.7, "score": 0}, {"content": "\r\n    Here is how I would approach it...\nC#\n\r\nint taskCount = 1000 ;\r\nint completedCount = 0;\r\n\r\nRandom rand = new Random();\r\n\r\nTask[] tasks = new Task[taskCount];\r\n\r\nfor (int i = 0; i < taskCount; i++)\r\n{\r\n    tasks[i] = DoLongTask(5000 + rand.Next(1000, 3000));\r\n}\r\n\r\n\r\nwhile (completedCount < taskCount)\r\n{\r\n    Task completed = await Task.WhenAny(tasks);\r\n    completedCount = tasks.Count(x => x.IsCompleted || x.IsFaulted || x.IsCanceled);\r\n}\r\n\r\nConsole.WriteLine(\"Done!\");\r\nConsole.ReadKey();\r\n\r\nasync Task DoLongTask(int delay)\r\n{\r\n    // yield for bulk starting of tasks\r\n    await Task.Yield();\r\n\r\n    // do a long async task here\r\n    await Task.Delay(delay);\r\n}\n", "id": "2_5356279_1", "owner_tier": 0.5, "score": 0}], "link": "https://www.codeproject.com/Questions/5356232/Why-does-my-process-slow-down-over-time", "question": {"content": "\r\n\t\t\t    I am processing a very large set of files. To do this I split the work up in +/- 500k tasks and wrote a little piece of code to iterate through and collect from these tasks.\n\nC#\n\r\nvar threads = 16;\r\n\r\nfor (int i = 0; i < task_amt; i++)\r\n{\t\r\n\ttaskList.Add(new Task<double>(() =>\r\n\t{\r\n\t\t// heavy computation code\t\r\n\t\treturn result;\r\n\t}));\t\r\n}\r\n\r\nfor (int j = 0; j < taskList.Count - threads; j += threads)\r\n{\t\r\n\tfor (int k = 0; k < threads; k++)\r\n\t{\r\n\t\tConsole.WriteLine($\"Starting task {j + k}.\");\r\n\t\ttaskList[j + k].Start();\r\n\r\n\t}\r\n\tfor (int k = 0; k < threads; k++)\r\n\t{\r\n\t\tConsole.WriteLine($\"Waiting for task {j + k}.\");\r\n\t\ttaskList[j + k].Wait();\r\n\t\tvar taskResult = taskList[j + k].Result;\t\t\r\n\t}\r\n}\n\r\nThis works like a dream, it utilizes all the logical cores, they are at 100% and this is CPU-heavy so the other resources are negligible low.\n\r\nBut after about 15 mins it looks like it's slowing down, and after 30 mins the CPU is utilized only 50%, and after 1.5 hour 30%(all other resources still negligible low).  This negatively influences the total calculation time severely, can anyone tell me why this happends?\n\r\nUPDATE:\n\r\nAfter 14 hours barely any CPU usage visible, process was gonna take about 1.5 hours at full CPU usage.\n\nWhat I have tried:\n\r\nThe only reason I can think of is thermal throtteling, I have tried finding out if my CPU is using thermal throttle to cool down, unfortunately the hardware monitoring applications I used did not show me the CPU temp.\r\n\t\t    ", "id": "5356232", "title": "Why does my process slow down 70% over time?", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["C#"]}, {"answers": [{"content": "\r\n    A \"foreach\" is slower than a stright up \"for\" indexer because the foreach has to maintain an enumerator object over the collection. Calling the navigation functions of the collection imparts an overhead multiplied by the number of items in the collection.\n\r\nA plain \"for\" loop is just an indexer into an array of values/objects. Basically, a for loop just increments a pointer into an array and has very little overhead.\n\r\nIn order to make the for loop faster, you'd have to have multiple threads, each working on a smaller segment of the entire array, but setting up and managing the threads itself may add more overhead than its worth.\n\r\nAs what's already been pointed out, it's not the speed of iterating over the array that's the problem. It's more going to be what you're doing to each element in the array that will slow things down.\r\n", "id": "2_5294614_1", "owner_tier": 0.7, "score": 5.0}, {"content": "\r\n    One way (but tricky) is using parallel functions, see: How to run async JavaScript functions in sequence or parallel[^] \r\nThis way you could use several functions that only process a 'chunk' of the array.\n\r\nAlso see: which-type-of-loop-is-fastest-in-javascript-ec834a0f21b9[^]\r\n", "id": "2_5294579_2", "owner_tier": 0.3, "score": 0}, {"content": "\nQuote:What is the fastest way to loop thorugh a long array\r\nFirst of all, a list of 121 is not long, long usually start after 100,000.\r\nWhatever the flavor of the loop (for, while ...), by itself, it is fast.\r\nWhat make a loop slow is what you do inside the loop.\n\r\nThe problem is that you told nothing of interest, we have no idea of what is the game or how you did the loop.\n\r\nBefore thinking to multithreading, you need to think about optimization.\nProgram optimization - Wikipedia[^]\r\n", "id": "2_5294609_2", "owner_tier": 0.5, "score": 0}, {"content": "\r\n    Consider the response, above, that ultimately references you towards code optimization.\n\r\nThere are a few things you can do (no code, so these are rather general) to speed up processing.\n\r\n0 - Do any calculations and value setting that are not modified within the loop outside of the loop.  No need to repeatedly set a constant to the same value.\r\n1 - if you're calculation values from a limited set (such as powers of 2, or something like that, use a lookup table instead of the calculations.  If you can just use an index into a table of pre-calculated values you can speed things up by saving, potentially, an immense number of calculations. See, also, item 0 \r\n2 - if your loop has internal nested loops, see if you can \"unroll them\" into straight series of calculations.  This is most feasible if you have a fixed number of elements for any the inner loops.\r\n3 -  If  you have internal functions, if they're very simple then just plug in the code, instead.  Calling a function and returning is a lot of overhead.\n\r\nThese are just a few and they  don't all apply all the time.  Use good judgement.\n\n\n", "id": "2_5294683_1", "owner_tier": 0.5, "score": 0}], "link": "https://www.codeproject.com/Questions/5294578/What-is-the-fastest-way-to-loop-thorugh-a-long-arr", "question": {"content": "\r\n\t\t\t    What is the fastest way to loop through an array of 121 items?\r\nI was wondering because (skip to what you have tried)\n\nWhat I have tried:\n\r\nI have tried looping using forEach() function and it causes my game to slow down to 10-25 fps\r\nso that's why I want some recommendations for fast looping\r\n\t\t    ", "id": "5294578", "title": "What is the fastest way to loop thorugh a long array", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["Javascript"]}, {"answers": [{"content": "\r\n    Much thanks for the responses. I think I need to add a little extra information.\n\r\nI am perhaps naive, but I really did hope for nearly a 16x speedup on my ancient server (Dell R900) with 16 processor cores. It's a Linux machine and my Windows program is running under Wine. The \"top\" command usually says that my program is getting 1580% to 1590% of the CPU out of a total of 1600%. There are certainly other processes running but they are taking very few cpu cycles. My program does no I/O, my threads do not interact (no queues, no locking, no waiting, etc.), and there is ample memory. (When I run on Windows, there is more \"Windows junk\" running in the background, but I can still usually get 350% percent or so of the machine for my program out of 400% possible with four processor cores.)\n\r\nI really do not believe my program has a multi-threading design problem (but of course it's certainly possible it does). Therefore, I decided to try to create a complete dummy program which illustrates the problem and which is small enough to post.  Here it is.\n\nC++\n\r\n// timing_test.cpp : dummy program to test the timing of multiple CPU bound threads.\r\n//\r\n\r\n#include \"stdafx.h\"\r\n#include <stdio.h>\r\n#include <vector>\r\n#include <algorithm>\r\n#include <boost/timer/timer.hpp>\r\n#include <boost/thread.hpp>\r\n\r\n\tstd::vector<int> numbers;\t// vector to contain 10,000,000 numbers to be sorted as dummy work.\r\n\tvoid timing_test_2(void);\t// function prototype for thread worker\r\n\r\n\t\tclass Comp_modulus\t// functor for comparing intergers mod k\r\n\t{\r\n\t\tpublic:\r\n\r\n\t\tint k;\t// k value to calculate modulus\r\n\r\n\t\tbool operator()(const std::vector<int>::iterator &x, const std::vector<int>::iterator &y)\r\n\t\t{\r\n\t\t\treturn ( ( (*x) % k ) < ( (*y) % k )  );\t// compare the integers by their modulus rather than by their\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t// .... values to sort in strange ways. It is iterators that\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t// .... are being sorted, not the integers themselves.\r\n\t\t}\r\n\t};\r\n\r\nint main(void)\r\n{\r\n\tint N = 16;\t\t\t\t\t// number of threads to create\r\n\tnumbers.resize(10000000);\t// storage for 10,000,000 numbers\r\n\r\n\tint i = 0;\r\n\tfor (std::vector<int>::iterator it = numbers.begin(); it != numbers.end(); it++)\r\n\t\t(*it) = i++; // fill in the numbers 0 to 9,999,999 which will be sorted in a variety of strange ways.\r\n\r\n\tstd::cout << \"beginning timing test, number of threads = \" << std::setw(2) << (int)N << std::endl;\t\r\n\r\n\tboost::timer::auto_cpu_timer auto_t;\t// Quick and dirty overall timer. It prints automatically\r\n\t\t\t\t\t\t\t\t\t\t\t// .... when it is destructed at the end of the program.\r\n\r\n\tboost::thread_group tg;\t\t\t\t\t\t\t\t\t// Define a thread group in preparation ....\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// .... for creating the threads\t\r\n\r\n\tfor (int i =  0; i < N; i++)\r\n\t{\r\n\t\ttg.create_thread( boost::bind( timing_test_2) );\t// Create the N threads\r\n\t}\r\n\r\n\ttg.join_all();\t\t\t\t\t\t\t\t\t\t\t// Wait until all the threads are done.\r\n\r\n\tstd::cout << \"completed timing test\" << std::endl;\r\n\r\n\treturn 0;\r\n}\r\n\r\nvoid timing_test_2(void)\t\t// this is the thread that creates the artificial load to simulate work.\r\n{\r\n\tstd::vector<std::vector<int>::iterator> numbers_p;\t// storage for iterators (pointers) which point to the array of integers\r\n\tnumbers_p.resize( numbers.size() );\t\t\t\t\t// allocate the correct amount of storage, same number of iterators\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t// ... as integers to which they point.\r\n\r\n\tstd::vector<int>::iterator it = numbers.begin();\t// point to the beginning of the integer array.\r\n\r\n\tfor (std::vector<std::vector<int>::iterator>::iterator it_it = numbers_p.begin(); it_it != numbers_p.end(); it_it++)\r\n\t\t(*it_it) = it++;\t// populate the iterator vector with iterators pointing the the integers\r\n\r\n\tComp_modulus comp_modulus;\t// instantiate the functor which does strange compares\r\n\r\n\t// create the dummy work\r\n\r\n\tcomp_modulus.k =  2; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k =  3; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k =  5; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k =  7; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 11; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 13; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 17; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 19; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 23; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\tcomp_modulus.k = 29; std::sort(numbers_p.begin(), numbers_p.end(), comp_modulus );\r\n\r\n}\n\r\nAs you can see, it's C++. I compile on Windows with Visual Studio 2010. Hence, I use boost for some timing and multi-threading functions. If you don't have boost installed and if you have a new enough C++ compiler, it would be straightforward to accomplish the timing and multi-threading with C++ standard library functions.\n\r\nWhat my test program does looks pretty strange, but it actually mirrors quite well what my real program does. The test program first creates an array (std::vector) containing 10,000,000 integers in the range of 0 to 9,999,999. It then creates an artificial workload for the purposes of timing by sorting the integers by various prime moduli. For example, sorting by a modulus 2 sorts all the even numbers in front of all the odd numbers. Using prime numbers for the moduli makes sure the numbers get scrambled pretty thoroughly and that std::sort has plenty of work to do.\n\r\nThe numbers being sorted are stored only a single time and conceptually are sorted simultaneously by each thread. However, there is no locking between threads and the numbers aren't really moved around by the sort at all. Rather, this magic is achieved by each thread creating it's own private set of pointers to the integers (iterators in C++ standard library terms). Then it's the pointers that are sorted rather than the integers.\n\r\nIn the timing test program, the integers are 32 bit and the pointers are 64 bit, so each thread could just as well have its own copy of the integers and sort them directly instead of having pointers and sorting the program. In my real program, the integers become 32 byte permutations and it's much more memory efficient for the threads to work with 64 bit pointers (8 bytes) than to work with 32 byte permutations. It's also much faster to sort items that are 8 bytes long than to sort items that are 32 bytes long.\n\r\nThe application is computational group theory and the problem is to attempt to enumerate an optimal solution for every single one of the possible positions of Rubik's Cube. Finding an optimal solution for a position is much more computationally intense than is simply finding a solution for a position. There are about 4.3 x 10^19 Rubik's cube positions, so it's an extremely non-trivial problem. It's ultimate solution will undoubtedly require many hundred if not thousands or hundreds of thousands of machines working on the problem in a piece-wise fashion.\n\r\nHere are the results of running the dummy timing program on my ancient server. The test program is so simple minded that I had to run it 16 different times, changing the number of threads each time. You can see how the wall clock and cpu times do not scale up as you might hope. For example, with one thread the wall clock time and the cpu time are both about 24 seconds. So one would hope would be that with two threads the wall clock time would be about 24 seconds and that the cpu time would be about 48 seconds. But the real figures are about 26.5 seconds and about 53 seconds, respectively. The problem only gets worse with more threads. The same thing happens on my Windows machine. And the same thing happens if you get rid of the threads and just run multiple instances of a one thread program.\n\r\nJerry Bryan\n\n \n\r\nbeginning timing test, number of threads =  1\r\ncompleted timing test\r\n 24.109956s wall, 23.990000s user + 0.080000s system = 24.070000s CPU (99.8%)\r\n\r\nbeginning timing test, number of threads =  2\r\ncompleted timing test\r\n 26.558628s wall, 52.820000s user + 0.210000s system = 53.030000s CPU (199.7%)\r\n\r\nbeginning timing test, number of threads =  3\r\ncompleted timing test\r\n 27.632885s wall, 82.410000s user + 0.310000s system = 82.720000s CPU (299.4%)\r\n\r\nbeginning timing test, number of threads =  4\r\ncompleted timing test\r\n 33.766357s wall, 120.150000s user + 0.390000s system = 120.540000s CPU (357.0%)\r\n\r\nbeginning timing test, number of threads =  5\r\ncompleted timing test\r\n 34.820234s wall, 152.480000s user + 0.510000s system = 152.990000s CPU (439.4%)\r\n\r\nbeginning timing test, number of threads =  6\r\ncompleted timing test\r\n 33.279489s wall, 183.450000s user + 0.680000s system = 184.130000s CPU (553.3%)\r\n\r\nbeginning timing test, number of threads =  7\r\ncompleted timing test\r\n 35.896250s wall, 235.820000s user + 0.890000s system = 236.710000s CPU (659.4%)\r\n\r\nbeginning timing test, number of threads =  8\r\ncompleted timing test\r\n 36.156149s wall, 275.370000s user + 1.170000s system = 276.540000s CPU (764.8%)\r\n\r\nbeginning timing test, number of threads =  9\r\ncompleted timing test\r\n 38.529383s wall, 319.610000s user + 1.540000s system = 321.150000s CPU (833.5%)\r\n\r\nbeginning timing test, number of threads = 10\r\ncompleted timing test\r\n 40.542099s wall, 367.840000s user + 1.970000s system = 369.810000s CPU (912.2%)\r\n\r\nbeginning timing test, number of threads = 11\r\ncompleted timing test\r\n 45.129656s wall, 421.800000s user + 2.340000s system = 424.140000s CPU (939.8%)\r\n\r\nbeginning timing test, number of threads = 12\r\ncompleted timing test\r\n 44.265782s wall, 485.820000s user + 2.650000s system = 488.470000s CPU (1103.5%)\r\n\r\nbeginning timing test, number of threads = 13\r\ncompleted timing test\r\n 45.339959s wall, 545.310000s user + 3.190000s system = 548.500000s CPU (1209.7%)\r\n\r\nbeginning timing test, number of threads = 14\r\ncompleted timing test\r\n 45.510356s wall, 587.020000s user + 3.660000s system = 590.680000s CPU (1297.9%)\r\n\r\nbeginning timing test, number of threads = 15\r\ncompleted timing test\r\n 47.840413s wall, 651.430000s user + 4.310000s system = 655.740000s CPU (1370.7%)\r\n\r\nbeginning timing test, number of threads = 16\r\ncompleted timing test\r\n 49.448533s wall, 731.640000s user + 4.400000s system = 736.040000s CPU (1488.5%)\n", "id": "2_1244571_1", "owner_tier": 0.1, "score": 0}, {"content": "\nQuote:I am perhaps naive, but I really did hope for nearly a 16x speedup on my ancient server (Dell R900) with 16 processor cores.\r\nthe speedup can only apply when the task is split between the threads, each doing only a part of the job in parallel.\r\nAs far as I can see, in your code, each thread is doing the full job.\nQuote:I really do not believe my program has a multi-threading design problem (but of course it's certainly possible it does). Therefore, I decided to try to create a complete dummy program which illustrates the problem and which is small enough to post. Here it is.\r\nIn your program, each thread is sorting a 10,000,000 array 10 times. This kind of activity is trashing the cpu cache. So the runtime is doe to continuous cache miss, thus preventing the cpu from taking advantage of cores.\n\r\nBy the way, the thread do not share memory, each thread works on its own array of 10,000,000, and is sorting it 10 times.\r\n", "id": "2_1244605_2", "owner_tier": 0.5, "score": 1.0}], "link": "https://www.codeproject.com/Questions/1244387/Multithreading-slowdown-CPU-bound-app-probably-har", "question": {"content": "\r\n\t\t\t    The exact details of my multi-theaded application probably don't matter. It's completely CPU bound (runs for hours or days at a time), does no I/O, there is ample memory for all the threads, and the threads barely communicate with each other (for all practical purposes they don't communicate with each other). There are many sub-problems to be solved by the computation, each of which runs for several hours, and the only purpose of multi-threading is to keep all the processor cores busy all the time. The threads do share some large blocks of memory, but the shared memory is only read and is never written into. Except for the sharing of the large block of memory which is only read, the application could just as easily be a separate \"job\" or \"task\" for each processor core.\n\r\nThe problem is that the CPU time to solve each sub-problem goes up with the multi-threading. For example, suppose there is a small test problem that requires 250 CPU seconds to run as a single thread (and also 250 wall clock seconds). Running two such sub-problems at the same time as two threads might take 270 CPU seconds for each thread for a total 540 CPU seconds and 270 wall clock seconds. In a perfect world where everything \"just works\", the test of two sub-problems at the same time would require 250 CPU seconds for each thread for a total of 500 CPU seconds and 250 wall clock seconds.\n\r\nIncreasing the number of simultaneous sub-problems and threads up to the number of processor cores exacerbates the apparent slowdown. It's always faster overall to add threads up to the number of processor cores, but the use of n processor cores overall do not produce an n times speedup. For example, with a 16 processor core example, using 16 threads only produces about 8 or 9 times as much computation.\n\r\nThere seems obviously to be some sort of hardware cause such as contention on the memory bus or contention between the multiple processor cores on the same processor chip, etc. Is this a known problem? Is there anything I can do about it?\n\nWhat I have tried:\n\r\nOne possible problem is that I run some of the speed tests on a high end laptop with a single quad-core processor. Running all the processor cores at 100% generates heat which causes laptop power management to intervene and slow down the processor clock speeds because slower clock speeds generate less heat. I have tweaked power management settings to no avail. I also have an ancient server with 64GB memory and four quad-core processors, for a total of 16 cores. Running speed tests on this machine produce results that are essentially equivalent to the results on the laptop. So laptop heat and power management may be an issue, but if so it does not seem to be the main issue. Depending on the exact size of the speed test that I run, it can be slightly faster to run 16 separate jobs with one thread each or it can be slightly faster to run 1 big job with sixteen threads. The contention therefore does not seem to be in my application. Rather, it seems to be in the hardware. I run under Windows 10 on the laptop and under Wine under Linux on the server.\r\n\t\t    ", "id": "1244387", "title": "Multithreading slowdown, CPU bound app, probably hardware related", "traffic_rate": 0}, "saved_time": {"$date": "2024-07-16T03:49:26.927Z"}, "source": "codeproject", "tags": ["threads"]}, {"answers": [{"content": "If the files are structured, kinda configuration files, it might be good to use ConfigParser library, else if you have other structural format then I think it would be better to store all this data in JSON or XML and perform any necessary operations on your data", "id": 3006895, "owner_tier": 0.5, "score": -3.3333333333333334e-09}, {"content": "Call the open to the file from the calling method of the one you want to run. Pass the data as parameters to the method", "id": 3006875, "owner_tier": 0.5, "score": -3.3333333333333334e-09}, {"content": "As a general strategy, it's best to keep this data in an in-memory cache if it's static, and relatively small. Then, the 10k calls will read an in-memory cache rather than a file. Much faster. If you are modifying the data, the alternative might be a database like SQLite, or embedded MS SQL Server (and there are others, too!). It's not clear what kind of data this is. Is it simple config/properties data? Sometimes you can find libraries to handle the loading/manipulation/storage of this data, and it usually has it's own internal in-memory cache, all you need to do is call one or two functions. Without more information about the files (how big are they?) and the data (how is it formatted and structured?), it's hard to say more.", "id": 3006810, "owner_tier": 0.9, "score": 0.9999999966666667}, {"content": "It might be better to load your data into a database and put some indexes on the database. Then it will be very fast to make simple queries against your data. You don't need a lot of work to set up a database. You can create an SQLite database without requiring a separate process and it doesn't have a complicated installation process.", "id": 3006804, "owner_tier": 0.9, "score": 0.33333333}, {"content": "Opening, closing, and reading a file 10,000 times is always going to be slow.  Can you open the file once, do 10,000 operations on the list, then close the file once?", "id": 3006800, "owner_tier": 0.9, "score": 0.6666666633333334}], "link": "https://stackoverflow.com/questions/3006769/how-to-speed-up-the-code", "question": {"content": "in my program i have a method which requires about 4 files to be open each time it is called,as i require to take some data.all this data from the file i have been storing in list for manupalation.\nI approximatily need to call this method about 10,000 times.which is making my program very slow? any method for handling this files in a better ways and is storing the whole data in list time consuming what is better alternatives for list? I can give some code,but my previous question was closed as that only confused everyone as it is a part of big program and need to be explained completely to understand,so i am not giving any code,please suggest ways thinking this as a general question... thanks in advance", "id": 3006769, "title": "how to speed up the code?", "traffic_rate": 262}, "saved_time": 1721101902, "source": "stackoverflow", "tags": ["python", "optimization"]}, {"answers": [{"content": "If the files are structured, kinda configuration files, it might be good to use ConfigParser library, else if you have other structural format then I think it would be better to store all this data in JSON or XML and perform any necessary operations on your data", "id": 3006895, "owner_tier": 0.5, "score": -3.3333333333333334e-09}, {"content": "Call the open to the file from the calling method of the one you want to run. Pass the data as parameters to the method", "id": 3006875, "owner_tier": 0.5, "score": -3.3333333333333334e-09}, {"content": "As a general strategy, it's best to keep this data in an in-memory cache if it's static, and relatively small. Then, the 10k calls will read an in-memory cache rather than a file. Much faster. If you are modifying the data, the alternative might be a database like SQLite, or embedded MS SQL Server (and there are others, too!). It's not clear what kind of data this is. Is it simple config/properties data? Sometimes you can find libraries to handle the loading/manipulation/storage of this data, and it usually has it's own internal in-memory cache, all you need to do is call one or two functions. Without more information about the files (how big are they?) and the data (how is it formatted and structured?), it's hard to say more.", "id": 3006810, "owner_tier": 0.9, "score": 0.9999999966666667}, {"content": "It might be better to load your data into a database and put some indexes on the database. Then it will be very fast to make simple queries against your data. You don't need a lot of work to set up a database. You can create an SQLite database without requiring a separate process and it doesn't have a complicated installation process.", "id": 3006804, "owner_tier": 0.9, "score": 0.33333333}, {"content": "Opening, closing, and reading a file 10,000 times is always going to be slow.  Can you open the file once, do 10,000 operations on the list, then close the file once?", "id": 3006800, "owner_tier": 0.9, "score": 0.6666666633333334}], "link": "https://stackoverflow.com/questions/3006769/how-to-speed-up-the-code", "question": {"content": "in my program i have a method which requires about 4 files to be open each time it is called,as i require to take some data.all this data from the file i have been storing in list for manupalation.\nI approximatily need to call this method about 10,000 times.which is making my program very slow? any method for handling this files in a better ways and is storing the whole data in list time consuming what is better alternatives for list? I can give some code,but my previous question was closed as that only confused everyone as it is a part of big program and need to be explained completely to understand,so i am not giving any code,please suggest ways thinking this as a general question... thanks in advance", "id": 3006769, "title": "how to speed up the code?", "traffic_rate": 262}, "saved_time": "Tue, 16 Jul 2024 03:51:42 GMT", "source": "stackoverflow", "tags": ["python", "optimization"]}]}