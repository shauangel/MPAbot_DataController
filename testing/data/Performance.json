{"result": [{"answers": [{"content": "I think using threading wouldnot be really a bad idea", "id": 77491031, "score": 0}, {"content": "I have done just a basic test below, but it shows that runpy can be used to solve this issue when you need to have a whole Python script to be faster (you don't want to put any logic in test_server.py). test_server.py test_client.py test_lag.py Testing: Based on this, module is pre-loaded for fast usage.", "id": 74382868, "score": 1}, {"content": "You can use lazy imports, but it depends on your use case. If it's an application, you can run necessary modules for GUI, then after window is loaded, you can import all your modules. If it's a module and user do not use all the dependencies, you can import inside function. [warning]\nIt's against pep8 i think and it's not recomennded at some places, but all the reason behind this is mostly readability (i may be wrong though...) and some builders (e.g. pyinstaller) bundling (which can be solved with adding missing dependencies param to spec) If you use lazy imports, use comments so user knows that there are extra dependencies. Example: For some specific libraries i think it's necessity. You can also create some let's call it api in __init__.py For example on scikit learn. If you import sklearn and then call some model, it's not found and raise error. You need to be more specific then and import directly submodule. Though it can be unconvenient for users, it's imho good practice and can reduce import times significantly. Usually 10% of imported libraries cost 90% of import time. Very simple tool for analysis is line_profiler This give results 75% of three libraries imports time is matplotlib (this does not mean that it's bad written, it just needs a lot of stuff for grafic output) Note: If you import library in one module, other imports cost nothing, it's globally shared... Another note: If using imports directly from python (e.g pathlib, subprocess etc.) do not use lazy load, python modules import times are close to zero and don't need to be optimized from my experience...", "id": 70496615, "score": 4}, {"content": "Not an actual answer to the question, but a hint on how to profile the import speed with Python 3.7 and tuna (a small project of mine): ", "id": 51300944, "score": 92}, {"content": "You can import your modules manually instead, using imp. See documentation here. For example, import numpy as np could probably be written as This will spare python from browsing your entire sys.path to find the desired packages. See also: Manually importing gtk fails: module not found", "id": 36120472, "score": 9}, {"content": "1.35 seconds isn't long, but I suppose if you're used to half that for a \"quick check\" then perhaps it seems so. Andrea suggests a simple client/server setup, but it seems to me that you could just as easily call a very slight modification of your script and keep it's console window open while you work: I assume your script is identical every time, ie you don't need to give it image stack location or any particular commands each time (but these are easy to do as well!).  Example RAAC's_Script.py: To end the script, close the console window or press ctrl+c. I've made this as simple as possible, but it would require very little extra to handle things like quitting nicely, doing slightly different things based on input, etc.", "id": 25547013, "score": 3}, {"content": "you could build a simple server/client, the server running continuously making and updating the plot, and the client just communicating the next file to process. I wrote a simple server/client example based on the basic example from the socket module docs: http://docs.python.org/2/library/socket.html#example here is server.py: and client.py: you just run the server: which does the imports, then the client just sends via the socket the filename of the new file to plot: then the server updates the plot. On my machine running your imports take 0.6 seconds, while running client.py 0.03 seconds.", "id": 16430894, "score": 28}], "link": "https://stackoverflow.com/questions/16373510/improving-speed-of-python-module-import", "question": {"content": "The question of how to speed up importing of Python modules has been asked previously (Speeding up the python \"import\" loader and Python -- Speed Up Imports?) but without specific examples and has not yielded accepted solutions. I will therefore take up the issue again here, but this time with a specific example.  I have a Python script that loads a 3-D image stack from disk, smooths it, and displays it as a movie. I call this script from the system command prompt when I want to quickly view my data. I'm OK with the 700 ms it takes to smooth the data as this is comparable to MATLAB. However, it takes an additional 650 ms to import the modules. So from the user's perspective the Python code runs at half the speed. This is the series of modules I'm importing: Of course, not all modules are equally slow to import. The chief culprits are: I have experimented with using from, but this isn't any faster. Since Matplotlib is the main culprit and it's got a reputation for slow screen updates, I looked for alternatives. One is PyQtGraph, but that takes 550 ms to import.   I am aware of one obvious solution, which is to call my function from an interactive Python session rather than the system command prompt. This is fine but it's too MATLAB-like, I'd prefer the elegance of having my function available from the system prompt.  I'm new to Python and I'm not sure how to proceed at this point. Since I'm new, I'd appreciate links on how to implement proposed solutions. Ideally, I'm looking for a simple solution (aren't we all!) because the code needs to be portable between multiple Mac and Linux machines. ", "id": 16373510, "title": "improving speed of Python module import", "views": 60036}, "resource": "StackOverflow", "tags": ["python", "performance", "import", "module"]}, {"answers": [{"content": "I had the same slow launch issue, and I ended up using the \"--onedir\" option instead,but creating a shortcut to a folder above the distribution folder created by pyinstaller so I could easily find the executable. the following is the batch command \"build.bat\" --", "id": 77093837, "score": 1}, {"content": "I solved this by adding an exception to the anti-virus monitoring software (F-Secure). It removed the wait of several minutes before running.", "id": 68760139, "score": 3}, {"content": "In case anyone is still have this issue, I resolved mine by running the exe locally and not on any sharedrives. This took the startup time from over a minute to under 10 seconds.", "id": 57366288, "score": 4}, {"content": "For my application, the long startup time almost entirely was caused by the antivirus system. Switching it off reduced the startup in my case from 3 minutes to less than 10secs! To bring these measurements into perspective: My application was bundled with extra data files (about 150 files with a payload of 250MB), besides carrying around Qt and numpy (that may depend on Intel MKL, which alone adds another 200MB to the bundle!) dependencies. It even didn't help much that the tested system was running with a solid state drive... In conclusion: if you have a large application with lots of dependencies, the startup time may be affected strongly by the antivirus system!", "id": 49350571, "score": 4}, {"content": "I agree with above answers. My Qt python program needed about 5 seconds to start up on a decent PC when using onefile mode. After I change to --onedir, it only took around one second to start; almost immediately after user double clicks the exe file. But the drawback is that there are  many files in that directory which is not so neat.", "id": 15892172, "score": 17}, {"content": "Tell PyInstaller to create a console-mode executable. This gives you a working console you can use for debugging. At the top of your main script, even before the first import is run, add a print \"Python Code starting\". Then run your packaged executable from the command line. This way you can get a clear picture whether the time is spent in PyInstaller's bootloader or in your application. PyInstaller's bootloader is usually quite fast in one-dir mode, but it can be much slower in one-file mode, because it depacks everything into a temporary directory. On Windows, I/O is very slow, and then you have antiviruses that will want to double check all those DLL files. PyQt itself is a non-issue. PyQt is generated by SIP which generates very fast lazy bindings; importing the whole PyQt is faster than any other GUI library because it basically does nothing: all bindings to classes/functions are dynamically created when (and if!) you access them, saving lots of memory too. If your application is slow at coming up, that will be true without PyInstaller as well. In that case, your only solution is either a splash screen (import just PyQt, create QApplication, create a display the splashscreen, then import the rest of your program and run it), or rework your code. I can't help you much without details.", "id": 9474575, "score": 53}, {"content": "I suspect that you're using pyinstaller's \"one file\" mode -- this mode means that it has to unpack all of the libraries to a temporary directory before the app can start. In the case of Qt, these libraries are quite large and take a few seconds to decompress. Try using the \"one directory\" mode and see if that helps?", "id": 9470393, "score": 91}, {"content": "I have 'compiled' a few wxPython apps using py2exe and cx_Freeze, None of them take more than 4 seconds to start. I never used pyQT, but with wxPython the startup speed is OK, and after the first initialize if I close and open again, it's faster than the first time.", "id": 9470332, "score": 0}], "link": "https://stackoverflow.com/questions/9469932/app-created-with-pyinstaller-has-a-slow-startup", "question": {"content": "I have an application written in Python and 'compiled' with PyInstaller. It also uses PyQt for the GUI framework. Running this application has a delay of about 10 seconds before the main window loads and is shown. As far as I can tell, this is not due to slowness in my code. Instead, I suspect this is due to the Python runtime initializing. The problem is that this application is started with a custom laucncher / taskbar application. The user will click the button to launch the app, see nothing appear to happen, and click elsewhere on another application. When my application shows it's window, it cannot come to the foreground due to the rules for SetForegroundWindow. I have access to the source for the PyInstaller win32 loader, the Python code, and even the launcher code. My questions are: How can I make this application start faster? How can I measure the time spend i the first few seconds of the process's lifetime? What is the generally accepted technique for reducing time until the first window is shown? I'd like to avoid adding a splash screen for two reasons - one, I expect it won't help (the overhead is before Python code runs) and two, I just don't like splash screens :) If I need to, I could probably edit the PyInstaller loader stub to create a window, but that's another route i'd rather not take.", "id": 9469932, "title": "App created with PyInstaller has a slow startup", "views": 91382}, "resource": "StackOverflow", "tags": ["python", "windows", "performance", "pyinstaller"]}, {"answers": [{"content": "This fixed it for me, run it as user root:", "id": 76452743, "score": 0}, {"content": "I don't know about PHP but for python3 I think I have a solution: The slow initial startup seems to be related to python3 being bundelnd with Xcode. After installing Xcode calling python3 resolves to /usr/bin/python3 which when called for the first time after reboot seems to check which python3 framework to use. (I'm not entirely sure what it really does though, but it seems to call Xcode-select for some reason?!?) This can take a very long time. Eventually it settles for calling a python3 framework from the Xcode installation e.g. \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/Current/bin/python3.9\" To circumvent this initial check I added a symlink to \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/Current/bin/python3.9\" under /usr/local/bin/python3 Since one cannot edit /usr/bin because of system protection this symlink has to go to /usr/local/bin/. Then I made sure that /usr/local/bin comes before /usr/bin in my $path variable and now a call to python3 resolves to /usr/local/bin/python3 which in turn directly resolves to the selected python3 framework from the Xcode installation.\nOne can use ll $(which python3) to check what gets called. This now is again as fast as one can expect it to be.", "id": 74920329, "score": 1}], "link": "https://stackoverflow.com/questions/61046565/php-and-python-cli-programs-start-too-slow-for-the-first-time-macos", "question": {"content": "I work on Mac with Python and PHP.\nFrom some period of time (I didn't notice exactly since when) all new scripts have started to launch very slowly. I mean, even a simple hello-world program on Python or PHP works about 5-6 seconds for the first time. After the first launch, if NOT changing anything for the second time, the program works fast. But if changing even one letter in \"hello world\" string it compiles as long as it did for the first time. I do remember that is wasn't like this before. Don't know how to fix it.", "id": 61046565, "title": "PHP and Python CLI programs start too slow for the first time (MacOS)", "views": 509}, "resource": "StackOverflow", "tags": ["python", "php", "macos", "command-line-interface"]}, {"answers": [{"content": "You can do a lot better by changing the structure of the core to use a matrix of \"alive\" cells represented by a numpy array of 0/1.  If you do this, you can chop out the double loop you have and let numpy do the heavy lifting of neighbor counting.  You could do some work to index that and get summations, but the 2nd enhancement is to use the process of \"convolution with a kernel\" which is very common in image processing.  Basically you are taking a kernel (in this case a 3x3 kernel to hit the neighbors) and multiplying that by the appropriate grid in your matrix for every member.  scipy has an image processing package that makes this a snap.  Note that you can set up the kernel to ignore out-of-bounds by filling it with zero (zero padding is the term.) I modified your neighbor counting function a bit to separate it from the cell class for comparison, but didn't change any of the guts. In a 500 x 500 structure, your approach takes about 1.3 seconds to calculate the number of living neighbors.  So that looping costs roughly 1 second per frame to do.  Using convolution, I can get it done in 0.004 seconds, around a 1000x speed-up. Note that when I compared results I got a failed comparison because I don't think your modulo arithmetic is accurate for the edges (it is wrapping--not desired.)  You can see it with my code if you print the first neighbor matrix. After you \"neighbor count\" you can use a couple numpy functions to figure out what the alive matrix looks like for the next evolution and just incorporate that in your code.", "id": 76444193, "score": 2}, {"content": "The simplest fix you can set for yourself is to pre-calculate your cell neighbors rather than running the calculation within an O(n^2) complexity. The modulo operation can be one of the slower operations to perform, so minimizing the number of times you run it will be good. Ultimately you want to minimize or eliminate regular O(n^2) calculations to minimize CPU strain. Given that you\u2019re not doing anything super complicated here, I think this is a simple and quick performance gain you can realize. Remember that Python passes values by reference, so just initialize your neighbors in __init__ rather than passing them to calculateNewState. If you\u2019re running into performance issues, a good helper is dis.dis to see the steps in assembly code. Find your bottlenecks as each step is something your CPU needs to manage. Then, if you can remove it, your code will have fewer steps and run faster. Sometimes without seeing the low level code, it can be hard to understand where your bottlenecks really are.", "id": 76440738, "score": 1}, {"content": "You could try importing and using threading so that each task that doesn't necessarily need to be completed in order can be done simultaneously and speed things up.\nIt could look something like this", "id": 76440497, "score": -1}], "link": "https://stackoverflow.com/questions/76440408/my-code-run-very-very-slow-how-can-i-make-my-python-code-that-runs-faster", "question": {"content": "please help me. I wrote a program to run \"Game of Life\" with PyQt6 but, it runs very very slow. How can I make it faster? main.py cell.py I want to write code that runs as fast as possible", "id": 76440408, "title": "My code run very very slow. How can I make my python code that runs faster?", "views": 260}, "resource": "StackOverflow", "tags": ["python", "gpu", "cpu", "pyqt6", "conways-game-of-life"]}, {"answers": [{"content": "I just contacted support: apparently this is a known bug caused by a newer Java runtime. There's a workaround here which I'll reproduce below:", "id": 62615216, "score": 1}], "link": "https://stackoverflow.com/questions/62394613/how-to-speed-up-slow-suggestions-in-pycharm", "question": {"content": "After update to PyCharm Pro 2020.01.02 I spotted a significant regress of speed of completion. Sometimes it works well, the same as before update, but sometimes it`s too slow. Maybe someone faced the same issue and knows how to solve it? My OS is Mac OS 10.15.5", "id": 62394613, "title": "How to speed up slow suggestions in PyCharm?", "views": 393}, "resource": "StackOverflow", "tags": ["macos", "performance", "autocomplete", "pycharm"]}, {"answers": [{"content": "How can it be that writing to physical disk is WAY faster than writing to the \"screen\" (presumably an all-RAM op), and is effectively as fast as simply dumping to the garbage with /dev/null? Congratulations, you have just discovered the importance of I/O buffering. :-) The disk appears to be faster, because it is highly buffered: all Python's write() calls are returning before anything is actually written to physical disk. (The OS does this later, combining many thousands of individual writes into a big, efficient chunks.) The terminal, on the other hand, does little or no buffering: each individual print / write(line) waits for the full write (i.e. display to output device) to complete. To make the comparison fair, you must make the file test use the same output buffering as the terminal, which you can do by modifying your example to: I ran your file writing test on my machine, and with buffering, it also 0.05s here for 100,000 lines. However, with the above modifications to write unbuffered, it takes 40 seconds to write only 1,000 lines to disk. I gave up waiting for 100,000 lines to write, but extrapolating from the previous, it would take over an hour. That puts the terminal's 11 seconds into perspective, doesn't it? So to answer your original question, writing to a terminal is actually blazingly fast, all things considered, and there's not a lot of room to make it much faster (but individual terminals do vary in how much work they do; see Russ's comment to this answer). (You could add more write buffering, like with disk I/O, but then you wouldn't see what was written to your terminal until after the buffer gets flushed. It's a trade-off: interactivity versus bulk efficiency.)", "id": 3857543, "score": 190}, {"content": "Thanks for all the comments!  I've ended up answering it myself with your help.  It feels dirty answering your own question, though. Question 1: Why is printing to stdout slow? Answer: Printing to stdout is not inherently slow.  It is the terminal you work with that is slow.  And it has pretty much zero to do with I/O buffering on the application side (eg: python file buffering).  See below. Question 2: Can it be sped up? Answer: Yes it can, but seemingly not from the program side (the side doing the 'printing' to stdout).  To speed it up, use a faster different terminal emulator. Explanation... I tried a self-described 'lightweight' terminal program called wterm and got significantly better results.  Below is the output of my test script (at the bottom of the question) when running in wterm at 1920x1200 in on the same system where the basic print option took 12s using gnome-terminal: 0.26s is MUCH better than 12s!  I don't know whether wterm is more intelligent about how it renders to screen along the lines of how I was suggesting (render the 'visible' tail at a reasonable frame rate), or whether it just \"does less\" than gnome-terminal.  For the purposes of my question I've got the answer, though.  gnome-terminal is slow. So - If you have a long running script that you feel is slow and it spews massive amounts of text to stdout... try a different terminal and see if it is any better! Note that I pretty much randomly pulled wterm from the ubuntu/debian repositories.  This link might be the same terminal, but I'm not sure.  I did not test any other terminal emulators. Update: Because I had to scratch the itch, I tested a whole pile of other terminal emulators with the same script and full screen (1920x1200).  My manually collected stats are here: The recorded times are manually collected, but they were pretty consistent.  I recorded the best(ish) value.  YMMV, obviously. As a bonus, it was an interesting tour of some of the various terminal emulators available out there!  I'm amazed my first 'alternate' test turned out to be the best of the bunch.", "id": 3860319, "score": 109}, {"content": "In addition to the output probably defaulting to a line-buffered mode, output to a terminal is also causing your data to flow into a terminal and serial line with a maximum throughput, or a pseudo-terminal and a separate process that is handling a display event loop, rendering characters from some font, moving display bits to implement a scrolling display.  The latter scenario is probably spread over multiple processes (e.g. telnet server/client, terminal app, X11 display server) so there are context switching and latency issues too.", "id": 3857378, "score": 2}, {"content": "Your redirection probably does nothing as programs can determine whether their output FD points to a tty. It's likely that stdout is line buffered when pointing to a terminal (the same as C's stdout stream behaviour). As an amusing experiment, try piping the output to cat. I've tried my own amusing experiment, and here are the results.", "id": 3857154, "score": 14}, {"content": "I can't talk about the technical details because I don't know them, but this doesn't surprise me: the terminal was not designed for printing lots of data like this. Indeed, you even provide a link to a load of GUI stuff that it has to do every time you want to print something! Notice that if you call the script with pythonw instead, it does not take 15 seconds; this is entirely a GUI issue. Redirect stdout to a file to avoid this:", "id": 3857173, "score": 4}, {"content": "Printing to the terminal is going to be slow. Unfortunately short of writing a new terminal implementation I can't really see how you'd speed this up significantly. ", "id": 3857108, "score": 3}], "link": "https://stackoverflow.com/questions/3857052/why-is-printing-to-stdout-so-slow-can-it-be-sped-up", "question": {"content": "I've always been amazed/frustrated with how long it takes to simply output to the terminal with a print statement.  After some recent painfully slow logging I decided to look into it and was quite surprised to find that almost all the time spent is waiting for the terminal to process the results. Can writing to stdout be sped up somehow? I wrote a script ('print_timer.py' at the bottom of this question) to compare timing when writing 100k lines to stdout, to file, and with stdout redirected to /dev/null.  Here is the timing result: Wow.  To make sure python isn't doing something behind the scenes like recognizing that I reassigned stdout to /dev/null or something, I did the redirection outside the script... So it isn't a python trick, it is just the terminal.  I always knew dumping output to /dev/null sped things up, but never figured it was that significant! It amazes me how slow the tty is.  How can it be that writing to physical disk is WAY faster than writing to the \"screen\" (presumably an all-RAM op), and is effectively as fast as simply dumping to the garbage with /dev/null? This link talks about how the terminal will block I/O so it can \"parse [the input], update its frame buffer, communicate with the X server in order to scroll the window and so on\"... but I don't fully get it.  What can be taking so long? I expect there is no way out (short of a faster tty implementation?) but figure I'd ask anyway. UPDATE: after reading some comments I wondered how much impact my screen size actually has on the print time, and it does have some significance.  The really slow numbers above are with my Gnome terminal blown up to 1920x1200.  If I reduce it very small I get... That is certainly better (~4x), but doesn't change my question.  It only adds to my question as I don't understand why the terminal screen rendering should slow down an application writing to stdout.  Why does my program need to wait for screen rendering to continue? Are all terminal/tty apps not created equal?  I have yet to experiment.  It really seems to me like a terminal should be able to buffer all incoming data, parse/render it invisibly, and only render the most recent chunk that is visible in the current screen configuration at a sensible frame rate.  So if I can write+fsync to disk in ~0.1 seconds, a terminal should be able to complete the same operation in something of that order (with maybe a few screen updates while it did it). I'm still kind of hoping there is a tty setting that can be changed from the application side to make this behaviour better for programmer.  If this is strictly a terminal application issue, then this maybe doesn't even belong on StackOverflow? What am I missing? Here is the python program used to generate the timing:", "id": 3857052, "title": "Why is printing to stdout so slow? Can it be sped up?", "views": 82743}, "resource": "StackOverflow", "tags": ["python", "linux", "printing", "stdout", "tty"]}, {"answers": [{"content": "Hey umm I had the same problem but now my pygame code runs at 60 fps which is good. I am using Idle with Python 3.6.3 and the appropriate pygame for it. Here is how I fixed it:", "id": 47585242, "score": 27}, {"content": "I suggest you read the documentation for pygame.time.Clock and, in particular, this: Note that this function uses SDL_Delay function which is not accurate on every platform, but does not use much cpu. Use tick_busy_loop if you want an accurate timer, and don\u2019t mind chewing cpu. The problem is likely due to issues with SDL_Delay() when you're calling self.clock.tick(self.fps) in the run function.", "id": 31686133, "score": 2}], "link": "https://stackoverflow.com/questions/31685936/pygame-application-runs-slower-on-mac-than-on-pc", "question": {"content": "A friend and I are making a game in Python (2.7) with the Pygame module. I have mostly done the art for the game so far and he has mostly done the coding but eventually I plan to help code with him once most of the art is done. I am on a Mac (latest version of OS X) and my friend is using a PC.  He has been building and running the game from his PC and as of right now it has been working as planned in his PC (perfect 60fps). However, whenever I pull the code from GitHub (I definitely have the most updated version of our code) and I try to run the game, the game runs like half as fast.  We have tried doubling the fps to 120 in the code and it then runs twice as fast on the PC but when I pull that code on my Mac it still seemed like I was capped around 30fps. We haven't really found any convincing answers to this problem anywhere else, however we are both pretty new to Pygame and Python so we may be missing something very obvious.", "id": 31685936, "title": "Pygame application runs slower on Mac than on PC", "views": 5024}, "resource": "StackOverflow", "tags": ["python", "python-2.7", "pygame"]}, {"answers": [{"content": "You could split it up into 4 loops: This will run n processes that each do 1/nth of the work, where n is the number of processors on your computer. The test_false_pos function has been modified to take three parameters: The function loops times times, then places i and sumA into the queue for further processing. The main thread (full_test) waits for each thread to complete, then places the results in the appropriate position in the results list. Once the list is complete, it is flattened, and the mean is calculated and returned.", "id": 39924339, "score": 3}, {"content": "Consider looking into Numba and Jit (just in time compiler).  It works for functions that are Numpy based.  It can handle some python routines, but is mainly for speeding up numerical calculations, especially ones with loops (like doing cholesky rank-1 up/downdates).  I don't think it would work with a BloomFilter, but it is generally super helpful to know about.  In cases where you must use other packages in your flow with numpy, separate out the heavy-lifting numpy routines into their own functions, and throw a @jit decorator on top of that function.  Then put them into your flows with normal python stuff.  ", "id": 39924499, "score": 1}, {"content": "Sure thing: Python 2.7 has to generate huge lists and waste a lot of memory each time you use range(<a huge number>).  Try to use the xrange function instead. It doesn't create that gigantic list at once, it produces the members of a sequence lazily.  But if your were to use Python 3 (which is the modern version and the future of Python), you'll find out that there range is even cooler and faster than xrange in Python 2.", "id": 39924228, "score": 4}], "link": "https://stackoverflow.com/questions/39924176/how-to-make-huge-loops-in-python-faster-on-mac", "question": {"content": "I am a computer science student and some of the things I do require me to run huge loops on Macbook with dual core i5. Some the loops take 5-6 hours to complete but they only use 25% of my CPU. Is there a way to make this process faster? I cant change my loops but is there a way to make them run faster? Thank you Mac OS 10.11\nPython 2.7 (I have to use 2.7) with IDLE or Spyder on Anaconda Here is a sample code that takes 15 minutes:", "id": 39924176, "title": "How to make huge loops in python faster on mac?", "views": 1872}, "resource": "StackOverflow", "tags": ["python", "performance", "python-2.7", "loops", "cpu-usage"]}, {"answers": [{"content": "It seems quite clear where the problem is, right now you got: cost(file) = 1.2s = 60ms + 1040ms, which means: cost(N*files) = N*1.2s now, why don't you change it to become: cost1(files) = 1040ms + N*60ms that way, theorically processing 100 files would be 7,04s instead 120s EDIT: Because I'm receiving downvotes to this question, I'll post a little example, let's assume you got this python file: The execution time is 1.3s on my box, now, if i do: I'll get 100*1.3s execution time, my proposal was turn foo.py into this: That way you're importing only once instead of 100 times", "id": 39100627, "score": 0}, {"content": "Write the template part as a separate process.  The first time \"script.py\" is run it would launch this separate process.  Once the process exists it can be passed the input/output filenames via a named pipe.  If the process gets no input for x seconds, it automatically exits.  How big x is depends on what your needs are So the parameters are passed to the long running process via the script.py writing to a named pipe.  The imports only occur once (provided the inputs are fairly often) and as BPL points out this would make everything run faster", "id": 39100929, "score": 1}, {"content": "It is not quite easy, but you could turn your program into one that sits in the background and processes commands to process a file. Another program could feed the processing commands to it and thus make the real start quite easy.", "id": 39100698, "score": 3}, {"content": "You could use glob to perform that actions with the files you need. Thus, you process all the files in the same script, instead of calling the script every time with every pair of files. At least that way you don't have to start python every time.", "id": 39100537, "score": 0}], "link": "https://stackoverflow.com/questions/39100391/how-to-improve-python-import-speed", "question": {"content": "This question has been asked many times on SO (for instance here), but there is no real answer yet. I am writing a short command line tool that renders templates. It is frigged using a Makefile:  In this dummy example, the Makefile parses every .in file to generate an .out file. It is very convenient for me to use make because I have a lot of other actions to trig before and after this script. Moreover I would like to remain as KISS as possible. Thus, I want to keep my tool simple, stupid and process each file separately using the syntax script -o out in My script uses the following:  The problem is that each execution costs me about 1.2s ( ~60ms for the processing and ~1140ms for the import directives): The overall execution of my Makefile for 100 files is ridiculous: ~100 files x 1.2s = 120s. This is not a solution, but this should be the solution.  What alternative can I use? EDIT I love Python because its syntax is readable and size of its community. In this particular case (command line tools), I have to admit Perl is still a decent alternative. The same script written in Perl (which is also an interpreted language) is about 12 times faster (using Text::Xslate).  I don't want to promote Perl in anyway I am just trying to solve my biggest issue with Python: it is not yet a suitable language for simple command line tools because of the poor import time. ", "id": 39100391, "title": "How to improve python import speed?", "views": 2120}, "resource": "StackOverflow", "tags": ["python", "performance", "python-import"]}, {"answers": [{"content": "Please consider trimming down your f_wo_append function: Edit in response to OP's comment \"\"\"This made it a lot worse! The trimmed version takes 4 times more time than the version I have posted above. \"\"\" There is no way that that could take \"4 times more\" (5 times?) ... here is my code, which demonstrates a significant reduction in the \"without append\" case, as I suggested, and also introduces a significant improvement in the \"with append\" case. and here are the results of running it (Python 2.7.1, Windows 7 Pro, \"Intel Core i3 CPU 540 @ 3.07 GHz\"): Edit 3 Why numpy takes longer: and here's why my f_wo_append2 function took 4 times longer: The empirical evidence is that these custom types aren't so fast when used as scalars ... perhaps because they need to reset the floating-point hardware each time they are used. OK for big arrays, not for scalars. Are you using any other numpy functionality? If not, just use the random module. If you have other uses for numpy, you may wish to coerce the numpy.float64 to float during the population setup.", "id": 4679413, "score": 5}, {"content": "There are many things you can try after optimizing your Python code for speed. If this program doesn't need C extensions, you can run it under PyPy to benefit from its JIT compiler. You can try making a C extension for possibly huge speedups. Shed Skin will even allow you to convert your Python program to a standalone C++ binary. I'm willing to time your program under these different optimization scenarios if you can provide enough code for benchmarking, Edit: First of all, I have to agree with everyone else: are you sure you're measuring the time correctly? The example code runs 100 times in under 0.1 seconds here, so there is a good chance the either the time is wrong or you have a bottleneck (IO?) that isn't present in the code sample. That said, I made it 300000 people so times were consistent. Here's the adapted code, shared by CPython (2.5), PyPy and Shed Skin: Running with PyPy is as simple as running with CPython, you just type 'pypy' instead of 'python'. For Shed Skin, you must convert to C++, compile and run: And here is the Cython-ized code: For building it, it's nice to have a setup.py like this one: You build it with:\n    python setupfaster.py build_ext --inplace Then test:\n    python -c \"import cymakefaster; print cymakefaster.file; cymakefaster.main()\"   Timings were run five times for each version, with Cython being the fastest and easiest of the code generators to use (Shed Skin aims to be simpler, but cryptic error messages and implicit static typing made it harder here). As for best value, PyPy gives impressive speedup in the counter version with no code changes.  Edit: Aaaand more meaningful timings, for 80000 calls with 300 people each: Shed Skin becomes fastest, PyPy surpasses Cython. All three speed things up a lot compared to CPython.", "id": 4654569, "score": 6}, {"content": "Depending on how often you add new elements to self.people or change person.utility, you could consider sorting self.people by the utility field. Then you could use a bisect function to find the lower index i_pivot where the person[i_pivot].utility >= price condition is met. This would have a lower complexity ( O(log N) ) than your exhaustive loop ( O(N) ) With this information, you could then update your people list if needed : Do you really need to update the utility field each time ? In the sorted case, you could easily deduce this value while iterating : for example, considering your list sorted in incresing order, utility = (index >= i_pivot) Same question with customers and nonCustomers lists. Why do you need them? They could be replaced by slices of the original sorted list : for example, customers = self.people[0:i_pivot] All this would allow you to reduce the complexity of your algorithm, and use more built-in (fast) Python functions, this could speedup your implementation.", "id": 4655176, "score": 2}, {"content": "This comment rings alarm bells: Aside from the fact that timePd is not used in the function, if you really want just to return the quantity, do just that in the function. Do the \"in addition\" stuff in a separate function.  Then profile again and see which of these two functions you are spending most of your time in. I like to apply SRP to methods as well as classes: it makes them easier to test.", "id": 4657252, "score": 2}, {"content": "It's surprising that the function shown is such a bottleneck because it's so relatively simple. For that reason, I'd double check my profiling procedure and results. However, if they're correct, the most time consuming part of your function has to be the for loop it contains, of course, so it makes sense to focus on speeding that up. One way to do this is by replacing the if/else with straight-line code. You can also reduce the attribute lookup for the append list method slightly. Here's how both of those things could be accomplished: That said, I must add that it seems a little redundant to have both a customer flag in each person object and also put each of them into a separate list depending on that attribute. Not creating these two lists would of course speed the loop up further.", "id": 4656399, "score": 1}, {"content": "You can eliminate some lookups by using local function aliases:", "id": 4654074, "score": 2}, {"content": "Some curious things I noted: timePd is passed as a parameter but never used price is an array but you only ever use the last entry - why not pass the value there instead of passing the list? count is initialized and never used self.people contains multiple person objects which are then copied to either self.customers or self.noncustomers as well as having their customer flag set.  Why not skip the copy operation and, on return, just iterate over the list, looking at the customer flag? This would save the expensive append. Alternatively, try using psyco which can speed up pure Python, sometimes considerably.", "id": 4653805, "score": 1}], "link": "https://stackoverflow.com/questions/4653715/increasing-speed-of-python-code", "question": {"content": "I have some python code that has many classes. I used cProfile to find that the total time to run the program is 68 seconds. I found that the following function in a class called Buyers takes about 60 seconds of those 68 seconds. I have to run the program about 100 times, so any increase in speed will help. Can you suggest ways to increase the speed by modifying the code? If you need more information that will help, please let me know. self.people is a list of  person objects. Each person has customer and utility as its attributes.  EDIT - responsed added -------------------------------------  Thanks so much for the suggestions. Here is the\nresponse to some questions and suggestions people have kindly\nmade. I have not tried them all, but will try others and write back later.  (1) @amber - the function is accessed 80,000 times.  (2) @gnibbler and others - self.people is a list of Person objects in memory. Not connected to a database. (3) @Hugh Bothwell  cumtime taken by the original function - 60.8 s (accessed 80000 times) cumtime taken by the new function with local function aliases as suggested - 56.4 s (accessed  80000 times) (4) @rotoglup and @Martin Thomas  I have not tried your solutions yet. I need to check the rest of the code to see the places where I use self.customers before I can make the change of not appending the customers to self.customers list. But I will try this and write back. (5) @TryPyPy - thanks for your kind offer to check the code.  Let me first read a little on the suggestions you have made to see if those will be feasible to use.  EDIT 2\nSome suggested that since I am flagging the customers and noncustomers in the self.people, I should try without creating separate lists of self.customers and self.noncustomers using append. Instead, I should loop over the self.people to find the number of customers. I tried the following code and timed both functions below f_w_append and f_wo_append. I did find that the latter takes less time, but it is still 96% of the time taken by the former. That is, it is a very small increase in the speed.  @TryPyPy - The following piece of code is complete enough to check the bottleneck function, in case your offer is still there to check it with other compilers.  Thanks again to everyone who replied. EDIT 3: It seems numpy is the problem This is in response to what John Machin said below. Below you see two ways of defining Population class. I ran the program below twice, once with each way of creating Population class. One uses numpy and one does not use numpy. The one without numpy takes similar time as John found in his runs. One with numpy takes much longer. What is not clear to me is that the popn instance is created before time recording begins (at least that is what it appears from the code). Then, why is numpy version taking longer. And, I thought numpy was supposed to be more efficient. Anyhow, the problem seems to be with numpy and not so much with the append, even though it does slow down things a little. Can someone please confirm with the code below? Thanks. Edit 4: See the answers by John Machin and TryPyPy Since there have been so many edits and updates here, those who find themselves here for the first time may be a little confused. See the answers by John Machin and TryPyPy. Both of these can help in improving the speed of the code substantially. I am grateful to them and others who alerted me to slowness of append. Since, in this instance I am going to use John Machin's solution and not use numpy for generating utilities, I am accepting his response as an answer. However, I really appreciate the directions pointed out by TryPyPy also.  ", "id": 4653715, "title": "Increasing speed of python code", "views": 5613}, "resource": "StackOverflow", "tags": ["python", "performance"]}, {"answers": [{"content": "The standard way to answer \"why is it slow\" questions is to profile it.\n\nMy favourite Python profiler at the moment is Py-spy, which runs on both Linux and Mac, and will give you some fairly good insight into what it's spending time on, expecially if it's Python code that's causing the issues.\n\nIf the issue is lower level, you *might* still get some visibility into it with Py-spy's native profiling, or you might have to use something tuned for C profiling. On Linux, I tend to use perf_events for C profiling, although sadly it's not portable. I gather DTrace on Mac fills sort-of the same role, but I'm not a DTrace or Mac expert.\n\nHowever, the differences here don't strike me as \"stark\", so I'd be slightly surprised if you found a \"smoking gun\", as it were. Compiler optimisation is one possibility (GCC and Clang do take very different approaches to optimisation, and from the Python side, there's probably been more effort spent by the Python developers on tuning GCC settings over the years than tuning Clang setting), although syscall overhead is another (Mac OS and Linux kernels have somewhat different internal architectures), and there are other even more subtle possibilities, like memory management overhead, OS scheduling, cache locality, and even CPU thermal scaling.", "id": "h6120eb", "parent": "t3_oouh9a", "vote": 86}, {"content": "How did you install it on your MacBook?\n\nIt could be you downloaded a binary that wasn't compiled to take advantage of all the optimizations for your CPU, building your python from scratch can yield very different performance", "id": "h61spqs", "parent": "t3_oouh9a", "vote": 36}, {"content": "This is unexpected. I've done benchmarks before in Python that showed Macs were about as fast as the same hardware on Linux. I expect it's the way Python was build. How did you install Python 3.9.6?", "id": "h61uqca", "parent": "t3_oouh9a", "vote": 11}, {"content": "Questions about performance are really tricky to answer. It depends so much on the environment it is running in. Not only that it also depends on the state of the memory and CPU when the program is running. The same program can perform differently when running at different times on the same machine. \n\nI suspect if you are anything like me your MacBook is running three IDEs that all use electron, discord, slack, and have 10GB of Firefox memory in swap. And your Linux machine is just a pure workhorse with only python running. Basically I'm saying the MacBook might have a higher concurrency workload that is keeping it busy and slowing down performance. But it could literally be anything. \n\nI really enjoyed this pycon presentation about performance.\n\n[https://youtu.be/yrRqNzJTBjk](https://youtu.be/yrRqNzJTBjk)", "id": "h61sudj", "parent": "t3_oouh9a", "vote": 14}, {"content": "The hardware difference considering the CPU alone is a big jump with 3 times the number of cores, more than double cache memory and a higher boost clock, theoretically the Macbook should completely destroy the thinkpad. I know that there are significant differences between that different operating systems but on a hardware level the only thing that could explain this might be thermal throttling. \n\nWith the better processor the Macbook had a much greater TDP (45W compared to 15W) and will generate much more heat. The intel macbooks were also know for their poorly designed cooling solutions in order to achieve their \"sleek and professional design\". It is possible especially with longer tests that the macbook hits a very high temperature quickly and the cpu slows down to prevent overheating whereas the thinkpad might have been able to maintain a higher boost clock for a longer period of time giving it an advantage especially in situations where the tasks were more single threaded.\n\nPS: This is entirely speculative and is just a possible explanation. There might be a lot more factors that come into play here.", "id": "h61y5pq", "parent": "t3_oouh9a", "vote": 6}, {"content": "These results look similar to what I get on Linux when comparing the python that's in the OS' package manager versus one I compiled myself optimized to my particular processor. As someone else suggested, that could be the issue. I'd really suggest compiling python yourself on the Mac and see if the results change. \n\nIt's also possible that some of the difference is OS-related - a combination of OS differences and perhaps CPython more optimized for Linux. As for startup, filesystem variations may be at play.", "id": "h61z7q6", "parent": "t3_oouh9a", "vote": 3}, {"content": "There are only 2 huge differences: pathlib and python_startup_no_site. Both of these are presumably limited by filesystem (or filesystem cache) speed, which is generally good on Linux. IPython (especially newer versions) is also known for doing aggressive path walking, and can be slow even on Linux.\n\n***\n\nFor all the smaller changes ... you should check `sysconfig.get_config_vars()` to see if there are any notable differences, particularly under the `PY_*FLAGS` keys.\n\nBe aware that it is a very large dict, so if you're not in IPython, be sure to use `pprint`. If you copy both dicts to a single process, maybe that one method in `unittest` will help?", "id": "h622o94", "parent": "t3_oouh9a", "vote": 2}, {"content": "I found the exact same thing when working on a game using the turtle graphics module - Mac was clearly slower than both Windows and Linux. Linux was generally 2 to 4 times faster than Mac (except when rendering text). Here is more info on the comparison I did for anyone who might be interested in some data points. https://youtu.be/eUO5T3BIvIo?t=293", "id": "h62n8qo", "parent": "t3_oouh9a", "vote": 2}, {"content": "I feel like if you use python for things that need to be fast, you are using the wrong language in the first place ;-p", "id": "h64j6na", "parent": "t3_oouh9a", "vote": 2}, {"content": "This is purely anecdotal (n = 3), but matches my observations over the last 2,5 years. Pairing with my MacOS peers is always sluggy, pytest running natively was/is way slower compared to my Arch machine (I would estimate around 2 to 3 times slower, depending on the project), but pytest within docker was an atrocity under MacOS (at least a magnitude slower, even worse if lots of on-disk database operations were made). Having to wait for a slow pytest suite is... let's say annoying if you know that it runs in under 20 seconds in the CI. \n\nNot sure if they misconfigured something, two of them were/are hellish bright and I doubt it's their fault (but not impossible, of course).\n\nPersonally, I moved on to Linux around 6 years ago and could never imagine switching back to the lib-legacy hell, MacOS and the Macbook Pro were great developer machines for a while, but became too end user centric (and overpriced) for my personal developer taste.\n\nBut everyone is different and your mileage may vary.", "id": "h620n9k", "parent": "t3_oouh9a", "vote": 3}, {"content": "Can it be intentional? There is an intentional fan flaw in 2020 macbook air. You may have a look:\nhttps://youtu.be/OfBroJFKF6w", "id": "h61r0ic", "parent": "t3_oouh9a", "vote": 2}, {"content": "I'm an idiot, but my gut says Linux is way leaner. My last mac would cannibalize my HDD when it needed more RAM", "id": "h62rghn", "parent": "t3_oouh9a", "vote": 1}, {"content": "Let me get this right\u2026 you have a sample size of 2, with a bunch of undeclared variables.\nAnd that has you assume that Python runs \u201cso much slower\u201d based on the OS?\nI feel like I\u2019m misreading your post.\nIt also gives me a feeling that you assume that the CPU having a maximum clock indicates at which clock your programs are being executed. Or rather, that the hardware should be the only factor in this.\n\nI honestly believe I\u2019m just reading your post wrong. So I\u2019m gonna \u201cassume\u201d (even though I\u2019d rather be sure) that this could give you some insight to understand what\u2019s going on. Keep in mind it\u2019s not a guide/tutorial on \u201chow to make Python go faster on OS X\u201d.\n\nCheck the reply and comments here: https://stackoverflow.com/questions/49541307/does-the-performance-of-numpy-differ-depending-on-the-operating-system\nIgnore the Pandas part. The point raised is what I think you should keep in mind, when thinking about realized Python performance.\n\nAlso keep in mind that the realized performance of a program depends on an incredible amount of factors. Especially for interpreted languages.\nIn addition to all the factors influencing performance of a given executable, the OS itself has to figure out how to schedule your program and how to use the resources at its disposal the best. This can either weigh for performance, energy usage and a ton of other factors. But basically, when you\u2019re running a program, there is still a lot of other stuff the OS has to deal with, besides executing the Python interpreter.\nIn addition to that, the hardware itself might be optimized differently. (In your case I believe the differences aren\u2019t that big, but it\u2019s still something to keep in mind). Especially in \u201cye olden days\u201d when companies like ibm, cyrix, sun, intel and amd were doing a lot of the \u201cground work\u201d when it comes to optimizations in the CPU\u2019s, some instructions would run significantly faster on some hardware. Memory access times would also differ quite a bit.\n\nNowadays those differences aren\u2019t that big anymore, but again still something to keep in mind.\n\nThe last thing I think you (or somebody googling this and finding this post years from now) should keep in mind, is that most modern OS\u2019es won\u2019t run your program directly. There\u2019s often some form of sandboxing or other means of isolation. There\u2019s a lot of ways to do that, so the impact this has on performance also varies, even though vendors do some amazing work in keeping the performance impact to a minimum.\nEspecially ram/cache read times can vary significantly. And this only gets worse the more other processes are run in the background. The more processes get moved between run, pause, wait, suspend, etc the more difficult it gets to predict exactly what happens. Even though many OS\u2019es apply some (to me mind blowing) tricks to predict things to make the scheduling more efficient, there\u2019s still sooo much that can affect performance of a program. And the more high-level (as in, removed from bare metal) it is, the more likely it is, your program will have to wait for other stuff. (The last sentence is more a rule of thumb, than an absolute and documentable fact).\n\nAlso: sorry for my shitty English - it\u2019s not my first language.\n\ntl;dr: it\u2019s complicated because computers are complicated.", "id": "h630ilr", "parent": "t3_oouh9a", "vote": -2}, {"content": "I am also getting faster results on my 4750 Ryzen Thinkpad. Python 3.85 compiled with GCC 9.3.0, kernel 5.8.0-53\n\nFor instance:\n    \n    pickle_dict: Mean +- std dev: 17.7 us +- 0.1 us\n\ncompared to: `28.5 us`\n\n\n    go: Mean +- std dev: 239 ms +- 4 ms\n\n\ncompared to: `326 ms`\n\nAll the tests are faster by about that much. Most of these tests appear to be single threaded so it's not the thread count.", "id": "h624odv", "parent": "t3_oouh9a", "vote": 1}, {"content": "Is your linux machine up to date? May be that the Patch for ZombieLoad hits performance, perhaps up to 40%.\n\nI\u2019d expect whichever machine has the patch to perform considerably worse.", "id": "h62db9o", "parent": "t3_oouh9a", "vote": 1}, {"content": "Number of cores doesn't matter. CPython has the GIL so it only executes on one thread at a time. Linux allows pinning threads to CPUs, macOS doesn't. So there's the possibility that macOS is just context switching a lot more in the benchmarks. Also, the difference in the load times likely has to do with macOS being ever so slightly slower at loading dynamic libraries and/or Linux having a smaller dynamic load path resolution scope and/or memory paging when jumping to the resolution of the dynamically loaded symbols. macOS app bundles are very self-contained and thus they have probably spend a lot less time optimizing the dynamic runtime loader and it's caching behavior.\n\nMore generally though, you should never expect Python to scale consistently with hardware specs. Python code is very, very far away from the hardware. A simple addition of two integers is at least a hundred CPU instructions whereas in natively compiled code, it's 1 instruction. All those extra instructions introduce more variability than can be accounted for by only looking a couple high-level HW specs", "id": "h63fzqq", "parent": "t3_oouh9a", "vote": 1}, {"content": "Check the temp and Max boost Hz of the active core on the mac", "id": "h63qckx", "parent": "t3_oouh9a", "vote": 1}, {"content": "I don\u2019t know if others would get this same result but a year ago I tried macOS and Linux on the same machine. Linux was running as a VM in Parallels. And yet python was still faster in Linux as a VM than macOS natively.", "id": "h6eh4gs", "parent": "t3_oouh9a", "vote": 1}, {"content": "I don\u2019t think you need to worry about python being slow with any up to date Mac model it should run just fine and if u have a old Mac then it really doesn\u2019t matter usually if it\u2019s so slow that compiling takes long (or whatever the the python equivalent of compiling is) then the problem isn\u2019t python and if u really care about run time I suggest u get a new Mac or maybe it\u2019s ur code that\u2019s running slow", "id": "h6kqn2g", "parent": "t3_oouh9a", "vote": 1}, {"content": "Thanks for your response. I agree most of the benchmarks aren't really that huge, but the fact that they are slower on MacOS despite being on faster hardware makes me think that if I normalized the hardware they'd be even more pronounced.\n\nThe annoying one (and there is a test for this in that suite confirming it) is that starting python is 2x as slow on MacOS. Not a big deal for production, but for opening python shells (which is how I originally noticed this) it is annoying. On the Linux system iPython takes about 500ms to start whereas on MacOS it takes an entire second. It's noticeable, and once you see it you can't unsee it!", "id": "h61mocv", "parent": "t1_h6120eb", "vote": 20}, {"content": "Homebrew", "id": "h6210ce", "parent": "t1_h61spqs", "vote": 8}, {"content": "It was installed off homebrew.", "id": "h61ypil", "parent": "t1_h61uqca", "vote": 3}, {"content": "also the thinkpad thermal throttles less if at all", "id": "h61zc02", "parent": "t1_h61sudj", "vote": 7}, {"content": "My first thought was thermal throttling. But I ran the Intel power Gismo on the Mac during the entire test and it showed the clock pinned to max frequency the entire time. Before the test it was below the base frequency (I didn't know that was possible) so there wasn't any other process pegging the CPU unless it started during the time of the test.", "id": "h61yz6v", "parent": "t1_h61y5pq", "vote": 5}, {"content": "This was going to be my guess too. My MBP 2015 has always underwhelmed me with its performance and poor heat dissipation. All too often I would have some random process rail one or more cores, and it would be a nightmare using it for work. I vaguely recall some underlying issues with Mac OS that can cause some slowdowns, but I forget the details. \nhttps://www.phoronix.com/scan.php?page=article&item=macos1015-win10-ubuntu&num=10", "id": "h62330u", "parent": "t1_h61y5pq", "vote": 1}, {"content": ">In your case I believe the differences aren\u2019t that big, but it\u2019s still something to keep in mind\n\nmind making a small list of resources for both cpus? like clock, nb of cores, memory, etc. just so everyone knows what you consider \"not that big\".\n\nI'm not going to comment on the sample size, and your big argument of how basically the drive for optimization made it slow on much faster hardware.\n\nI mean you probably right and I'm with you on this one but you know...", "id": "h637ii4", "parent": "t1_h630ilr", "vote": -1}, {"content": "You clearly did not read the post.", "id": "h6uowae", "parent": "t1_h6kqn2g", "vote": 1}, {"content": "If we're talking about double the time, I reckon that probably is a big enough difference that it'd show up in profiling, as long as the sampling frequency is reasonably high (the `--rate` flag in Py-spy). I think Py-spy should be able to profile startup, at least from the bit when it starts the interpreter. If the performance bottleneck is before the interpreter starts interpreting (maybe linking is slower on Mac?) you might need to use lower-level tools.", "id": "h61roei", "parent": "t1_h61mocv", "vote": 11}, {"content": "This should probably take <1ms on modern computers -- what the heck is the Python executable doing?", "id": "h61qyt9", "parent": "t1_h61mocv", "vote": 3}, {"content": "That\u2019s your problem. Homebrew installs unoptimized interpreters to ensure successful execution on a wide range of Mac systems.\n\nInstall the interpreter for your system, thru Python.org or similar, then retest.", "id": "h623kcz", "parent": "t1_h6210ce", "vote": 54}, {"content": "I would expect that was compiled properly.", "id": "h6243nn", "parent": "t1_h61ypil", "vote": 5}, {"content": "And there goes my explanation. The only other thing to point at is software level optimizations but that is really not convincing because of the sheer difference in raw cpu power.", "id": "h620i3m", "parent": "t1_h61yz6v", "vote": 2}, {"content": ">I'm not going to comment on the sample size\n\nThen why are you bringing it up? Am i missing something?\n\n&#x200B;\n\n>and your big argument of how basically the drive for optimization made it slow on much faster hardware.\n\nI never said that. I tried to say the opposite of that. But please keep a potential language/culture barrier in mind.\n\n&#x200B;\n\n>mind making a small list\n\nIt'd probably be better if i linked to the information directly.\n\n[https://ark.intel.com/content/www/us/en/ark/products/134906/intel-core-i7-8750h-processor-9m-cache-up-to-4-10-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/134906/intel-core-i7-8750h-processor-9m-cache-up-to-4-10-ghz.html)\n\n[https://ark.intel.com/content/www/us/en/ark/products/95451/intel-core-i7-7500u-processor-4m-cache-up-to-3-50-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/95451/intel-core-i7-7500u-processor-4m-cache-up-to-3-50-ghz.html)", "id": "h63h0v8", "parent": "t1_h637ii4", "vote": 1}, {"content": "True", "id": "h6up29x", "parent": "t1_h6uowae", "vote": 1}, {"content": "The time spent loading dynamic libraries and resolving the dynamically loaded symbols is not something that will show up in the profiler beyond a very opaque call to a system library.", "id": "h63gskx", "parent": "t1_h61roei", "vote": 2}, {"content": "To be fair, it's iPython, so it's a Python shell + a bunch of imports and code being executed up front.\n\n&#x200B;\n\nThe normal python repl loads in like 50ms on Linux, 60-80ms on MacOS.", "id": "h61zd0x", "parent": "t1_h61qyt9", "vote": 7}, {"content": "On Linux at least, I'd like to think I'd know what time spent in `ld-linux.so` meant. Although I guess I'd have less idea on Mac.", "id": "h63w1t3", "parent": "t1_h63gskx", "vote": 1}, {"content": "It's not just loading the libraries. It's once loaded, function calls can very indirect through several different pages of memory. All of that will only really show up in an obscure way. Python is powerful and modular bc of it's reliance on dynamic linking but it's also an inherent source of overhead bc every function call requires computing the address of the function call relative to the address of the library contain the function. For huge packages with tons of dynamic libraries, the number of offset calculations are not only extremely high but also ubiquitous", "id": "h64zerg", "parent": "t1_h63w1t3", "vote": 2}], "link": "https://www.reddit.com/r/Python/comments/oouh9a/why_is_python_so_much_slower_on_macos/", "question": {"context": "Hello,\n\nI have both a 2018 Macbook pro and a 2016 Thinkpad t470s running Linux.\n\nThe Thinkpad has a [7th gen Intel Core I-7  i7-7500U with a max frequency of 3.5GHz](https://ark.intel.com/content/www/us/en/ark/products/95451/intel-core-i7-7500u-processor-4m-cache-up-to-3-50-ghz.html)\n\nThe Macbook has a [8th gen Intel Core I-7 i7-8750H with a max frequency of 4.10GHz](https://ark.intel.com/content/www/us/en/ark/products/134906/intel-core-i7-8750h-processor-9m-cache-up-to-4-10-ghz.html)\n\nDespite the Macbook having a faster processor in every way (and more impressive harware all around), I noticed that some things just seem sluggish with it, like opening an iPython terminal, on the Macbook compared to the Thinkpad.\n\nSo I decided to run some actual benchmark suite. I found [pyperformance](https://github.com/python/pyperformance) which would seem to do the trick. \n\nI ran the suite on both machines, same Python version (3.9.6) with no other running applications. Sure enough, here are the results:\n\n    performancemacos.json\n    =====================\n    \n    Performance version: 1.0.2\n    Report on macOS-11.4-x86_64-i386-64bit\n    Number of logical CPUs: 12\n    Start date: 2021-07-21 09:23:02.300645\n    End date: 2021-07-21 09:30:01.775892\n    \n    performancelinux.json\n    =====================\n    \n    Performance version: 1.0.2\n    Report on Linux-5.13.4-arch1-1-x86_64-with-glibc2.33\n    Number of logical CPUs: 4\n    Start date: 2021-07-21 09:21:44.317849\n    End date: 2021-07-21 09:27:59.117890\n    \n    ### 2to3 ###\n    Mean +- std dev: 438 ms +- 4 ms -> 403 ms +- 15 ms: 1.09x faster\n    Significant (t=9.62)\n    \n    ### chaos ###\n    Mean +- std dev: 139 ms +- 3 ms -> 130 ms +- 7 ms: 1.07x faster\n    Significant (t=5.28)\n    \n    ### deltablue ###\n    Mean +- std dev: 9.45 ms +- 0.60 ms -> 8.28 ms +- 0.21 ms: 1.14x faster\n    Significant (t=8.27)\n    \n    ### fannkuch ###\n    Mean +- std dev: 602 ms +- 8 ms -> 524 ms +- 9 ms: 1.15x faster\n    Significant (t=28.80)\n    \n    ### float ###\n    Mean +- std dev: 147 ms +- 2 ms -> 140 ms +- 2 ms: 1.05x faster\n    Significant (t=10.94)\n    \n    ### go ###\n    Mean +- std dev: 326 ms +- 6 ms -> 294 ms +- 5 ms: 1.11x faster\n    Significant (t=19.62)\n    \n    ### hexiom ###\n    Mean +- std dev: 12.6 ms +- 0.1 ms -> 11.3 ms +- 0.2 ms: 1.12x faster\n    Significant (t=22.71)\n    \n    ### json_dumps ###\n    Mean +- std dev: 18.0 ms +- 0.3 ms -> 14.9 ms +- 0.3 ms: 1.21x faster\n    Significant (t=31.89)\n    \n    ### json_loads ###\n    Mean +- std dev: 35.1 us +- 0.8 us -> 30.4 us +- 0.6 us: 1.15x faster\n    Significant (t=21.20)\n    \n    ### logging_format ###\n    Mean +- std dev: 12.4 us +- 0.2 us -> 11.7 us +- 0.3 us: 1.06x faster\n    Significant (t=8.64)\n    \n    ### logging_silent ###\n    Mean +- std dev: 252 ns +- 4 ns -> 220 ns +- 11 ns: 1.15x faster\n    Significant (t=12.74)\n    \n    ### logging_simple ###\n    Mean +- std dev: 11.4 us +- 0.4 us -> 10.7 us +- 0.3 us: 1.06x faster\n    Significant (t=6.22)\n    \n    ### meteor_contest ###\n    Mean +- std dev: 130 ms +- 1 ms -> 132 ms +- 2 ms: 1.02x slower\n    Significant (t=-5.10)\n    \n    ### nbody ###\n    Mean +- std dev: 169 ms +- 7 ms -> 149 ms +- 2 ms: 1.13x faster\n    Significant (t=11.68)\n    \n    ### nqueens ###\n    Mean +- std dev: 128 ms +- 1 ms -> 114 ms +- 2 ms: 1.12x faster\n    Significant (t=29.06)\n    \n    ### pathlib ###\n    Mean +- std dev: 58.6 ms +- 2.7 ms -> 23.3 ms +- 0.5 ms: 2.51x faster\n    Significant (t=56.83)\n    \n    ### pickle ###\n    Mean +- std dev: 13.3 us +- 0.8 us -> 11.9 us +- 0.2 us: 1.12x faster\n    Significant (t=7.39)\n    \n    ### pickle_dict ###\n    Mean +- std dev: 28.5 us +- 0.3 us -> 32.4 us +- 2.5 us: 1.14x slower\n    Significant (t=-6.94)\n    \n    ### pickle_list ###\n    Mean +- std dev: 4.81 us +- 0.05 us -> 4.80 us +- 0.08 us: 1.00x faster\n    Not significant\n    \n    ### pickle_pure_python ###\n    Mean +- std dev: 609 us +- 8 us -> 512 us +- 22 us: 1.19x faster\n    Significant (t=18.61)\n    \n    ### pidigits ###\n    Mean +- std dev: 237 ms +- 2 ms -> 211 ms +- 2 ms: 1.13x faster\n    Significant (t=36.07)\n    \n    ### pyflate ###\n    Mean +- std dev: 942 ms +- 11 ms -> 815 ms +- 12 ms: 1.15x faster\n    Significant (t=34.76)\n    \n    ### python_startup ###\n    Mean +- std dev: 37.4 ms +- 2.4 ms -> 22.8 ms +- 0.3 ms: 1.64x faster\n    Significant (t=46.74)\n    \n    ### python_startup_no_site ###\n    Mean +- std dev: 17.6 ms +- 0.8 ms -> 8.8 ms +- 0.2 ms: 2.00x faster\n    Significant (t=78.68)\n    \n    ### raytrace ###\n    Mean +- std dev: 635 ms +- 25 ms -> 563 ms +- 9 ms: 1.13x faster\n    Significant (t=11.84)\n    \n    ### regex_compile ###\n    Mean +- std dev: 226 ms +- 6 ms -> 213 ms +- 3 ms: 1.06x faster\n    Significant (t=8.49)\n    \n    ### regex_dna ###\n    Mean +- std dev: 226 ms +- 2 ms -> 238 ms +- 4 ms: 1.05x slower\n    Significant (t=-12.97)\n    \n    ### regex_effbot ###\n    Mean +- std dev: 4.00 ms +- 0.04 ms -> 3.49 ms +- 0.09 ms: 1.15x faster\n    Significant (t=23.35)\n    \n    ### regex_v8 ###\n    Mean +- std dev: 31.2 ms +- 0.5 ms -> 27.7 ms +- 0.5 ms: 1.13x faster\n    Significant (t=21.25)\n    \n    ### richards ###\n    Mean +- std dev: 98.1 ms +- 11.8 ms -> 79.6 ms +- 2.3 ms: 1.23x faster\n    Significant (t=6.90)\n    \n    ### scimark_fft ###\n    Mean +- std dev: 481 ms +- 39 ms -> 411 ms +- 7 ms: 1.17x faster\n    Significant (t=7.95)\n    \n    ### scimark_lu ###\n    Mean +- std dev: 236 ms +- 34 ms -> 179 ms +- 10 ms: 1.32x faster\n    Significant (t=7.21)\n    \n    ### scimark_monte_carlo ###\n    Mean +- std dev: 133 ms +- 8 ms -> 119 ms +- 3 ms: 1.11x faster\n    Significant (t=7.40)\n    \n    ### scimark_sor ###\n    Mean +- std dev: 270 ms +- 21 ms -> 221 ms +- 4 ms: 1.22x faster\n    Significant (t=10.23)\n    \n    ### scimark_sparse_mat_mult ###\n    Mean +- std dev: 6.51 ms +- 0.64 ms -> 5.25 ms +- 0.05 ms: 1.24x faster\n    Significant (t=8.68)\n    \n    ### spectral_norm ###\n    Mean +- std dev: 198 ms +- 17 ms -> 160 ms +- 3 ms: 1.24x faster\n    Significant (t=9.98)\n    \n    ### sqlite_synth ###\n    Mean +- std dev: 3.61 us +- 0.07 us -> 3.24 us +- 0.07 us: 1.12x faster\n    Significant (t=17.59)\n    \n    ### telco ###\n    Mean +- std dev: 8.39 ms +- 0.12 ms -> 290.89 ms +- 4.12 ms: 34.68x slower\n    Significant (t=-306.86)\n    \n    ### unpack_sequence ###\n    Mean +- std dev: 70.3 ns +- 1.1 ns -> 63.8 ns +- 0.9 ns: 1.10x faster\n    Significant (t=19.82)\n    \n    ### unpickle ###\n    Mean +- std dev: 18.8 us +- 0.2 us -> 14.0 us +- 0.1 us: 1.34x faster\n    Significant (t=80.19)\n    \n    ### unpickle_list ###\n    Mean +- std dev: 5.10 us +- 0.29 us -> 4.97 us +- 0.09 us: 1.03x faster\n    Not significant\n    \n    ### unpickle_pure_python ###\n    Mean +- std dev: 441 us +- 5 us -> 369 us +- 9 us: 1.19x faster\n    Significant (t=30.01)\n    \n    ### xml_etree_generate ###\n    Mean +- std dev: 117 ms +- 5 ms -> 107 ms +- 3 ms: 1.10x faster\n    Significant (t=7.79)\n    \n    ### xml_etree_iterparse ###\n    Mean +- std dev: 176 ms +- 34 ms -> 133 ms +- 2 ms: 1.33x faster\n    Significant (t=5.67)\n    \n    ### xml_etree_parse ###\n    Mean +- std dev: 207 ms +- 7 ms -> 186 ms +- 2 ms: 1.12x faster\n    Significant (t=12.63)\n    \n    ### xml_etree_process ###\n    Mean +- std dev: 99.9 ms +- 11.3 ms -> 83.2 ms +- 1.0 ms: 1.20x faster\n    Significant (t=6.57)\n    \n\nThe Linux machine was faster in every benchmark (except for one where it was 35x slower). Not what you would expect if only taking hardware into account.\n\nI've heard that python compiled with GCC may be more optimized then LLVM which the MacOS build was built with. Could this really account for the stark performance difference?", "id": "oouh9a", "title": "Why is python so much slower on MacOS?"}, "resource": "Reddit"}, {"answers": [{"content": "Try to reduce the imports and third-party dependencies. The executable is packed afik with upx. Executing it, requires first unpacking the whole content and placing it into a temporary directory. Then the main application is executed. If the executable is huge, it could take time.\n\nA possible alternative is [nuitka](https://nuitka.net/).", "id": "j2rdz8b", "parent": "t3_1025bvx", "vote": 2}, {"content": "Thanks \ud83d\udc4d", "id": "j2wtiwn", "parent": "t1_j2rdz8b", "vote": 1}], "link": "https://www.reddit.com/r/learnpython/comments/1025bvx/how_can_i_speed_up_my_python_executable_code/", "question": {"context": "I just made my python code executable and It's runs slow, Even if I use \"pyintaller -w main.py\" \n\nHow do I increase the speed of it ??", "id": "1025bvx", "title": "How can I speed up my python executable code"}, "resource": "Reddit"}, {"answers": [{"content": "    from hardware import cpu\n    \n    for core in cpu.cores:\n        core.speed *= 1.5", "id": "hqe5fdt", "parent": "t3_rqytcd", "vote": 6}, {"content": "Haha, cool 8y.o. MacBook pro is slow, just don't post it in an apple fan sub ..  considering it's just 8y.o. it should have SSD already, so I would say make a clean install of the OS and check it out. As it's a MacBook i think there is no option for upgrade like ram etc, all are soldered for a long time now, but I can be wrong I'm not really looking for them anymore and never had one.", "id": "hqdxyhh", "parent": "t3_rqytcd", "vote": 2}, {"content": "I doubt Python will help. Your idea of deleting files with a Python script is a decent one, but probably unnecessary and could easily mess up your system. As long as you\u2019re not running out of space, clearing out more hard drive space won\u2019t make your computer faster. For a super innacurate number, 50GB would be more than enough. I did a quick google search and this was the first site I came across with tips on this cleanup: https://www.google.com/amp/s/macpaw.com/amp/how-to/clean-up-mac\n\nOne of the few things you could do to make your computer feel faster is to reduce the number of programs running. Look up how to safely quit running processes, and keep programs from starting themselves on startup.\n\nOh, and it sounds silly, but the easiest way to make your computer \u201cfeel\u201d faster if you write a lot is to go to your keyboard settings and turn the key repeat speed all the way up and the delay all the way down.", "id": "hqf3bd7", "parent": "t3_rqytcd", "vote": 2}, {"content": "Don't even need python. https://downloadmoreram.com/", "id": "hqddj7e", "parent": "t3_rqytcd", "vote": 4}, {"content": "Off topic, but adding an ssd, reapplying thermal paste/pad on cpu, changing to two sticks of RAM, blowing out dust with compressed air, removing animal hair/fur/whatever that is wedged and the compressed air won't get are some refresh steps that I look at for old laptops.\n\nI don't work on macs, and have no idea how easy or pain in the rear they are (although, I have a guess).\n\nDo what you feel comfortable with, and consult your local independent shop(s) to see if they think it's worth doing what you're not. One that puts clients first.", "id": "hqdeh2z", "parent": "t3_rqytcd", "vote": 1}, {"content": "Awesome, yeah I really need to figure out how to stop apps from starting on startup. I never use steam except for every once in awhile but It always start on start up.\n\nEdit:Thank you king\ud83d\ude4f", "id": "hqf878w", "parent": "t1_hqf3bd7", "vote": 1}, {"content": "And that\u2019ll do it? Its just been such a pain running any program lately including the internet. I\u2019m not a Comp Sci guy so don\u2019t really understand ram/cpu etc.", "id": "hqde45a", "parent": "t1_hqddj7e", "vote": 1}, {"content": "Thanks, my computer is so fast is just flew off my desk!", "id": "hqe8nnh", "parent": "t1_hqddj7e", "vote": 1}, {"content": "If you haven't realised yet OP, that website is just a joke. You can't really download RAM. :)", "id": "hqdvk3m", "parent": "t1_hqde45a", "vote": 5}, {"content": "Lol I had to ask a CS major friend but I knew something was off", "id": "hqdvq39", "parent": "t1_hqdvk3m", "vote": 3}, {"content": "LOL... Alright, who's trollin' who here?", "id": "hqdvxw7", "parent": "t1_hqdvq39", "vote": 1}, {"content": "Lol I knew ram was hardware but honestly with my lack of understanding of the cloud I could see it being plausible. It was a classic bit, I gotta try it out sometime", "id": "hqdw1bh", "parent": "t1_hqdvxw7", "vote": 1}], "link": "https://www.reddit.com/r/learnpython/comments/rqytcd/anyone_have_a_good_method_for_increasing_macbook/", "question": {"context": "To clarify I am not asking how to increase the speed my python scripts run at. I am asking if there is any sort of script I can run in python to increase my computers speed on startup, my browsing speed on the internet, and the speed my documents load/save/download etc. particularly Ableton files. \n\nI have an 8 year old Macbook pro and She slow. I can\u2019t afford another mac rn but I bought ableton for mac, and studio one for mac awhile back.\n\nI know there are some scripts I could probably run to clean up my downloads/desktop but I am wondering whats the best way to go about it without accidentally deleting anything of importance. I probably shouldn\u2019t delete anything based on file type (e.g. deleting all of my .jpeg, .docx etc.) but beyond that I don\u2019t really know what strategy I should use.\n\nShould I use regular expressions? Should I delete everthing older than say 6 years? Is there a way to find out which docs I have duplicates of and save one copy to a new folder and delete the other two? I am very open to suggestions.", "id": "rqytcd", "title": "Anyone have a good method for increasing macbook speed with python?"}, "resource": "Reddit"}, {"answers": [{"content": "If your memory isnt being used it likely is not needed. What you need to identify is what is slowing the code down, so you would need to profile the code to identify what parts are executing slowly. There can be various causes. Examples:\n\n\\- Program wants to read an write from some file, but only one file exists so they have to wait\n\n\\- Many threads want to read from some queue, but the process feeding that queue is too slow\n\n\\- Some internal memory store is inefficient, like a list, and every time an operation happens the whole list is scanning over and over\n\nPlus dozens of others.\n\nMemory is likely not the direct issue.", "id": "kbgayre", "parent": "t3_187qto4", "vote": 2}, {"content": "It's not that the program can become faster because you tell it to use more memory, it's that you can possibly make the program faster by leveraging the spare memory.\n\nOne strategy in this regard could be to add the file contents into a cache as you read them, so it gets faster on successive accesses. Of course if the total weight of the all the files is larger than the available ram you'll need to manage this somehow (e.g. keeping only recently accessed files) so it doesn't run out of memory.\n\nBut before you even start doing anything you should verify where the bottlenecks are, all of this would be for nothing if reading the files isn't what's making your program so slow.", "id": "kbkiycs", "parent": "t3_187qto4", "vote": 1}, {"content": "I'm not the best when it comes to coding, what do you mean by \"profile the code\"? Just look through the files to try and figure out what is going on?", "id": "kbgd0ts", "parent": "t1_kbgayre", "vote": 1}, {"content": "It might be the processors - when running a search most cores are pegged at 100%. Might try caching the files, I'm not sure", "id": "kbl36gm", "parent": "t1_kbkiycs", "vote": 1}, {"content": "There are special tools in every language that help you measure time. In python you can (as a beginner approach):\n\ntime\\_start = time.time()\n\nSOME STUFF\n\ntime\\_delay = time.time() - time\\_start\n\nprint(time\\_delay)\n\nYou theorize about what is slow, and fast, and slowly work towards identifying the part of the code that is slowest. When you find it, you fix it.\n\nThere are also advanced tools that can \"watch\" the whole program, and print results. However in my experience those tools can be really hard to understand without practice.", "id": "kbgl5go", "parent": "t1_kbgd0ts", "vote": 2}], "link": "https://www.reddit.com/r/AskProgramming/comments/187qto4/trying_to_get_python_to_use_more_ram_for_faster/", "question": {"context": "Hey all,\n\nSo I'm working on a project at work: an internal ChatGPT server, using PrivateGPT, that is able to read and manage all of our policy documents and give answers to any asked questions.\n\nI've been working for the last week and a half on this project and I'm about nearing completion, but I'm stuck on one thing: the speed. On average it takes the program about 40 seconds to spit out an answer, and while that's better than the 80-some it took at their last attempt it still isn't even close to perfect. For context, I'm running this on an older Dell server that's more than capable enough for anything I'd need it to do. \n\nI looked in the resource manager and it appears that I'm only using about 6gb of the 104gb of RAM available to the machine, and my hope is that if I'm able to make the program run using about all of the RAM it should be able to search through the documents quicker and maybe spit out a faster result.\n\nFrom what I've seen the program is running in Python, specifically using the Poetry application. Does anyone have any ideas on how to get around this limitation?", "id": "187qto4", "title": "Trying to get Python to use more RAM for faster program results"}, "resource": "Reddit"}, {"answers": [{"content": "Python has an abstraction level on top of C, so it will be slower than C, whatever you do. If you rewrite a C program in pure python, it will be much slower than in C. However there are three things that make python interesting:\n\n- what actually matters is life cycle cost for a software. It includes developer time, running time, debugging time and cost of resources. Python is much more flexible than C and therefore faster/easier to develop with (but with great power comes great responsibility). So if you need to write a small script that will run for a few seconds every day, maybe it is not worth spending more time writing it in C to save maybe a minute of runtime every year.\n\n- CPU limitation is just an element of your code speed. When you are dealing with network access, or even file system access, a lot of you execution time is waiting for these operations to finish. You won't gain a lot by speeding up the code itself, unless you have enough operations to run things in parallel.\n\n- a lot of time in software, there are just a few bottlenecks in your code. Since python is capable of executing C libraries, you can code these in C , or even assembly if C is too slow, and you will have addressed 80% of your bottlenecks. That's basically the model used in ML: data preparation, model definition are the parts that can change a lot every time so keeping them in python saves development time. And also they are not the most CPU intensive task overall so no need to optimise them to death.", "id": "hnfsszh", "parent": "t3_ra2aqh", "vote": 525}, {"content": "> I work as ML Engineer\n\nThen you should know that the ML libraries and any library with heavy math that Python uses are mainly written in C/C++/Fortran/any other fast compiled language, not Python, Python is mainly used for calling functions from those languages. \n\nThat's why you \"never felt like Python is slow\", cause you were really running C/C++ that Python just calls, if those libraries were written in pure Python, they would be 100-1000 times slower.\n\nIt's a good combo, fast but inflexible language to do the \"heavy lifting\" part, slow but flexible language to do the \"management\" part, best of both worlds, and works surprisingly well.\n\nOf course that ends once you stop using and start writing a \"Python\" math heavy library, then Python is not an option anymore, you will have to use another language, at least for the heavy parts.", "id": "hngad3z", "parent": "t3_ra2aqh", "vote": 261}, {"content": "Depends on how your program is written.\n\nIf you are \"vectorizing\" your code and calling fast libraries like Numpy or Pandas (which are itself written in Fortran or C) your code can be very fast - often faster than \"hand-written\" solutions in other languages. Same for JIT-compiled code with Numba.\n\nBut if you are writing large loops (>> 10k iterations) in pure (C-)Python it is very slow - often a factor of 100 slower than in fast compiled languages.", "id": "hnfrb9q", "parent": "t3_ra2aqh", "vote": 110}, {"content": "As an ML engineer, you probably use pretty little Python in fact, as your work will leverage Numpy, Pandas, Dask, PySpark or whatever. You don't actually create and iterate over Python-specific data structures using Python.\n\nPandas, Numpy leverage a lot of C code for the more CPU-complex tasks. You use Python simply to orchestrate those tasks.\n\nThat said, your experience is a testament of the fact that computers today are really fast, and for the most part you shouldn't care if your program is 60-200 times slower than if it were written in C. This is linear performance anyway, and most performance issues that I've seen are based in the fact that developers chose O(n^(2)) algorithms or worse, when an O(log n) could have been used.\n\nThe real world situations where Python isn't fast enough, are really few and hard to find. Maybe if you have some code that manages a huge amount of data, using pure Python, due to a custom logic, then you might feel like it's really slow, and actually impacting your business.\n\nWhen you get to that level of optimisation, you'll see people complain about latency spikes when .NET Garbage Collection is triggered, or other nitty-gritty details about pure performance.\n\nYou won't be building a new database using Python, that's for sure.\n\nBut if you use Python to glue stuff together, and let the real performance-intensive stuff to be done by systems designed for performance, then you'll be Fiiiiiiine.", "id": "hng9te6", "parent": "t3_ra2aqh", "vote": 24}, {"content": "As others have said, \"too slow\" is a question of context, but figured I'd give an example from my day job and what we did about it.\n\nThere's a system I work on where one of its features is that it lets users download CSV reports. These reports are generated on the fly from data in the database. We did performance test most of the system before going live, so we identified most of the performance issues, but we overlooked the CSV report download feature. After go-live, users complained about slow download speeds (around 2mbit/s). When we profiled the code, we discovered that the bottleneck was the CSV library we were using which was written in pure Python (we weren't using the standard library one, which is well optimized and written in C, for reasons that I won't go into). Rewriting the bits of the CSV library that we needed in Cython proved to be enough to get download speeds up to the point where it was faster than most users internet connection speeds.\n\nSo the requirements are your context. Sometimes the requirements are not explicit, and you only discover them when you get them wrong (we never had any requirements for download speed, until users started using the system), but \"too slow\" always depends on how fast you need it to be.", "id": "hng0pt1", "parent": "t3_ra2aqh", "vote": 47}, {"content": "Rarely is your python the bottleneck in most domains where python is popular. When it is, you can pretty much always do something about it.\n\nQ. When do you know your Python is really \"too slow\"?  \nA. When you profile it.", "id": "hnfrnnk", "parent": "t3_ra2aqh", "vote": 44}, {"content": "> Is Python really 'too slow'?\n\nNo. Is it slower than other languages? yes.\n\nIf you find it slow or expensive in your usecase, rewrite the high-cost section in rust or c (for example) and call in to it with python bindings (similar to numpy, tensorflow and the rest).", "id": "hnfwr4y", "parent": "t3_ra2aqh", "vote": 25}, {"content": "When you code in python, you need to be aware that plain python is very slow & shouldn't be used for large loops.  Instead you should either use libraries that have code written in other languages, or if that isn't possible, then use something like numba to JIT compile the code into much faster code.\n\nFor example, if you were to write code to fill a 2000x2000 python list of lists with random integers & then sum the values, this would be very slow:\n\n    def fill_data(data: list[list[int]]):\n        for i in range(0,2000):\n            data_row = []\n            for j in range(0,2000):\n                data_row.append(random.randint(0,1000))\n            data.append(data_row)\n\n    def sum_data(data: list[list[int]]):\n        total = 0\n        for i in range(0,2000):\n            #total = total + sum(data[i])\n            for j in range(0,2000):\n                total = total + data[i][j]\n        return total\n\n    data1 = []\n    t0 = time()\n    fill_data(data1)\n    t1 = time()\n    total = sum_data(data1)\n    t2 = time()\n    print(total)\n    print(f'fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\nfill:22.5902s, sum:2.57115s\n\nIf you were to do the same thing using numpy and the built-in randint and sum functions:\n\n    t0 = time()\n    data2 = numpy.random.randint(0,1000,(2000,2000))\n    t1 = time()\n    total = data2.sum()\n    t2 = time()\n    print(total)\n    print(f'fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\nfill:0.0209434s, sum:0.00199556s\n\nThat's over 1000x speedup (from 22.59 seconds to 0.02 seconds) to populate the 2000x2000 with random data and over 1200x speedup (from 2.57 seconds to 0.002 seconds) to sum all the data.\n\nIf there is no function that does exactly what you want and you want to write your code in python, then one way to make it faster is to use numba. It can JIT compile your code into a form that runs much closer to native code speed:\n\n    @numba.jit(nopython = True)\n    def fill_data2_jit(data):\n        for i in range(0,2000):\n            for j in range(0,2000):\n                data[i,j] = random.randint(0,1000)\n\n    @numba.jit(nopython = True)\n    def sum_data2_jit(data):\n        sum = 0\n        for i in range(0,2000):\n            for j in range(0,2000):\n                sum = sum + data[i,j]\n        return sum\n\n    t0 = time()\n    data2 = numpy.zeros((2000,2000),dtype=numpy.int32)\n    fill_data2_jit(data2)\n    t1 = time()\n    total = sum_data2_jit(data2)\n    t2 = time()\n    print(f'total: {total}')\n    print(f'[numba] fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\n[numba(first time)] fill:0.707387s, sum:0.174748s\n\n[numba(all other times)] fill:0.0239353s, sum:0.000998497s\n\nThe first time it runs it needs to compile the code, so it takes much longer, but all subsequent runs are very fast.\n\nIf you use numpy arrays but don't make use of the built-in functions (or numba), it appears to be no faster than native python code with lists:\n\n    def fill_data2(data):\n        for i in range(0,2000):\n            for j in range(0,2000):\n                data[i,j] = random.randint(0,1000)\n\n    def sum_data2(data):\n        sum = 0\n        for i in range(0,2000):\n            for j in range(0,2000):\n                sum = sum + data[i,j]\n        return sum\n\n    t0 = time()\n    data2 = numpy.zeros((2000,2000),dtype=numpy.int32)\n    fill_data2(data2)\n    t1 = time()\n    total = sum_data2(data2)\n    t2 = time()\n    print(f'total: {total}')\n    print(f'[numpy (no builtin)] fill:{t1-t0:.6}s, sum:{t2-t1:.6}s')\n\n[python] fill:20.5389s, sum:2.26132s", "id": "hngpzfh", "parent": "t3_ra2aqh", "vote": 11}, {"content": "\"Too slow\" is of course relative and it may not be too slow for you - but \"slow\" is absolute and in the grand scheme of things Python is indeed terribly slow.\n\nI've also very succesfully written applications that people probably wouldn't think would work out in Python (Like real time image processing) - but I also ran into the case where Python was too slow (and speeding it up a bigger hassle than just going to a faster language) (happens quite often in numerical simulations like Monte Carlo simulations). It's usually quite easy to write simple code in a fast language that outperforms well written and potentially complicated Python.\n\nAnd it should also be said that a lot of the fancier dynamic stuff in python kinda pushes you away from high performance.", "id": "hnfrs11", "parent": "t3_ra2aqh", "vote": 23}, {"content": "Generally the models you're writing for data science are using modules that are written in C with an API in Python, so these won't be affected by Python's speed. \nFor general purpose Python really isn't all that bad at all - you're more likely to run into IO problems, especially with web APIs. \nAs you begin to scale in say a larger application the speed can become noticeable. But it really depends on what you're doing! Choose the right tool for the right job.", "id": "hnfrkgg", "parent": "t3_ra2aqh", "vote": 9}, {"content": "I am working with Python for the last 4 years and I am building physical simulations and optimizations which we expose as web APIs. Python is wonderful for this:\n\n* FastAPI and Flask are terrific web frameworks,\n* nothing comes even close to Numpy and SciPy for linear algebra in other languages (except Matlab and Mathematica, but they are a non-starter for general development)\n* Python is beautiful, productive, and easy to read and maintain.\n\nIf you are talking about the web and API things, then I've found that the performance hit you get from using Python is largely irrelevant. You have to get to be serving millions of requests per day to be affected by the comparative slowness of Python.\n\nIf you are talking about compute-heavy things, Numpy does a very good job at abstracting most of the heavy loads to other languages. If you then find then that something takes a long time, then you have an option of Cythonizing parts of your code. Or you can use Numba to compile pieces of code just-in-time. Or you can use an alternative compiler like PyPy to try to speed it up that way. But there are limitations to how far you can get and it's a pain because you have to navigate the minefield of incompatibilities between compilers, libraries and Python versions.\n\nThe solution that I am currently trying out is to use Rust to write the custom compute-heavy stuff and have Python do the web API and glue everything together. I'm finding that Rust is MUCH faster than Python+Numpy. Like 10x to 100x easily. It actually allows me to do things that I would not even be thinking about if I was using just pure Python. But you need quite more lines of code to do the same thing in Rust than you would need in Python. So it's about finding the optimum between how fast something HAS to run, how much time we have to develop it and how often it's going to be run.", "id": "hnggvhb", "parent": "t3_ra2aqh", "vote": 3}, {"content": "> I haven't developed very large-scale apps\n\nThis is probably why. Try writing an API that's going to receive 1 million QPS and has a 100 ms latency budget. You'll quickly find Python isn't a good choice for this problem.", "id": "hni4x05", "parent": "t3_ra2aqh", "vote": 4}, {"content": "As long as it fulfills the goals it set out to achieve, it's not too slow. \n\nThere is a massive difference between \"slower than C\" and \"too slow for practical use\". Most people seem to focus on the first, while for any actual purposes, the only thing that matters is the second. \n\nSo, no, absolutely Python is not too slow. If optimal utilization of resources is one of your design goals, it's not the best option. But most of the time, it isn't.", "id": "hng44p4", "parent": "t3_ra2aqh", "vote": 5}, {"content": "I don't think it matters until you have to buy additional hardware or processing time to meet your needs. It doesn't matter so much if one computer is 99% idle most of the time vs 5% of the time. You still had to buy that computer or server. Buying two or three extra machines isn't a huge cost. \n\nWhen the difference in hardware and power is 10 servers versus 1,000 - then it really starts to matter", "id": "hng7sle", "parent": "t3_ra2aqh", "vote": 3}, {"content": "Most of ML libraries are written in low level languages like C, C++ etc. So in your case you use python to control flow, where most heavy calculations are done in heavy optimized low level code. \n\nSo I do not think that you could meet a scenario where using python may significantly impact on performance.", "id": "hng9o58", "parent": "t3_ra2aqh", "vote": 3}, {"content": "Bad python > bad C\n\nIt doesn't matter if python is a little slower, it's still more reliable to write code in and faster to write code in than C. Speed of development trumps speed of performance for almost all applications. I'm an engineering manager at a robotics company and literally none of our server software needs to be fast. The robot software is real time, the server software just needs to be well written. \n\nAnd also people ask about speed but we are talking just CPU speed and not a lot slower but like 10%-20% more to get the same task done, so slow in terms of usage rather than slow in terms of time. I can still write my server software to answer complex queries in the same amount of time.", "id": "hnfymkx", "parent": "t3_ra2aqh", "vote": 5}, {"content": "Depends on your metric.  It's fast to develop in and that's generally the most expensive time.  When realtime performance is required, you don't generally reach for Python in the first place (rightfully so).\n\nAs such, you don't tend to see issues.", "id": "hng3m23", "parent": "t3_ra2aqh", "vote": 2}, {"content": "That's because ML doesn't use python. All the main libraries hand it off to something faster, C etc. It just looks like python.", "id": "hnggtfp", "parent": "t3_ra2aqh", "vote": 2}, {"content": "What I often need is a language which makes it simple to deploy my tools. This is the main reason why I'm more into Go lately. It's not the slowness of Python. Although Python is not the fastest language under the sun, it's fast enough for most use cases. And there are lots of optimized C libs for Python, especially when it comes to ML (so I don't wonder that the OP doesn't suffer too much here).\n\nWhat I'd like to see in Python is a flawless possibility to create self-containing executables. I know there're 3rd party libs and tools providing that feature but either there're not open source or they are  behind of the latest Python releases.\n\nSoftware deployment is one of the bigger issues with Python, not speed.", "id": "hnhrab0", "parent": "t3_ra2aqh", "vote": 2}, {"content": "Do you find it to be too slow?\n\nIf you do find it to be too slow, then it's too slow.\n\nIf you don't find it to be too slow, then it's not too slow.", "id": "hnhwnpn", "parent": "t3_ra2aqh", "vote": 2}, {"content": "Most of Python's libraries are written in C or C++, which is very fast. If you want a language as easy and flexible as Python but with much better native performance, you may want to try Julia.", "id": "hnih6uq", "parent": "t3_ra2aqh", "vote": 2}, {"content": "I work in Python and Go in a mostly Python shop.  We have a few applications that decode and manage high volume binary messages where Python is really slow and Go is really fast.  The applications aren't exactly the same, but the difference is probably at least 10x and maybe more.  However, most applications are just fine in Python, and it is a lot quicker to do things in Python and a lot easier to hand off to team mates.\n\nGenerally speaking, Python is almost always fast enough.  If you have a performance problem, you can look at other languages/techniques for performance.  If you do not have a performance problem, then don't worry about it.  If you have to do 1 thing every minute, there is no bonus for doing it in 2 seconds vs 57 seconds.  There are probably better things you can do for your business, like writing documentation and tests, than making things go faster than you need to.\n\nSpecifically, Python is the most popular language for ML and data science because it allows you to focus on the problem at hand with minimum fuss.  When you are running scipy or tensor operations on numpy data, all of the calculations are being run in highly optimized C code and then the results passed back to Python, so you get the best of both worlds.", "id": "hniqn4j", "parent": "t3_ra2aqh", "vote": 2}, {"content": "You are using C my friend, all ur libraries are written in that. Python is just the top level logic.", "id": "hnj2hcp", "parent": "t3_ra2aqh", "vote": 2}, {"content": "I have, once.\n\nA couple of jobs ago, I worked in a lab doing RF work.  The lab would do month-long test runs of their radios (they were for sending telemetry), and the data would get spooled to a NAS in a rack in the lab.  They were using MATLAB with the Parallel Computing Toolbox to do the data analysis.  Two things about the PCT are that they CUDA enable processing without having to modify your analysis code to use it (It Just Works^(tm), amazingly).  They were also using MATLAB Parallel Server to distribute the work throughout the building.  Just about every engineer's workstation had at least one and usually two nVidia Tesla GPUs installed to speed up the number crunching.\n\nFor budgetary reasons, management was investigating migrating to NumPy or SciPy because MATLAB licensing was making the folks with the checkbooks unhappy.  So, as an experiment, we rewrote the analysis code in Python using SciPy, chopped 48 hours of data out of a test run, and did a shootout to see which finished first.  MATLAB, unsurprisingly, was done in about an hour.  However, without having a buttload of GPUs in the entire building to throw at it, the SciPy test was terminated after about two weeks of runtime.\n\nI'm pretty sure that if we'd turned it into an actual R&D project we could have replicated most of what we used MATLAB for (ad-hoc clustering across the building, automatic GPU acceleration, and so forth) but management refused to try to spin up a brand new project, allocate resources, and suchlike.  After they crunched the numbers they figured that it was more cost-effective to keep using MATLAB.  And so they did until I left.", "id": "hnj7k84", "parent": "t3_ra2aqh", "vote": 2}, {"content": "No. But, yes, compared to some other languages.  Never will you notice any slowness, unless you don't right good code.  That can be done, in any language.  \n\nI saw a YT video of a raytracer written in C++, being code reviewed.  It was taking 7 1/2 minutes to render a scene & it was using all his cores pegging them at 100%. He changed one thing in the guys code and cut the core count down to half and it rendered the same scene in 22 seconds.\n\nThat's an extreme example.  But still.  :>", "id": "hng9t4p", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Obviously not for you. But it is really easy to gain a factor of 10 and with some work you can usually increase the performance by a factor of 100 when going to compiled languages. \n\n\nAnd it is not hard to argue that that difference is important. Training a ML algorithm for a day compared to a hundred days or over a weekend instead of a year is a very big difference. \n\n\nThe only reason you probably haven't noticed that is that you haven't written any performance critical code or you have, maybe unconsciously, limited yourself to problems that you can deal with in a reasonable time frame with pure python.", "id": "hngdemt", "parent": "t3_ra2aqh", "vote": 1}, {"content": "If you were proficient enough in the language then you'd know that the vast majority (or actually all?) of ML libraries are actually just python wrappers over C/C++ backends.", "id": "hnkd752", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Python is not slow.", "id": "hng0obh", "parent": "t3_ra2aqh", "vote": -5}, {"content": "You are asking the question on the Python forum \ud83d\ude05", "id": "hngtgug", "parent": "t3_ra2aqh", "vote": 0}, {"content": "Yes.\n\nA constant slowdown, as expected for an interpreted language, is acceptable.\n\nWhat is *not* acceptable is that refactoring your code to be more readable makes it significantly slower.\n\nFor example, it is very inefficient to call a function, so you have to perform inlining manually.", "id": "hnhg4ni", "parent": "t3_ra2aqh", "vote": 0}, {"content": "It depends on whether your app is CPU bound or I/O bound. If it is I/O bound there is no big difference as long as you use async libs because bottle neck exists in memory, disk or network.\n\nIf it is CPU bound like machine learning, due to green thread and GIL, you can not utilize multicore servers. In fact most of python libs like Tensorflow are implemented in C. Python is just wrapper for it. Try to learn Rust which allows you to realize how native threads and async work and affect performance.", "id": "hnjnjr9", "parent": "t3_ra2aqh", "vote": 0}, {"content": "This is the article I always refer people to when discussing this topic:\n\n[The myth that python is slow](https://www.pythonforengineers.com/why-is-python-so-popular-if-its-so-darn-slow/)", "id": "hnhrr44", "parent": "t3_ra2aqh", "vote": -1}, {"content": "sometimes and too much ram usage.", "id": "hng8cao", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I don\u2019t know much about this but wanted to add that all of the heavy duty code that runs on massive supercomputers (quantum mechanics,weather modelling,  astrophysics, comp biology, etc.) is still written in C and FORTRAN(!) as there is no value to moving to anything else in terms of gain in processing time. Yes, mostly data prep and analysis is done in Python (like that famous lady with that famous photo of a black hole with like a hundred hard drives of data and a Matplotlib image)", "id": "hngdufg", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I tend to write stuff that takes hours/days/weeks to execute each time you run them (after paralellization), and that are pretty much CPU-limited. In this cases pure python is kinda bad. Most of the times I'm actually calling C stuff, Fortran stuff or Numba accelerated stuff.\n\nIt will probably be better to go directly to C, but I feel the time needed to learn will make me lose a ton of short- and mid-term productivity, which I don't think I can afford.", "id": "hngg91k", "parent": "t3_ra2aqh", "vote": 1}, {"content": "In my experience speed is rarely the real an issue in software. Perhaps back in the day it was but with modern hardware not so much. That's not to say it's never an issue, there are certainly cases where squeezing every last drop of performance out of the code is necessary but as a general rule it's better to just write parallel code and throw processors at it until the problem goes away. What's really important is correctness. If you can have more confidence in your code being correct because you are using a high level language that's money in the bank.", "id": "hnggp5w", "parent": "t3_ra2aqh", "vote": 1}, {"content": "For web servers it's not bad. It worked okay on my home computer which I temporarily ran a server off of using Flask. I received about 120 visitors per minute(7200/hr), half of which were using a function on the webpage which continuously loaded data from the server every 15sec. The server was also pulling data from about 6 other servers. I tested loading it from other internet providers and it still loaded fast.", "id": "hnggyft", "parent": "t3_ra2aqh", "vote": 1}, {"content": "A very simple case in point: playing video. In python, playing one video stream, which is actually running the ffmpeg C bindings, consumes 2 to 4 times the processor that the exact same functions and system calls consume when using C/C++. I've been benchmarking the same operations in both languages, as I figure out what portions of my newer version of a facial recognition system should remain in C/C++ and what portions I can gravitate to Python. (The entire system was originally C/C++.)", "id": "hngh1jk", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Yeah, I\u2019ve definitely run into Python being slow; I started using Python around version 1.6 so I remember before the entire ecosystem you depend on existed. What happened is that people loved working with Python, so they built foundational libraries like NumPy in order to let them do computational work in C & Fortran but with a Python interface.\n\nThis led to a host of projects: Cython, Pandas, Dask, PySpark, TensorFlow\u2026all of them integrated inside Jupyter notebooks\u2026and you\u2019re right, no one cares if the thing you do 5 times in your program is 100x slower, because it may as well be a constant overhead. But the moment you need to do something that doesn\u2019t have an optimized implementation in a lower-level language, you\u2019ll find that your perf drops off a cliff \u2014 pure Python is just so much slower.\n\nIs that a problem? Maybe not. There are so many people using Python and its ecosystem that there\u2019s a reasonable chance your problem has been tackled somehow by someone. But the actual solutions to those problems aren\u2019t written in Python\u2014they\u2019re just given a Python API.", "id": "hngjhrd", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Fast or slow, it depends on how you use that.\n\nPython is a more than a script language while the Python interpreter can generate byte codes like what JAVA(JVM) does.\n\nThis kind of discuss has been there for a long time. You can do Python programming without naming the type of a veriable since every variable in Python is an object, which means it takes more memory and more computing time comparing to binaries generated by low-level languages, such as C/C++/C#. But the with Cython, you can name the type of variables while you code in Python.\n\nIn the latest Python 3.10+, you can predict type of input while you code.\n\n    def get_my_score(x: int | float) -> float:\n    \n        return x*1.23\n\nYou can predict the type of the input variable, also you can predict the output variable. This will make codes easier to read while the Python Interpreter can generate more efficent byte codes and it improves the performance.\n\nBut with Cython, the performance of your codes could be much closer C/C++. Around 90-98% in some core algorithms. Or you can just use the python modules compiled by C/C++ for the future projects.\n\nHere's is how we do projects.\n\n1. Code in Python\n2. Tune variables, memeory usage and iterations in codes\n3. Cython for high performance\n\nHere's my view of Python using in machine learning.\n\nWhen you want to train some models, yes, you could use C or C++ to do the same thing, but when you figured out the machanism of C/C++, you might have spent a long time on types and readability and your competitors might have already started to build their models or even finished and been ready for the next round.\n\nPython is coding language meant for all humans to pick up.\n\n\\----\n\nBTW, if you have performance bottle neck of Python codes, you can polish the codes using pure C and optimise the C compiler such GCC/Clang. Yes we all know assembly language performs the best but are you going to do that?\n\nPython gives you unlimited possibilities.\n\n\\-----\n\nEdited\n\nI have just tried Cython with Python on my M1 MacBook Pro.\n\nResults are:\n\nFor calculating primes, Cython with C performs 37 times faster than pure python.\n\nCython with C VS Cython with CPP, Cython with C wins by 2%.\n\nGo Python!", "id": "hngo5be", "parent": "t3_ra2aqh", "vote": 1}, {"content": "For what it's worth, I spent time working through Cisco's \"CML\" (Cisco modeling lab) a couple of years back, largly written in java from cisco's 'bragging' about it, and even small simulations took forever and a day (okay 30-40 minutes) to start.  The identical simulation in GNS3 written in python with the same QCOW switch and router images took around 8 minutes to start up.  Both were using qemu to run the images and the CML product shipped on a box running ubuntu 14.04.\n\nIf things seem slow it may have something to do with how they are written and the environment they run in as much as anything else.", "id": "hngofo5", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Too slow depends on your app and scale. I created an API for Amazon and while python is easy to work with, java just provided us the response latency we needed to run on the homepage. They have a really strict standard for how long it takes to load their page cuz each millisecond of load time translates to millions of dollars in revenue loss per year", "id": "hngtlom", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I start feeling the limitations of python when doing multithreading (or multiprocessing due to GIL) stuff, compared to languages like C++ or Java. For the rest (ML stuff and math stuff) numpy + numba are enough.  \n Sure, if there was a good alternative to the stack Tensorflow/Pytorch + numpy + matplotlib for C++ I'd not use python, since usually the code for solving this task has little to benefit from python's high level syntax and the equivalent C++ code would look more or less the same and run a lot faster", "id": "hngucex", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Well, you may happen to develop things where Python performance doesn't matter that much, not everyone is bothered by its limitations. \n\nSome situations where I had to work hard on optimizing it or even ditch it entirely:\n\n1. ML project where cleaning data was done in pure Python interactively. Once we reached certain scale, this turned out to exceed human patience limit of maybe 10s for page load.\n\n2. Another project used interactively (run, see results, tweak model parameters, repeat until you are happy with the results).\n\nKey things that made those projects different than many other done with Python:\n\n- large amount of data processed in pure Python\n- a domain that does not provides C libraries (so there is no no numpy/pandas).\n- interactive nature, where people need to wait to see the results before they can continue their work\n- users whose main job was dealing with those projects (if someone needs to wait long once a month, nobody would complain, but on a daily basis its just waste of people time).\n\nI did tons of projects without encountering those conditions too, however any large project will eventually hit those usecases somewhere, and the main disadvantage for me was that it made difficult for people to enjoy their work, or they couldn't do it at all at this speed.\n\nSpeeding up some nightly job most likely makes no sense, but being an obstacle for whole team is not the position you want to be in. Not everything can be offloaded to C libraries.", "id": "hnguk48", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Is it slow? A little bit yes, but does it matter day to day if something takes microseconds or milliseconds?  It really. Development time matters much more. And the stuff that needs to be high performing, can rely on libraries with c bindings to at least get them to a very reasonable level of performance (numpy, pyspark, ...).  So don't worry about it. Caching and a good infrastructure are better anyway", "id": "hngumxz", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Python is slow if you don't use fast libraries like numpy, pandas, and xgboost for example. Fast libraries are usually written in a language other than python and they have a python api so you can use them easily from python.", "id": "hngvegu", "parent": "t3_ra2aqh", "vote": 1}, {"content": "It depends. There are applications that are IO bound or memory bound, where your CPU is mostly idle. You would gain nothing from a more efficient program in that case.\n\nSimilarly, if if you are CPU bound, but the results are not time critical, and the general load on your system is low, then you might get to wait longer for your results, but at no other cost\n\nThe reality is that a lot of python packages are not pure, they are simply python wrappers around optimized c++. The shitty performance of the python interpreter is not actually that important for the performance of your application. It is simply used to implement the flow of data between various pieces of C++", "id": "hngzyvz", "parent": "t3_ra2aqh", "vote": 1}, {"content": "An example of where python can be slow:\n\nI have a cron job that runs a python script for grabbing metrics from an app that runs on hundreds of servers. I used to run the script once for each server, knowing that by forking I'd get \"for free\" concurrency. That was fine until the number of servers broke 400, at which point the interpretor overhead alone would bring the host to its knees. \n\nSo I refactored the script so I could pass in the servers that were due and run it in just one interpreter instance. That, unsurprisingly, fixed the memory and CPU utilization, but it would still take several minutes (like, 5-15 depending on the app load) to run to completion doing each server in sequence, one at a time. That part was easy enough to work around with async and now the script finishes in about 20 seconds, but that's beside the point...\n\nThe script gets metrics via REST API call to the app. The app itself takes up to a couple seconds to gather up the data and serialize it and there's nothing I can do to improve that. But, the `requests` library (and later `asks` when I went to async) has to do several object instantiations for every request and response. Overall, it's probably adding less than a quarter of a second per HTTP transaction. But, add that up 400 times and you get nearly an extra 1m:40s. (I now have over 500, btw.) \n\nNow, does that mean python is \"too slow\"? Well, as others have said ad nauseum, it depends. For my application, it's fine. The overhead of using python for the backend of my webapp is almost negligible compared the overhead of getting db records over the network and coordinating the various other APIs involved, so performance improvements in my code wouldn't really translate to visible performance improvements in UX. If, however, I were hosting the db on the same machine as the app and only had my own business logic to worry about, the game would be different: my code would be the only bottleneck. Even then, the nature of the app makes a big difference in what \"too slow\" means. If your app is a robo-trader or ticket scalper, python is probably too slow and you should use Go, instead. If it's yet another cat blog, you could do the whole thing with GNU awk and it wouldn't matter that much.", "id": "hnh5imd", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Keep in mind that most python data processing libraries don't actually do the data crunching in python. Libraries like numpy and pandas actually use C and fortran under the hood so it doesn't really matter if python is \"slower.\"", "id": "hnh7bqn", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Just a heads up- if you are an ML engineer you are mostly interacting with python wrappers of libraries written in other faster languages.\n\nNumpy/scipy/pandas - c, c++, Fotran\\*  \nTensorflow /pytorch etc - c++/c\n\nThe small services it sounds like you have written arround it also probably compile down to non python wrappers for some part of their day.  IE Flask, django, etc all do this.  When anything needs to be high performance in python its usually written and compiled in another language with a python wrapper.  \n\n\nEdit: Brain melted today and mixed fortran and COBOL", "id": "hnhb86o", "parent": "t3_ra2aqh", "vote": 1}, {"content": "There are lots of decent answers on this here, but I'd just like to note that Python's slowness is mostly in *instruction execution*, and a *lot* of computing tasks depend much more on IO than they do on processing instructions. This means that unless the thing that you're doing specifically *computes* things via CPU in Python code (instead of calling to C libraries) as its critical path, Python's slowness never really comes into play much as instead most of the time will come from waiting for hardware devices or network stuff to respond.", "id": "hnhg021", "parent": "t3_ra2aqh", "vote": 1}, {"content": "It's not really slow, but its slower than other languages, which matters only if youre doing large-scale stuff", "id": "hnhghnk", "parent": "t3_ra2aqh", "vote": 1}, {"content": "It's when you're dealing with A LOT of data (analyzing or moving it around) that you really start to understand Python's speed limitations.", "id": "hnhmd0l", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I'm just happy, as a ruby developer, to enjoy hearing people complain about something other than ruby being slow. =<^.^>*=\n\n/s", "id": "hnhpu0i", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I compared two apps for a huge data processing project. When running it was expected that the app should consume a large amount of the resources of the physical server. We scaled out horizontally. One app was written in C. The other was written in Java. Hardware requirements were about 2x for the app written in Java. Efficiency definitely matters sometimes,", "id": "hni5esq", "parent": "t3_ra2aqh", "vote": 1}, {"content": "By the time I start running into performance issues significant enough to care about, I find that I have been doing something stupid or wrong already, and fixing that tends to push the performance back into acceptable range.\n\nBig-O issues, doing unnecessary steps with strings, creating objects only to discard them a statement later while iterating on a loop, etc.\n\nThat said, my use case isn't typical anyway. Systems engineer in a bioinformatics space. The bottlenecks tend to be disk or network throughput or memory capacity rather than computational speed.", "id": "hniif97", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Is Ruby considered for doing the same things like python  .example ml ?and can it be faster", "id": "hnk2p1k", "parent": "t3_ra2aqh", "vote": 1}, {"content": "ML engineer you probably use tensor and keras, python in this case act more like an interface language. C behind the back is doing most of the work. \n\nPython can definitely be too slow for some stuff, for example, mastercard\u2019s transaction server. Honestly, any language can be too slow, hell there\u2019s a reason crypto miner use asic.", "id": "hnk8ykm", "parent": "t3_ra2aqh", "vote": 1}, {"content": "I've never had any issue where i thought speed was at all a problem personally.", "id": "hnkhgqv", "parent": "t3_ra2aqh", "vote": 1}, {"content": "Python is a very good language for getting to a working solution, and that always beats fast code that doesn\u2019t work. And for many many areas it\u2019s a good match for the domain and good for writing code that is flexible and reusable and gets you to a working solution quickly - including ones in which the parts that need speed can be provided by a nice C library, like ML. \n\nPeople coming here saying \u2018but pandas/ML/etc are just C wrappers\u2019 are missing the point - Python gets useful solutions for problems in those domains much faster than writing your ML code in pure C ever would, so it\u2019s not a very useful comparison. Now, if someone made a language as flexible and productive as Python, but speed that approached C on complex code, that would be a useful comparison - which seems to what eg Julia is aiming at. But just comparing it to C etc seems quite pointless.", "id": "hnkktpa", "parent": "t3_ra2aqh", "vote": 1}, {"content": "> what actually matters is life cycle cost for a software. It includes developer time, running time, debugging time and cost of resources\n\nTo put it another way, there are better languages than Python for making things work quickly. Python is a language for making things work, *quickly*.", "id": "hngrkmj", "parent": "t1_hnfsszh", "vote": 308}, {"content": "[deleted]", "id": "hnhsylf", "parent": "t1_hnfsszh", "vote": 31}, {"content": "Here's a very crude example of this at work. Consider adding `1` to every entry of a huge array of numbers. In python you could just use a big ol' list of lists, or, if you're smart, you'd use `numpy`. That latter is much faster:\n\n    import numpy as np\n\nfrom timeit import default_timer as timer\n\nSIZE = 10000\n\n\nprint(\"Starting list array manipulations\")\nrow = [0] * SIZE\nlist_array = [row] * SIZE\nstart = timer()\nfor x in list_array:\n    for y in x:\n        y += 1\nend = timer()\nprint(end - start)\n\nprint(\"Starting numpy array manipulations\")\na = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\nstart = timer()\na += 1\nend = timer()\nprint(end - start)\n\n\nOn my laptop:\n\n    Starting list array manipulations\n4.841244551000273\nStarting numpy array manipulations\n0.40086442599931615", "id": "hnh2xun", "parent": "t1_hngad3z", "vote": 22}, {"content": "That's one of the beauties of python, it was designed to be really easy to leverage new or existing binary libraries. So while it is maybe not pure python, it is part of what python was designed to do.", "id": "hnh9t81", "parent": "t1_hngad3z", "vote": 4}, {"content": "[deleted]", "id": "hnh1wlr", "parent": "t1_hngad3z", "vote": 0}, {"content": "Cython tries (reasonably successfully) to make up for the gap\n\nAnd Spyder is the best ide for *cythonising*", "id": "hnfwm5p", "parent": "t1_hnfrb9q", "vote": 29}, {"content": "Which is faster at adding big integers:perl or python?", "id": "hng166g", "parent": "t1_hnfrb9q", "vote": 4}, {"content": "> But if you are writing large loops (>> 10k iterations) in pure (C-)Python it is very slow - often a factor of 100 slower than in fast compiled languages.\n\nIt's even worse than that:\n\nThe loop does not need to be large. It just needs to be sufficiently hot.\n\nAnd 100x is just a single threaded penalty. Multi-threaded, multiply it by the number of cores available: so you'd get 800x to 3000x penalty", "id": "hnj0by7", "parent": "t1_hnfrb9q", "vote": 2}, {"content": "[deleted]", "id": "hnhpwo8", "parent": "t1_hnfrb9q", "vote": 1}, {"content": "\n>The real world situations where Python isn't fast enough, are really few and hard to find. \n\nDon't want to bash Python, I'm a big fan... but every single videogame out there is written in C++ (mainly) or other compiled language. Not really hard to find situations where Python is just no no.", "id": "hnhi51g", "parent": "t1_hng9te6", "vote": 19}, {"content": "The other place the time effect matters is in high-frequency trading where firms compete over offices that are physically closer by meters to the NYSE mainframe.", "id": "hnhawws", "parent": "t1_hng9te6", "vote": 3}, {"content": "Minor nitpick: you discover that your code is too slow when you benchmark it or performance test it (perf testing and benchmarking are essentially the same thing, but different names are used in different contexts). What profiling tells you is _why_ your code is as slow as it is.", "id": "hnfzvt8", "parent": "t1_hnfrnnk", "vote": 13}, {"content": "wow thats cool, didn't know about numba", "id": "hni5233", "parent": "t1_hngpzfh", "vote": 2}, {"content": "[deleted]", "id": "hng6pzy", "parent": "t1_hnfrs11", "vote": 12}, {"content": "[deleted]", "id": "hngmjz4", "parent": "t1_hng44p4", "vote": 0}, {"content": "At which point you should have the resources to hire developers to optimize your code and consider moving away from Python. Unless we're talking about specific niches such as videogames, I don't think it's worth it to worry about Python being slow when starting a project. That applies to most languages though, so of course, if it's a big organization with a lot of viable options, Python might not be the best pick.", "id": "hnga81p", "parent": "t1_hng7sle", "vote": 2}, {"content": "Any big loop will give you factor of 100 slowdown compared to any compiled language tho.", "id": "hngl303", "parent": "t1_hnfymkx", "vote": -2}, {"content": "That's more about your scope and setup and has little or nothing to do with programming language. When rendering or generating visually pleasing representations of stuff, it is really about what shortcut you can get away with and the image still being recognizable. E.g. limiting the amount of rays fired because your target resolution can't even show the difference.", "id": "hngbiwt", "parent": "t1_hng9t4p", "vote": 1}, {"content": "Compared to almost every single other popular programming language - yes it is.", "id": "hng3rsh", "parent": "t1_hng0obh", "vote": 3}, {"content": "Maybe check out Rust. It should be close to C in speed, but it protects you from the unsafe memory issues which are common in C. Especially if you are thinking about multi-threading. Plus it's modern, has most of the conveniences we are accustomed to in Python (easy to add libraries, test, build docs) and it's easy to interface with Python.", "id": "hngj1tr", "parent": "t1_hngg91k", "vote": 0}, {"content": "Even though I'd probably still prototype in python and then translate to C++ in order to avoid a continuous recompilation of the code.", "id": "hnguj3r", "parent": "t1_hngucex", "vote": 1}, {"content": "> Numpy/scipy/pandas - c, c++, COBOL\n\nIs there actually any COBOL code involved in any of numpy/scipy/pandas? Just curious, I'd enjoy learning about it.\n\nEDIT now, if you'd said Fortran, I'd have just glanced past it. But COBOL?", "id": "hnih3in", "parent": "t1_hnhb86o", "vote": 1}, {"content": "To cap it off, Python's undergone such a huge amount of development in the last 10 years, that if you want that quick solution in development/deployment/production, 90% of the time you can just drop it into an existing system where everything just works. Containerization and cloud development has only made this a more compelling architecture.", "id": "hngur4m", "parent": "t1_hngrkmj", "vote": 56}, {"content": "That's a very Pythonic way of saying that.", "id": "hnh6ufo", "parent": "t1_hngrkmj", "vote": 24}, {"content": "For these reasons why i made Nim my go to language.", "id": "hnhyuj5", "parent": "t1_hngrkmj", "vote": 1}, {"content": "I've also been feeling this more and more as I've gotten more experience developing software. There are entire classes of bugs that just don't exist in a statically typed language, just like there are entire classes of bugs that don't exist in memory safe languages like Python (and unlike C).", "id": "hnhudyw", "parent": "t1_hnhsylf", "vote": 18}, {"content": "> For everything significant I prefer statically typed languages.\n\nYep, exactly why TypeScript was made for JavaScript. I think you are a certain level of insane if you take on a large project without typing and the IDE yelling at you before you run the code when you are using the wrong type.", "id": "hnhw3yt", "parent": "t1_hnhsylf", "vote": 9}, {"content": "Formatted edition:\n\n----\n\nThat latter is much faster:\n\n    import numpy as np\n\n    from timeit import default_timer as timer\n\n    SIZE = 10000\n\n\n    print(\"Starting list array manipulations\")\n    row = [0] * SIZE\n    list_array = [row] * SIZE\n    start = timer()\n    for x in list_array:\n        for y in x:\n            y += 1\n    end = timer()\n    print(end - start)\n\n    print(\"Starting numpy array manipulations\")\n    a = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\n    start = timer()\n    a += 1\n    end = timer()\n    print(end - start)\n\n\nOn my laptop:\n\n    Starting list array manipulations\n    4.841244551000273\n    Starting numpy array manipulations\n    0.40086442599931615", "id": "hnh89si", "parent": "t1_hnh2xun", "vote": 42}, {"content": "If someone knows how to make the markdown editor actually accommodate code blocks sensibly, please fix this mess.", "id": "hnh35ei", "parent": "t1_hnh2xun", "vote": 10}, {"content": "Edit: disregard my conclusions here, per the responses to this comment. Leaving the comment up so people can follow the discussion.\n\n~~Iterating through every item of the every list is not necessary. Instead, one could use the python built-in \"map\" and it would go much faster. Faster than using numpy, in fact. The numpy code is easier to read, of course, but not faster.~~\n\n    import numpy as np\n    from timeit import default_timer as timer\n    \n    SIZE = 10000\n    \n    print(\"Starting list array manipulations\")\n    row = [0] * SIZE\n    list_array = [row] * SIZE\n    start = timer()\n    # for x in list_array:\n    #     for y in x:\n    #         y += 1\n    list_array = map(lambda y: list(map(lambda x: x+1, y)), list_array)\n    end = timer()\n    print(end - start)\n    \n    print(\"Starting numpy array manipulations\")\n    a = np.zeros(SIZE * SIZE).reshape(SIZE, SIZE)\n    start = timer()\n    a += 1\n    end = timer()\n    print(end - start)\n\nOn my 10-year-old desktop:\n\n    Starting list array manipulations\n    2.6170164346694946e-06\n    Starting numpy array manipulations\n    0.6843039114028215", "id": "hnhsyg9", "parent": "t1_hnh2xun", "vote": -4}, {"content": "Every programming language has a foreign function interface that can speak to the C ABI, it's a requirement for communicating with the OS via syscalls (without which you will not have a very useful programming language).\n\nHaving such an ABI does not make Python particularly special, and I would argue CPython's ABI is not particularly good. It's actually a very nasty hairball with a lot of unintuitive dead ends and legacy cruft. NodeJS is probably the market leader on this today for interpreted languages, and obviously compiled languages like D/Rust/Go/etc can use C headers and C code rather trivially.", "id": "hni5sri", "parent": "t1_hnh9t81", "vote": 7}, {"content": "Having python as a bridge layer isn't a bad practice. Serving models directly from python tends to be really slow (depending of course on the library and model itself, but I'm assuming some level of deep learning here) compared to using an actual inference engine (nvidia's Triton server has been great), so I would definitely not recommend that, but Python makes for great API code. Most of the deploys I've done have included python on the user interaction layer with the inference pipeline being built with heavier systems.", "id": "hnh5lnx", "parent": "t1_hnh1wlr", "vote": 3}, {"content": "An issue with Cython is that it gets slow again if you are calling Python functions from within. Thus, for good speed you need to make sure to use only C (or other compiled) libraries inside critical loops. \n\nAs a toy example I tried to write a Monte-Carlo pricer in Cython (and other languages). The issue with the Cython version was the nomal distributed random number generator:\n\n1. using the default Python one was slow\n\n2. I could not find a fast C library for it (I am sure there exists one, but search and integration effort is significant)\n\n3.  writing your own normal distributed random number generator based on \"standard\" algorithms gives you rather poor performance compared to optimized algorithms", "id": "hnfx5l8", "parent": "t1_hnfwm5p", "vote": 21}, {"content": "How does Spyder help with cythonizing?", "id": "hnh6yh4", "parent": "t1_hnfwm5p", "vote": 2}, {"content": ">Cython tries (reasonably successfully) to make up for the gap\n\nIt's fine if you Cythonize 10 lines of code. If you realize most of a module of 1000+ lines of code is slow, it's really a pain.", "id": "hngjk17", "parent": "t1_hnfwm5p", "vote": 2}, {"content": "I have no experience with Perl therefore I cannot answer your question.\n\nBigIntegers are slow in any language because they are not a native machine type. Consider using Int64 or Float64 instead.", "id": "hng4q5q", "parent": "t1_hng166g", "vote": 7}, {"content": "I can't comment on performance per se but python handles big integer seamlessly compared to other languages. It's a+b or a%b vs say Java BigInteger.add, etc. So shifting from math to code is a lot nicer in python.\n\nOne thing to keep in mind is that native exponentiation (**) has some limits. You'll want to use a fast exponential algorithm or similar. I just wrote my own but I'd be shocked if there isn't a good version in standard libs.\n\nI'm taking a masters level cryptography course and have implemented all the number and group theory in python,  going to Java when doing more standardized tasks because of Javas excellent crypto algorithm support.", "id": "hnh7730", "parent": "t1_hng166g", "vote": 2}, {"content": "While I don't have your specific answer, there seems to be a toss-up of which language is better, based on submissions to the Benchmark Game site.\n\nhttps://benchmarksgame-team.pages.debian.net/benchmarksgame/q6600/fastest/perl-python3.html", "id": "hng56wa", "parent": "t1_hng166g", "vote": 0}, {"content": "Yes, the GIL severely limits the applicability of multithreading.", "id": "hnkcbzm", "parent": "t1_hnj0by7", "vote": 1}, {"content": "Depends again what exactly you are doing.\n\nIf you are calling \\``max(my_large_numpy_array)`\\` it will be roughly 100 times slower than calling \\``np.max(my_large_numpy_array)`\\`.\n\nIf this matters for your application is another question. To answer this, you should profile your code, e.g. single functions with IPython \\``%timeit`\\` or a profiler ([https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)).", "id": "hnhrklc", "parent": "t1_hnhpwo8", "vote": 3}, {"content": "Yea, a couple of years ago I actually wrote a 2d vector type game (visually similar to asteroids), which as far as games go is pretty simple of course. I wrote the vector collision detection in Python, actually a couple of different naive implementations, basically porting over similar C routines from the available literature.\n\nI benchmarked the collision detection by adding a lot of actors on the screen and found it was adequate for my specific case but if the game I had designed had been busier, slower computers would have struggled pretty soon already.\n\nNow of course there's two things I could have done: first, optimize the routine - probably very much possible to do, but that would have probably taken way more effort than writing all the rest of the game, and also, if I had written the game in C++ in the first place, optimization would have not been necessary at all, the naive implementation would have been fast enough.\n\nSecond is the age old \"just implement the critical parts in C!\"... Yeah, if this had been for work, sure, whatever, but since I was doing this for fun in my spare time, no, absolutely I will not just do that.", "id": "hnhz6rl", "parent": "t1_hnhi51g", "vote": 7}, {"content": "C# for Unity.\n\nYeah, of course, large user base software will try to optimize things a lot, which is why OS, browsers, hand engines, are written on the platform that has the least performance cost.", "id": "hniit2a", "parent": "t1_hnhi51g", "vote": 2}, {"content": "The idea is that you don't know if it's actually your code that's slow before you profile it; it can be any layer in the request cycle.", "id": "hng1c4n", "parent": "t1_hnfzvt8", "vote": 14}, {"content": "> For nearly all analytics, AI, ML, scientific and similar workloads, no.\n\nI'd argue that *pure* Python *is* too slow for these workloads.  Thankfully, the intensive portions are implemented in C/Fortran and then made accessible to Python.  So, from an \"end-user\" standpoint they're using Python, but in reality they're actually using a lot of C if they're using Cython.  \n\nAnd I do think this is an important distinction to make, as there are some commercial codes with Jython APIs.  Because Jython is built on Java, it is incompatible with NumPy and SciPy, which are written in C/C++/Fortran.  You wouldn't want to write a pure-Python linear system solver in Jython rather than use one of the solvers provided by SciPy, because it would be too slow.", "id": "hnhrifx", "parent": "t1_hng6pzy", "vote": 4}, {"content": "Numpy, tensorflow, torch etc. are absolutely written in Python. They are python modules. They *use* a C/C++ library in the backend, but they are 100% a python interface. This is what annoys me so much. People are dissuaded from using python for anything 'real' while it's often *the* best way to interface with other libraries.", "id": "hnh1y7m", "parent": "t1_hngmjz4", "vote": 0}, {"content": "There are plenty of use cases outside of games where python is too slow, anything that relies on a loop to do things will get painfully slow. For me it was trying to do AI for a game where it has look n moves ahead to calculate it's strategy.", "id": "hngloje", "parent": "t1_hnga81p", "vote": 1}, {"content": "But that's where anyone who is experienced in python at all will say, do those in a thread or have a thread pool to do those, or if it's even bigger fan them out to other processes for the work, then it doesn't even have to be in Python for that specific task. For instance ZeroMQ has push/pull behaviour which is designed for fanning out tasks to multiple workers, then have the workers send the results back. There are multiple better approaches than just doing something in a loop and stopping there if you REALLY want performance", "id": "hngoezy", "parent": "t1_hngl303", "vote": 2}, {"content": "Which is not necessarily relevant.\n\nOn my laptop it takes about 35 ms to start-up a python program, but then there's little difference between a simple loop of 100 iterations vs one of 100,000 iterations.\n\nSometimes performance matters - like when I'm processing tens of billions of records a day - and just want to keep it economical.  But I'd guess that 95% of what typical programmers write on a daily basis doesn't need the absolute fastest performance.", "id": "hngtlky", "parent": "t1_hngl303", "vote": 1}, {"content": "Yes, absolutely.\n\nSorry.  bad example maybe.  But, I guess, what I was saying was that your code will be slow, if you don't program it correctly.  That, NO, you wouldn't notice any slowness because of python ... but, cause you probably did something \"wrong\".  Well, not necessarily wrong ... I think, you see what I mean.", "id": "hngpt2o", "parent": "t1_hngbiwt", "vote": 1}, {"content": "[deleted]", "id": "hng78qw", "parent": "t1_hng3rsh", "vote": 0}, {"content": "Ha good callout meant Fortran not Cobol wasn't functioning well yet today when I wrote it lol.", "id": "hnik3yf", "parent": "t1_hnih3in", "vote": 2}, {"content": ">Containerization and cloud development has only made this a more compelling architecture.\n\nBe warned that python is even slower than normal on a container, due to libseccomp screwing you over (I think with Spectre/Meltdown mitigations).", "id": "hnhy56j", "parent": "t1_hngur4m", "vote": 15}, {"content": "[deleted]", "id": "hnhv8d7", "parent": "t1_hnhudyw", "vote": 8}, {"content": "Try Nim", "id": "hnhz04w", "parent": "t1_hnhudyw", "vote": 0}, {"content": "Just prepend every line with four spaces and it works (triple backticks does NOT work on old reddit).\n\nIt's easiest to do this by just copying it into a code editor (like vim or vscode) and indenting all of the code once, then paste it into the reddit box.", "id": "hnh53ik", "parent": "t1_hnh35ei", "vote": 24}, {"content": "Unless you're running Python 2, this comparison is not at all the same, `map` returns a generator and not a list -- you're timing how long it takes to create a generator object, not how long it takes to construct the list. If you want an equal comparison, you need to wrap `map` calls with `list` -- just like you did with the inner `map`.\n\nIt is **much** slower.\n\n    \n    >>> from timeit import default_timer as timer\n    >>> \n    >>> SIZE = 10000\n    >>> \n    >>> def mapped():\n    ...     print(\"Starting map timing\")\n    ...     row = [0] * SIZE\n    ...     list_array = [row] * SIZE\n    ...     start = timer()\n    ...     # for x in list_array:\n    ...     #     for y in x:\n    ...     #         y += 1\n    ...     list_array = map(lambda y: list(map(lambda x: x+1, y)), list_array)\n    ...     end = timer()\n    ...     print(end - start)\n    ... \n    >>> def nomapped():\n    ...     print(\"Starting list timing\")\n    ...     row = [0] * SIZE\n    ...     list_array = [row] * SIZE\n    ...     start = timer()\n    ...     # for x in list_array:\n    ...     #     for y in x:\n    ...     #         y += 1\n    ...     list_array = list(map(lambda y: list(map(lambda x: x+1, y)), list_array))\n    ...     end = timer()\n    ...     print(end - start)\n    ... \n    >>> mapped()\n    Starting map timing\n    5.516994860954583e-06\n    >>> nomapped()\n    Starting list timing\n    5.158517336007208\n\nJust using `map` is only faster in some situations -- situations where you only need to iterate over a set once. If you're using numpy, you presumably are going to be reusing your arrays (well, dataframes) across multiple operations.", "id": "hni4v5l", "parent": "t1_hnhsyg9", "vote": 9}, {"content": "Now try calling `list()` on `list_array` and have it actually evaluate. ;-)\n\nOn my super duper M1 MBA:\n\n    Starting list array manipulations\n    4.422247292000001\n    Starting numpy array manipulations\n    0.1452333329999993\n\nedit: Nicer code IMO:\n\n    list_array = [[y+1 for y in x] for x in list_array]\n\nThis gives 2.79 on my system, better than that ugly map/lambda-line but still way slower than numpy.\n\nedit 2: Interestingly, the nested list comprehension is significantly faster than the simple for-loop.", "id": "hni66pt", "parent": "t1_hnhsyg9", "vote": 8}, {"content": "First off, system calls are just a dedicated assembly instruction in pretty much any platform. It doesn't require an ABI, you just load the ID of the syscall that you want to make into a register and then make the call. Very simple.\n\nAs for the NodeJS ABI, it isn't great. Python's feels much cleaner in my opinion. If it's too much of a hassle to handle directly, just take a look at pybind11. It's a header only library that makes the interface extremely intuitive to use. Jack of Some has a good video overview of it if you're interested in learning more.", "id": "hnia3aq", "parent": "t1_hni5sri", "vote": 2}, {"content": "[deleted]", "id": "hng69rg", "parent": "t1_hnfx5l8", "vote": 11}, {"content": "Because it automatically has a Cython backend, loads the Cython extension, is written in Python\n\nPyDev is good too\n\n\nBecause of the strong Cython integration Spyder and PyDev execute code much faster than the other ides", "id": "hnk1jfa", "parent": "t1_hnh6yh4", "vote": 1}, {"content": "If someone is asking about BigInts, using Int64/Float64 is likely not going to be suitable. At least to my knowledge, BigInts are mostly used by crypto algorithms (like for RSA keys). So unless there's some low-level bit math that I am not familiar with, you cannot just swap in even 64b ints.", "id": "hngmook", "parent": "t1_hng4q5q", "vote": 8}, {"content": "I was actually baiting a little bit.\n\nTake a look at this thread.:\n\nhttps://i.reddit.com/r/perl/comments/qejoud/perl_vs_python_summing_arrays_of_numbers/\n\nThe difference in speed is perplexing.\n\nETA: I cut my teeth on perl 20 years ago.  I've done only hobby projects in python and find it so much easier to work in, but... it goes all over me when people talk about how python is adequately fast.  Anything is adequately fast if you throw enough cpu cycles at it.", "id": "hnl5d13", "parent": "t1_hnh7730", "vote": 1}, {"content": "On the big integer test (aka pidigits), it's worth noting that that they used GMP bindings there. Because of that, that benchmark becomes mostly a testing of FFI speed and not how long a typical program written in that language will take.", "id": "hnhv4zt", "parent": "t1_hng56wa", "vote": 3}, {"content": "The Unity engine is almost entirely C++. It then loads Mono as a scripting runtime, with the C# API using binding to the C++ stuff.\n\nC# is at least an order of magnitude faster than Python either way.", "id": "hnj5rbz", "parent": "t1_hniit2a", "vote": 1}, {"content": "But if performance is your problem, then ultimately it's all \"your code\", even if the bottleneck turns out to be an external call to a third party service. It's then your job to find a way to make that faster (maybe you can cache the result of the third party call? Or there's some way you can optimise the query you're making? Or you can find a way to avoid the call entirely?)", "id": "hng7apg", "parent": "t1_hng1c4n", "vote": 1}, {"content": "[deleted]", "id": "hnh91lm", "parent": "t1_hnh1y7m", "vote": 2}, {"content": "Yeah, that's reasonable... Although there are alternatives to loops for a lot of things.", "id": "hnhgtfx", "parent": "t1_hngloje", "vote": 1}, {"content": "This isn't a trivial approach, I don't think it is really intended for an industrial working on a project, and more for enterprise who need to somehow scale their python codebase. I only say that limits of performance can be reached very easily in some real world tasks, which isn't really the case with compiled languages.", "id": "hngq8uk", "parent": "t1_hngoezy", "vote": 2}, {"content": "all good ;)", "id": "hngzmi0", "parent": "t1_hngpt2o", "vote": 1}, {"content": "I can't speak for the others, but I'm not sure this holds true for the most recent release of PHP.", "id": "hnhrcir", "parent": "t1_hng78qw", "vote": 0}, {"content": "Python outperform Java? In what universe?", "id": "hni7jra", "parent": "t1_hng78qw", "vote": 0}, {"content": "I didn't know that! \n\nWhen researching this further, [I read you can set `seccomp=False` on `docker run`.](https://betterprogramming.pub/faster-python-in-docker-d1a71a9b9917) \n\nThat does open you up to security vulnerabilities, so use it at your own risk. It does actually seem to be faster using containers on Windows when using this \"fix\".", "id": "hni1gn1", "parent": "t1_hnhy56j", "vote": 13}, {"content": "How is a container significantly different from local development on the same OS?\n\nIs it a default Docker runtime setting? Most K8s clusters default to CRIO. Is this issue present there too?\n\nUpdate: seccomp is not enabled by default as it is in beta for K8s 1.19; see https://kubernetes.io/docs/tutorials/clusters/seccomp/", "id": "hnimx1k", "parent": "t1_hnhy56j", "vote": 4}, {"content": "Yeah, that's why I'm looking closely at where I can maybe try out Rust in a production project. It checks all of those boxes and is just a general pleasure to work in. Over the last year or two it has finally reached the point of broader ecosystem maturity. Unfortunately, none of my coworkers know Rust so that kind of makes things problematic.", "id": "hnhw4gw", "parent": "t1_hnhv8d7", "vote": 5}, {"content": "Wow, good point. I totally missed the outer list() call.", "id": "hni6rie", "parent": "t1_hni4v5l", "vote": 5}, {"content": "> First off, system calls are just a dedicated assembly instruction in pretty much any platform. It doesn't require an ABI, you just load the ID of the syscall that you want to make into a register and then make the call. Very simple.\n\nGood luck passing anything to the kernel if you can't follow the ABI requirements. On Windows, _the only_ well defined way to make syscalls is window.h and kernel32.dll, which is a C ABI and requires following both the layout and calling convention requirements. On *Nix all the structs are also in C header files and require following C ABI layout requirements at least, but as a practical requirement if you want your code to be linkable at all you'll follow the calling conventions too.\n\n> As for the NodeJS ABI, it isn't great. Python's feels much cleaner in my opinion. If it's too much of a hassle to handle directly, just take a look at pybind11. It's a header only library that makes the interface extremely intuitive to use. Jack of Some has a good video overview of it if you're interested in learning more.\n\nI have an opinion because I've used them extensively, SWIG remains the industry standard and hides the pitfalls of the Python ABI. PyBind is fine if your codebase is C++ and you don't want to use SWIG or figure out how to expose your API under `extern C`.\n\nNone of this really addresses my point though, let's look at a simple example that implements a print function:\n\n    #define PY_SSIZE_T_CLEAN\n    #include <Python.h>\n    \n    static PyObject *print_func(PyObject *self,\n        PyObject *const *args, Py_ssize_t nargs) {\n      const char *str;\n      if(!_PyArg_ParseStack(args, nargs, \"s\", &str))\n        return NULL;\n      puts(str);\n      Py_RETURN_NONE;\n    }\n    \n    static PyMethodDef CPrintMethods[] = {\n      {\"print_func\", (PyCFunction) print_func, METH_FASTCALL},\n      {0}\n    };\n    \n    static struct PyModuleDef CPrintModule = {\n      .m_base = PyModuleDef_HEAD_INIT,\n      .m_name = \"CPrint\",\n      .m_size = -1,\n      .m_methods = CPrintMethods,\n    };\n    \n    PyMODINIT_FUNC PyInit_CPrint(void) {\n      return PyModule_Create(&CPrintModule);\n    }\n\nFrom the very beginning, we need `PY_SSIZE_T_CLEAN`, why? Weird legacy cruft that should have gone away ages ago.\n\nThe function parameters are reasonable enough, but what's this `_ParseStack` nonsense and why is it prefixed with an underscore? Simple, there are [a dozen ways to handle the arguments CPython passes you](https://docs.python.org/3/c-api/arg.html), half of them are undocumented, and all the \"modern\" APIs used internally are `_`-prefixed because the CPython team is afraid of declaring anything useful as stable.\n\nThe rest of the function is simple enough so we can look at the remainder of the module. The first oddity to notice is the `{0}` element of the `PyMethodDef` table. These tables are null terminated in CPython, no option for passing lengths. Also this `METH_FASTCALL` weirdness. Turns out [there are a _lot_ of ways to call a function in Python](https://docs.python.org/3/c-api/structures.html#c.PyMethodDef), which is weird for a language that espouses \"one right way\". The one right way most of the time is `METH_FASTCALL`, which is why it is of course the least documented.\n\nFinally `PyModuleDef` which is a helluva struct, I draw your attention to `.m_size` only because it relates to CPython's ideas about \"sub-interpreters\". Sub-interpreters are a C API-only feature that's [been around since the beginning](https://www.python.org/dev/peps/pep-0554/) that I have never seen anyone use correctly, and yet make their presence known throughout the API. Setting this field to `-1` (which, you might not be able to figure out from its name, forbids the use of a given module with sub-interpreters) is my universal recommendation.\n\nThis is just a simple print module, literally everything in the raw Python ABI is like this. There's always 8 ways to do a given thing, often times with performance implications, and without fail the best option is the least documented one. There's tons of random traps and pitfalls like knowing to include `PY_SSIZE_T_CLEAN`, and may the Lord be with you if you need to touch the GIL state because no one else is coming to help.", "id": "hnif5d0", "parent": "t1_hnia3aq", "vote": 6}, {"content": "I did it before the Numpy release 1.19 came out, but good to know for the next time. Thanks!", "id": "hng9npg", "parent": "t1_hng69rg", "vote": 16}, {"content": "Fair enough. \n\nHonestly, everyone who says python is adequately fast just hasn't come across a situation where it matters in my opinion. And as you elude to - the ongoing march of CPU progress means \"lots of CPU cycles\" is a shorter and shorter amount of wall time. \n\nCase in point: I was working on an MLaaS system - we had an **amazing** new feature to add to the product. Problem was that it required ~ n^2 input changes + re-score. \n\nThe initial implementation, run-time went from <20m on a test set to 8 hours. \n\nOr doing some academic cryptography and having to wait long minutes for the totient to compute.", "id": "hnl9zpu", "parent": "t1_hnl5d13", "vote": 2}, {"content": "I dunno, almost every solution uses GMP, from what I can see. Even the C solution imports GMP into the code space for use. Rust didn't, but C, C++, Pascal, Fortran, some Chapel solutions, and even Ada. \n\nI get your point, that it isn't \"real code\" if you're passing all your work off to another library, but if the playing field is completely fair of course they're all going to use roughly the same solution and/or utilities. The example I ~~linked~~ read was the n-body problem, which I think more accurately represents the limitations of the language, though I understand your point that Pi Digits is a more strictly mathematical problem.\n\nTake it or leave it, but I find the site to be a good resource for comparisons of the relative strengths and weaknesses of a given language.\n\nEdit: I did NOT link the n-body problem, that was just the one I spent the most time reading after linking the fastest. Woops", "id": "hni9hhg", "parent": "t1_hnhv4zt", "vote": 1}, {"content": "Well, when you boil it down to it, CPython is entirely C, and Python is just a scripting runtime, with the Python API using binding to the C stuff.\n\n> C# is at least an order of magnitude faster than Python either way.\n\nDepends on what you do and how you do it. C# is just C#. Python can be so many things. It can be CPython, PyPy, CPython with `@numba.jit`, CPython with C libraries like Numpy, etc. Its native types are more powerful than in .NET, which allow fast operations on data types for which you have to install 3rd party libraries on .NET. Used correctly, it can surpass the performance of .NET.\n\nFor instance, how difficult, and how fast will be your best attempt at determining the 1 millionth Fibonacci number?\n\nIn Python, on my desktop, it's 5.21 seconds using a 5-line function. Or 0.15 seconds using a 4-line long function and an import (numpy).\n\nFor 10 million'th number, it took 5.63 seconds to compute the number.\n\nNow I can do that in Python because it's fast, and it's good at numbers. It took this code to calculate the n'th Fibonacci number:\n\n    import time\n    import sys\n    import numpy as np\n    import math\n\n\n    def fibfast(n):\n        base = np.matrix([[1, 1], [1, 0]], dtype=object)\n        result = np.linalg.matrix_power(base, n)\n        return result[0, 0]\n\n    if __name__ == \"__main__\":\n        n = int(sys.argv[1])\n        start = time.monotonic()\n        val = fibfast(n)\n        end = time.monotonic()\n        print(f\"{end - start:.4f} seconds\")\n        print(f\"{math.ceil(math.log10(val))} digits\")\n\n\nShow me how you can get comparable results in C#.\n\nAnd it's not just this damn Fibonacci problem. In many places, Python is just really fast, with its infrastructure of really fast stuff.\n\nWhat's slow is the Python code itself, and I have only 3 lines in a function that takes up 99% of the time.", "id": "hnjat8d", "parent": "t1_hnj5rbz", "vote": 1}, {"content": "But this is about *python being \"too slow\"*. It's not about \"your application being too slow\". \n\nYou need to bring out a profiler (or enable/make some performance logging) to know whether it's your code or any of your dependencies that is the problem.\n\nBenchmarking/performance testing won't give you any insight into whether it's your code (the Python part) that's being slow (which is what OP is referring to).", "id": "hng8jp2", "parent": "t1_hng7apg", "vote": 4}, {"content": "There are plenty people on Reddit who tell starters to not learn Python because it's slow. They're the same type of people who say you can only code if you use vim.", "id": "hnhcqc7", "parent": "t1_hnh91lm", "vote": 1}, {"content": "> This isn't a trivial approach\n\nErr have you seen the code for ZeroMQ usage? It's 100 lines of copy paste code for the most part. \n\nThe overall point I'd make is there are 100 ways around Python's slowness, there aren't any ways around C/C++...etc when it comes to ease of development. You can implement your performance critical piece in C if you want and just wait for the results in Python, there are a million ways to fix this but ease of use trumps everything. \n\nI've had this argument 1000 times since I started using Python. If you have enough Python devs you don't need NodeJS (for the most part), you don't need C/C++ (for the most part) you just need Python and some glue and you will be able to do everything. It doesn't address speed well but as a general purpose language it's good enough for everything and everywhere but the very most perf sensitive applications. I wouldn't be timing rocket stages with it but I'd very much say it could be used for the basic rocket internals other than control for example.", "id": "hnh6ij2", "parent": "t1_hngq8uk", "vote": 3}, {"content": "[deleted]", "id": "hnikz05", "parent": "t1_hni7jra", "vote": 0}, {"content": "You can, yes. But the protections are there for a reason. We're currently having this debate at work. The likely outcome is to run most of our code on a separate network segment with seccomp disabled, and leave it enabled for anything running in a public facing DMZ.", "id": "hni4xih", "parent": "t1_hni1gn1", "vote": 11}, {"content": "Or run the workload on ARM.", "id": "hnim1qb", "parent": "t1_hni1gn1", "vote": 3}, {"content": "Try Nim", "id": "hnhz24h", "parent": "t1_hnhw4gw", "vote": 1}, {"content": "Come on now. That's not how the internet works. You can't just concede that you were wrong. You've got to double down and start throwing insults around. What is this, amateur hour?", "id": "hnkktqi", "parent": "t1_hni6rie", "vote": 3}, {"content": "Ah, I see. I had heard low level work in windows was a horribly disgruntled mess, I didn't realize it was quite that bad though. In unix and unix like systems you just load the registers and issue the call, nice and simple.\n\nAs for the \"legacy cruft\" and undocumented stuff, there's a reason for that. Avoid touching those, they're almost always bad practice or deprecated and are just kept around for backwards compatibility or some niche use case.", "id": "hniixzc", "parent": "t1_hnif5d0", "vote": 1}, {"content": "[This](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/regexredux-python3-2.html) \"Python3\" implementation?\n\n    # We'll be using PCRE2 for our regular expression needs instead of using\n    # Python's built in regular expression engine because it is significantly\n    # faster.\n    PCRE2=CDLL(find_library(\"pcre2-8\"))\n\nThe [_actual_](https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/regexredux-python3-1.html) Python3 implementation is ranked #20, just behind the slowest Java implementation and almost twice the runtime of the fastest Java implementation (both of which use the Java standard library).", "id": "hnjkfop", "parent": "t1_hnikz05", "vote": 1}, {"content": "Those penalties aren't as great on AMD processors if I am not mistaken.", "id": "hnigpno", "parent": "t1_hni4xih", "vote": 4}, {"content": "The protections from seccomp aren't _crazy_ valuable. A lot of the default seccomp profile is duplicated by the capabilities that docker drops by default.\n\nKubernetes actually runs containers in unconfined seccomp by default.\n\nIf you really want to go for security you should ensure your containers run as non-root and use `--security-opt no-new-privileges` which will render seccomp superfluous.", "id": "hnjiw8k", "parent": "t1_hni4xih", "vote": 5}, {"content": "You have to actively dodge the cruft, `PY_SSIZE_T_CLEAN`/setting `m_size = -1`/null terminated tables. That's what makes it bad.\n\n`METH_FASTCALL` is part of the [stable API](https://docs.python.org/3/c-api/stable.html), it shouldn't be avoided, you should absolutely be using it. The dearth of documentation and the glut of other function calling options is because, again, the CPython API is a mess of ideas from the last 20 years.\n\nInternal functions like `_ParseStack` we could go back and forth about, suffice to say _lots_ of projects use them (including SWIG generated wrappers) because they're objectively better than their non-`_` brethren. The fact that all the internal code uses these APIs instead of dog-fooding the \"public\" APIs should tell you enough about how the Python teams feels about it though.", "id": "hnikdtz", "parent": "t1_hniixzc", "vote": 3}], "link": "https://www.reddit.com/r/Python/comments/ra2aqh/is_python_really_too_slow/", "question": {"context": "I work as ML Engineer and have been using Python for the last 2.5 years. I think I am proficient enough about language, but there are well-known discussions in the community which still doesn't fully make sense for me - such as Python being slow.\n\nI have developed dozens of models, wrote hundreds of APIs and developed probably a dozen back-ends using Python, but never felt like Python is slow for my goal. I get that even 1 microsecond latency can make a huge difference in massive or time-critical apps, but for most of the applications we are developing, these kind of performance issues goes unnoticed.\n\nI understand why and how Python is slow in CS level, but I really have never seen a real-life disadvantage of it. This might be because of 2 reasons: 1) I haven't developed very large-scale apps 2) My experience in faster languages such as Java and C# is very limited. \n\nTherefore I would like to know if any of you have encountered performance-related issue in your experience.", "id": "ra2aqh", "title": "Is Python really 'too slow'?"}, "resource": "Reddit"}, {"answers": [{"content": "I'm a big fan of python aswell, but I'll play the devils advocate:\n\n* What about maintainability? How does a large python codebase compare in ease of maintenance than a large codebase in another popular language?\n* What about program correctness? Is it easier to shoot yourself in the foot and introduce expensive bugs in python than in another language, for example one with static typing?\n\nTime to market is of course probably the most important factor, but if you lose all your velocity in maintenance after a while you lose your fast time to market.", "id": "dft1ybv", "parent": "t3_63c9e1", "vote": 12}, {"content": "Studies where college students are the entirety of sample set or simple algorithms are the extent used to identify productivity are lame.\n\nLarge Python applications are crap to maintain, debug, improve, refactor.\n\nRuntime errors that a compiler could have caught suck ass.  \n\nPython is great for small scripts and command line tools, although at this point, Go or Rust might be better.", "id": "dft42yd", "parent": "t3_63c9e1", "vote": 8}, {"content": "> It used to be the case that programs took a really long time to run.\n\nI had to stop reading after this. The guy is a moron. The above very much still applies. Ever heard of web development? Both servers and clients need to be _fast as *fuck*_, and rarely are they both. Does it mean it won't work? Of course not!\nBut it does mean that any potential customers will stop using whatever service or site, if it's too slow.", "id": "dfszvgi", "parent": "t3_63c9e1", "vote": 30}, {"content": "Your users care, if you have any.", "id": "dft2hqi", "parent": "t3_63c9e1", "vote": 17}, {"content": "Development is not limited to web development. If your video codec is slow, it sucks. If your weather prediction soft is slow, it sucks. If your ML algorithm is slow, it sucks. Anything that relies on successive iterations (the more iterations, the better the result) requires maximum speed.\n\nAnd guess what : if the author of your DBMS thinks speed doesn't matter, you're out of business.", "id": "dft83pj", "parent": "t3_63c9e1", "vote": 6}, {"content": "> and I Don't Care\n\ncool! I do...", "id": "dft29qp", "parent": "t3_63c9e1", "vote": 17}, {"content": "> Hardware is very cheap compared to your time. \n\nThat's not how any company's finance department looks at it.  They have to pay you, so you're basically a sunk cost.   Overworking your employees is basically free.  Hardware on the other hand is a line item expense.", "id": "dft1la7", "parent": "t3_63c9e1", "vote": 14}, {"content": "But what about power consumption?", "id": "dft1fni", "parent": "t3_63c9e1", "vote": 8}, {"content": "They're both equally important.  Development speed is important.  Application performance is important.", "id": "dft3o6v", "parent": "t3_63c9e1", "vote": 3}, {"content": "Or you can build lighting fast services with extremely high development velocity by using Go or Rust. I would HATE to maintain a huge service using a dynamically typed language.", "id": "dft1yq6", "parent": "t3_63c9e1", "vote": 8}, {"content": "His fundamental argument, that you always have a trade-off between execution speed and productivity, is flawed imo. It is certainly possible to have both high productivity *and* high execution speed, if you design the language well. This is the reason Google invented Go. It's the reason Mozilla invented Rust. This modern trend of old-fashioned statically-typed AOT-compiled languages but with modern, powerful abstractions has shown this premise to be false.\n\nAdditionally, I consider Python to be a good language for small tools and one-off batch jobs, but very unproductive for anything larger than around 1000 loc. Beyond that, I find the dynamic typing and general lack of structure to be prohibitive. I've on several occasions spent days chasing a bug caused by someone sending the wrong type of object into a function, which due to duck-typing only triggered the bug 20 files away (but only if the stars happened to align that day). The call site could even have been correct when it was written, as such a bug can be caused by someone making a breaking change somewhere else without even realizing it. If it were any statically typed language it would have been a compiler error instead, and those three days could have been productive. Python is a write-only language in my book.\n", "id": "dft3iim", "parent": "t3_63c9e1", "vote": 4}, {"content": "Give this programmer slow IDE that takes ungodly long amount of time to open or find anything and have noticeable delay when typing code.", "id": "dft2tto", "parent": "t3_63c9e1", "vote": 3}, {"content": "I love Lines of Code metrics ", "id": "dft3b2w", "parent": "t3_63c9e1", "vote": 1}, {"content": "No, Python does not make you productive. And this is what you really must care about. Python is a very restrictive, low level language that forces you to do things in a convoluted way, it encourage boilerplate and bars users from building higher levels of abstraction.\n\nAnd, on  top of all this shit, it's also slow. I cannot comprehend where all those dimwit fanboys are coming from.", "id": "dft399q", "parent": "t3_63c9e1", "vote": -5}, {"content": "Sure, maintainability can start to suffer, but if your project never gets to that point because time to market was too slow, then it doesn't matter. That's a good problem to deal with, cause it means you have success. Python now has typing though which can be used with MyPy to check the types like a compiler would. \nMaintainability can be fixed by moving to smaller pieces such as modules/Microservices as well.\n\nOn the program correctness aspect. Let's just assume the dynamic nature does cause a bunch of extra problems. Since it's faster to write, those bugs should also be dramaticly easier to fix. If your using CD and shipping often, then as long as you can fix bugs fast, it's still better off. (You WILL have bugs no matter what, static languages just protect you from a certain class of bugs)", "id": "dfv1q5f", "parent": "t1_dft1ybv", "vote": 1}, {"content": "> Large Python applications are crap to maintain, debug, improve, refactor.\n\n[[citation needed]](https://github.com/cheery/lever/tree/master/runtime)", "id": "dft8rnu", "parent": "t1_dft42yd", "vote": 2}, {"content": "[deleted]", "id": "dft782n", "parent": "t1_dft42yd", "vote": 2}, {"content": "Literally every website was faster ten years ago. ", "id": "dft21c0", "parent": "t1_dfszvgi", "vote": 15}, {"content": "The key observation is that you don't normally write the critical path in Python, or at least, only a small fraction of it. The web server is fast as fuck, exactly because most of it wasn't written in Python. The Python code really just orchestrates the underlying library and system calls, and most of that stuff was written in C or something similarly close to the metal.\n\nThe client is a slightly different story, but even there, the most whopping performance factor is not JavaScript execution performance, but network latency and layout.\n\nA 10% speed boost in code that spends 99% of its execution time waiting for network resources and library calls produces an effective speed boost of 0.1%. Meanwhile, avoiding 10% of the library calls in the first place produces a 9.9% speed boost.", "id": "dft3mwi", "parent": "t1_dfszvgi", "vote": 12}, {"content": "I wonder what you think of WordPress. Obviously it must be a technology that nobody uses because it's pretty slow on both server and client.", "id": "dfv16o5", "parent": "t1_dfszvgi", "vote": 2}, {"content": "on the other hand, zero customers can even start using your service/site if you don't have anything to ship. the point of the article is don't let early decisions on performance get in the way of shipping, which seems totally reasonable. with your web development example, you'll run into a hundred more problems scaling your app before you're truly handicapped by a particular language's runtime performance (at which point you'll be so familiar with the pain points and constraints that you'll know more about whats the best solution than any medium article)", "id": "dft15gn", "parent": "t1_dfszvgi", "vote": 5}, {"content": "I fail to see how your point applies to the article.\n\nNot everything needs to calculate with lightning speed down to the nanosecond level.\n\nThings may only have to be \"sufficiently fast\". And if you reach +twitter size and beyond, I am sure you can easily afford to transition into languages where speed is a primary objective, at the cost of fun and productivity.\n\n> But it does mean that any potential customers will stop using whatever service or site, if it's too slow.\n\nExplain this again with the rise of php please. I am sure wikipedia has not been written in C for a point.", "id": "dft05qs", "parent": "t1_dfszvgi", "vote": 3}, {"content": "[deleted]", "id": "dft3csl", "parent": "t1_dfszvgi", "vote": 1}, {"content": "[deleted]", "id": "dft1gzo", "parent": "t1_dfszvgi", "vote": -3}, {"content": "Overworking your employees costs you turnover (they are likely to leave) and intangible goodwill (why would I go out of my way for a company that overworks me?). Some finance departments might be myopic enough not to realise this, but it's true nonetheless ", "id": "dft2yts", "parent": "t1_dft1la7", "vote": 11}, {"content": "> That's not how any company's finance department looks at it\n\nThat's exactly how they will look at it when you'll tell them that they can have site working 2x faster if they will pay 2x the cost of hardware of 10x or more cost of people to write and maintain codebase harder for people to deal with.", "id": "dft6pzm", "parent": "t1_dft1la7", "vote": 3}, {"content": "...and latency. Buying more servers doesn't lower your average response time. Buying better servers doesn't help either because single-core performance is stagnant. ", "id": "dft2zq0", "parent": "t1_dft1fni", "vote": 10}, {"content": "I forget where I saw the stats (e.g. citation needed) - but I recall recently seeing a page abandonment rate that started to really matter at around 1-2s. Pretty much any modern web framework is going to be able to meet that until they start seeing some moderate scale.", "id": "dfv8guf", "parent": "t1_dft3o6v", "vote": 2}, {"content": "[deleted]", "id": "dft2j50", "parent": "t1_dft1yq6", "vote": 5}, {"content": "Funny how each comments speaks of statically typed language where you can build something that is fast and never mention C. I wonder why...", "id": "dft4qa3", "parent": "t1_dft1yq6", "vote": 2}, {"content": "Execution speed isn't really the problem with dynamically typed languages though; long-term developer performance is.\n\nPython is fast to write at first, you can quickly jot down \"something\" and then get it working, and peppered with a bunch of unit tests, it'll be of reasonably quality. The pain starts when you need to perform nontrivial refactorings; dynamic languages are absolutely terrible for refactoring, because they lack a formal system for expressing assumptions and assertions in such a way that the toolchain can reliably check them for you.", "id": "dft356h", "parent": "t1_dft1yq6", "vote": 4}, {"content": "[deleted]", "id": "dft7gq5", "parent": "t1_dft3iim", "vote": 2}, {"content": "> I find the dynamic typing and general lack of structure to be prohibitive\n\nOn large code bases, you don't need them if you make correct unit and functional tests, which you should do in any language anyway.\n\n(source: I'm working on a large Python code base)", "id": "dft3ud2", "parent": "t1_dft3iim", "vote": 1}, {"content": "> It is certainly possible to have both high productivity and high execution speed, if you design the language well.\n\nNo, it is not. It is possible to design productive language faster than Python maybe (for some people), it is not possible (or nobody did it yet) to design languages that provide top speed in class combined with top productivity. There are always trade-offs there. You can have the same argument we are having if you substitute python with any other language.", "id": "dft6huu", "parent": "t1_dft3iim", "vote": 1}, {"content": "> I've on several occasions spent days chasing a bug caused by someone sending the wrong type of object into a function, which due to duck-typing only triggered the bug 20 files away (but only if the stars happened to align that day).\n\nThe likelihood is that you're doing something stupid there and it's not a problem in the language. I have a repository consisting of 25000 lines of code total, written in two different dynamically typed languages. I don't have hard to-debug bugs, or difficulties to maintain or refactor the project.\n\nThe problem you describe is common in combinator libraries that transform something into objects and then generate something. I've considered creating a mechanism to tag the caller of a combinator, but so far it's been easier to just validate the info in the caller or treat the information on the destination such that it is valid in most ways.", "id": "dft9eum", "parent": "t1_dft3iim", "vote": 0}, {"content": "Not sure why you think python is very low level.\n\nIn any case, the biggest bonus with python for me is how straight forward everything is from top to bottom. It feels like it takes hours to set up a JavaScript project with npm correctly, yet python is dead simply. Just slap in a requirements.txt with maybe a virtual environment if you're being really fancy. Plus, that requirements.txt often has just a couple lines since python takes the reasonable approach that libraries should be LIBRARIES i.e. a collection of something, not one solitary thing.\n\nNot only is it easy to set up python, well written python libraries are generally also very simple to use. requests is a library that other languages have now since emulated because it is so incredibly easy to get going http://docs.python-requests.org/en/master/. For another example of a fantastic library, look at skikit-learn. I would argue that there is no machine learning library that has so many tools available and that is so easy to use.\n\nPython is not the best language for everything, but it is hands down the best language for writing short scripts that require mashing together a bunch of other tools. Science would progress half as fast if python or something like python did not exist.", "id": "dft3tj5", "parent": "t1_dft399q", "vote": 7}, {"content": " >Since it's faster to write, those bugs should also be dramaticly easier to fix.\n\nI'm not sure that I think this is true, faster to write does not necessarily equal easier to maintain.\n\nIn addition, software defects that reach production are dramatically more expensive to fix than defects that are caught in development, which might be something to consider.\n", "id": "dfv3zvd", "parent": "t1_dfv1q5f", "vote": 1}, {"content": "Google's experience with Python suggests that large Python applications are difficult to maintain (and that small Python applications tend to become large Python applications). That's a decent chunk of why they came up with Go. Unfortunately, they came up with Go.\n\nMypy might improve things significantly, but that's a relatively recent thing.", "id": "dftiz49", "parent": "t1_dft8rnu", "vote": 3}, {"content": "> Granted static typing helps avoiding type errors and a whole class of bugs.\n\nMore importantly, it informs you of these problems very early on, and even when your tests are not exhaustive. (Spoiler: your tests are not exhaustive.)", "id": "dftj19q", "parent": "t1_dft782n", "vote": 3}, {"content": "There's probably not much runtime checking that's going on in your typical go server. It'll probably be quite safe, type wise.", "id": "dft7g09", "parent": "t1_dft782n", "vote": 1}, {"content": "> If PHP can retrofit type guards, then any language should be able to.\n\nThat's an understatement. PHP devs have done an amazing job with static types, performance optimizations and much more. At this point, the PHP with which I started is almost non existent. Anyone that talks shit about PHP in it's current form needs to get out of his basement.", "id": "dft9tq9", "parent": "t1_dft782n", "vote": 0}, {"content": "Do you have a moment to talk about our Lord and Savior, NoScript?", "id": "dft3n7f", "parent": "t1_dft21c0", "vote": 12}, {"content": "Unless you remember what the average bandwidth was and take it into account ...", "id": "dft3pc0", "parent": "t1_dft21c0", "vote": 2}, {"content": "At least one person here who understand how to use Python smartly ! Thank you.", "id": "dft3qlr", "parent": "t1_dft3mwi", "vote": 3}, {"content": "WordPress is pretty fast.", "id": "dfvoyq6", "parent": "t1_dfv16o5", "vote": 1}, {"content": "I'm not sure whether rapid development only applies to interpreted languages like python anymore. In my personal experience, you can ship a service quite fast using languages like go or crystal. An added bonus is that they are both faster than, say, python, although that might not be of any concert during initial development.", "id": "dft1dte", "parent": "t1_dft15gn", "vote": 7}, {"content": "on the other hand, what if 90% of startups fails, because they think that performance doesn't matter and use prototype as their product?", "id": "dft1o3y", "parent": "t1_dft15gn", "vote": 7}, {"content": "I'm talking web development in PHP. It doesn't take a lot to make systems slow as balls. The language is not what determines this, but the code. I'm not arguing that we should be using C for everything, but saying that speed is no matter is plain wrong. Of course speed is important still, even today. If you have clients that deal with financial data, that's an obvious area, but that's not all. Imagine being a simple shared hosting company with a web bureau making the websites. If said websites are slow as balls, your clients are gone. Their sites need to be fast. Most of all they need to be responsive, but if they're slow, that'll affect the responsiveness. Performance matters a lot. Want to support a platform that isn't just 1-2 years old? Performance matters.", "id": "dft0wds", "parent": "t1_dft05qs", "vote": 4}, {"content": "> Not everything needs to calculate with lightning speed down to the nanosecond level.\n\nBut CPU time = heat being emitted (who cares about environment?) and electricity bills. Why should I spend the amount of heat and CPU time suitable for heavy computation on a trivial task?", "id": "dft2tn9", "parent": "t1_dft05qs", "vote": 3}, {"content": "But it never was and never will be most important. Sometimes it is second most important, sometimes 27-nth. Executing wrong code fast is useless.", "id": "dft4z3l", "parent": "t1_dft3csl", "vote": 1}, {"content": "I scan through it and all I read is speed doesnt matter, what if it does, it doesnt, what if it does, it doesnt, what if it does, it doesnt...", "id": "dft1o91", "parent": "t1_dft1gzo", "vote": 5}, {"content": "> 2x faster if they will pay 2x the cost of hardware\n\nOh if only that was possible. Not everything can be parallelized so easily. ", "id": "dfuq86b", "parent": "t1_dft6pzm", "vote": 1}, {"content": "> Buying more servers doesn't lower your average response time.\n\nIt does, when requests start queueing up. Same for servers with more cores.\n\nAdding more servers will also allow you to distribute them globally and load-balance at the DNS level, which also improves latency.", "id": "dft3j1c", "parent": "t1_dft2zq0", "vote": 3}, {"content": "Why not use `class` ?", "id": "dft4rq7", "parent": "t1_dft2j50", "vote": 2}, {"content": "Have you given the new type hinting a try (it also requires usage of `mypy`)? Seems to be promising (but I agree with you that when a codebase starts to get large a dynamic language can be more difficult to maintain).", "id": "dfv8iv3", "parent": "t1_dft2j50", "vote": 2}, {"content": "Well the BIG thing about this article was development velocity. Now, I'm a big fan of C - and use it when I can. But C is in NO WAY shape or form a language with a high development velocity. ", "id": "dftj7bm", "parent": "t1_dft4qa3", "vote": 3}, {"content": "Null pointer explosions. C is so old that the last 40 years of PL development have shown that you shouldn't use C at all unless unless you have a really good reason to do so.", "id": "dftelsq", "parent": "t1_dft4qa3", "vote": 1}, {"content": "Development velocity refers to the speed at which your team can build shit. When I was referring to dynamic languages (or my hate for them), I wasn't referring to their execution speed. I was referring to how much of a pain in the ass it is to manage huge codebases with a dynamic language. ", "id": "dftj9fu", "parent": "t1_dft356h", "vote": 2}, {"content": "The real pain I've had with Python is Unicode.\n\nIn other languages with static typing, there's a concrete difference between a byte array and a UTF* string. Python doesn't give me this. I have had a huge series of bugs related to that in a 500-line script. I reimplemented it in C# and D, and those problems just disappeared.\n\nDynamic typing is part of the problem, and weak typing is the rest.", "id": "dftjjkj", "parent": "t1_dft356h", "vote": 1}, {"content": ">> This is the reason Google invented Go.\n> \n> it's questionable whether developers are more productive using Go, there is absolutely 0 data on that matter.\n\nGoogle had sufficient problems with Python that they chose to create another language. Unfortunately, the language they came up with was Go. This does not make Python code more maintainable; it just sucks if you're working at Google.", "id": "dftj9fw", "parent": "t1_dft7gq5", "vote": 2}, {"content": "I write functional tests. It is faster (by about 1.5 minutes times the number of type errors) for me to have static typing.", "id": "dftjdt4", "parent": "t1_dft3ud2", "vote": 3}, {"content": "Now this unit testing religion... Dynamic languages zealots are so predictable.", "id": "dft4qgt", "parent": "t1_dft3ud2", "vote": 3}, {"content": "I worked on Python code at Google. We had very good tools for navigating source code. It still took me five or ten minutes sometimes to figure out the return type of a function and what methods I could call on it.\n\nJava? I had to search for the symbol and then click on the return type. Fifteen seconds, maybe.", "id": "dftj6f5", "parent": "t1_dft9eum", "vote": 4}, {"content": "It is very low level indeed: rigid syntax, only the most primitive explicit control flow, no functionality whatsoever for building higher level abstractions.\n\nAnd, if you compare it to something like Mathematica or even Axiom, it still sucks a lot. Yes, there are tons of libraries, but language itself is so poor that it does not help much.", "id": "dft4jkm", "parent": "t1_dft3tj5", "vote": 0}, {"content": "I would hope you would catch all type based errors in development/staging... Because the only way that error is making it to prod is if you never tested it in any way shape or form. ", "id": "dfv48aq", "parent": "t1_dfv3zvd", "vote": 1}, {"content": "We don't know whether the results of Google's studies are reproducible on this subject. After all they came up with Go.", "id": "dftzkmn", "parent": "t1_dftiz49", "vote": 3}, {"content": "Unless you use interface{} anywhere.", "id": "dfteeye", "parent": "t1_dft7g09", "vote": 2}, {"content": "it's not enough anymore, even *fonts* take days to load nowadays (because, you see, the fonts on your computer are not good enough)", "id": "dft87mb", "parent": "t1_dft3n7f", "vote": 5}, {"content": "> you can ship a service quite fast using languages like go or crystal\n\nThis is true for a path that has been well travelled. Doing things a lot of other people do in those languages is indeed easy. Stray from that, and python is still way more productive.", "id": "dft50ms", "parent": "t1_dft1dte", "vote": 3}, {"content": "I agree that the \"productivity\" argument for Python is overstated, but again, the core argument I'm interpreting from the article is that you should prioritize what will get code out the door before something like  language runtime performance. ", "id": "dft23xp", "parent": "t1_dft1dte", "vote": 2}, {"content": "then we've shifted to bizarro world where the rules have changed? in this world, the vast majority of startups don't fail because their server response times were too long", "id": "dft1xct", "parent": "t1_dft1o3y", "vote": 4}, {"content": "> If said websites are slow as balls, your clients are gone. \n\nIf you are doing the same thing as a lot of other people, maybe.\nIf you provide service that is not easy to replace, getting it right matters far more than getting it fast, and people are more likely\nto leave you for missing/inadequate functionality than speed.\n\nAlso I've been working on optimising various sites, and I've got\nfar more benefits from improving logic (avoiding doing unnecessary\nthings at all, offloading them from request, using more appropriate data storage) than from making any executed code doing the same thing just faster.", "id": "dft4xo3", "parent": "t1_dft0wds", "vote": 3}, {"content": "You got me. That's a great point. I guess I will just write everything in assembly now. \nYou just had to pull the environment card...", "id": "dfv2bjj", "parent": "t1_dft2tn9", "vote": 1}, {"content": "Sometimes it's about making the user wait 2 minutes versus 30 minutes.  It's not nice to waste the end user's time.", "id": "dft5fvb", "parent": "t1_dft4z3l", "vote": 5}, {"content": "Wow, you should actually read it then. Because I address the \"but what if it really does\" in the last section.", "id": "dfv1x4t", "parent": "t1_dft1o91", "vote": 1}, {"content": "That's a separate issue; I'm talking about the time users spend waiting for jobs to complete.  For example, when you upload a video to imgur/gfycat, you have to wait a few seconds to a minute for it to be reencoded. \n \nSpeed-of-light latency is outside my scope. If requests spent any appreciable time in a queue, I'd be very sad :P", "id": "dft3skl", "parent": "t1_dft3j1c", "vote": 4}, {"content": "Yes, but I wasn't referring to your use of the word \"velocity\" (I know perfectly well what that word means), but to the \"lightning fast services\" part. Dynamic languages tend to be slow, but they're still fast enough; execution speed is not the problem, long-term velocity is.", "id": "dftw049", "parent": "t1_dftj9fu", "vote": 1}, {"content": "Are you talking about Python 2? ", "id": "dfuqa52", "parent": "t1_dftjjkj", "vote": 1}, {"content": "Wait .. I remember reading a discussion (troll?) you had with someone else a while ago ... DSLs, right ?\n\nYou realize that by being so negative, you're advocating your cause very poorly, right ? You're probably even hurting it a lot.", "id": "dft6dye", "parent": "t1_dft4qgt", "vote": 1}, {"content": "I use dynamic languages and I don't agree with him. You see large unit and functional tests often to be just superfluous fluff that doesn't matter.\n\nHas your mental health declined lately? You seem whole lot more cranky than usual.", "id": "dft9t4z", "parent": "t1_dft4qgt", "vote": -2}, {"content": "I tend to ask myself \"what the program is doing?\"\n\nNot \"what type does the program return?\"", "id": "dftzn3i", "parent": "t1_dftj6f5", "vote": -2}, {"content": "Actually I don't think you know what low-level means. The fact that you can call a language with Python's first-class functions, OO features and data structures \"very low level\" is evidence that you're either a troll or a complete novice.", "id": "dftiphp", "parent": "t1_dft4jkm", "vote": 5}, {"content": "That's pretty hard to really guarantee. You can check that a function/method handles the expected duck typed object and some error cases if it isn't the right one - but ensuring that _all calls_ actually provide the right one is pretty hard to do. Especially when deadlines loom and things need to be shipped, or you have some JRs and the SRs can't keep up on all the code reviews.", "id": "dfva7le", "parent": "t1_dfv48aq", "vote": 1}, {"content": "Anyone who has written Go for more than a few days hopefully doesn't do this anymore. Create a concrete interface and implement it.", "id": "dfv77fa", "parent": "t1_dfteeye", "vote": 1}, {"content": "Firefox has an option to prevent sites from selecting fonts other than those you explicitly choose. That should eliminate this problem.", "id": "dftjop3", "parent": "t1_dft87mb", "vote": 5}, {"content": "Maybe. But how Python is relevant here? If anything, it will delay the delivery, being such an unproductive language.", "id": "dft3co4", "parent": "t1_dft23xp", "vote": 4}, {"content": "Absolutely, and if python is your most familiar language, by all means, you should be using it if you need to get something out the door.  There's little point in first needing to spend time learning a new language when you're on a tight time budget.", "id": "dft2nd3", "parent": "t1_dft23xp", "vote": 2}, {"content": "not because response time were too long, but if you efficiency is too low, even great idea will suffer. If you need 5 aws server with 12gb ram to implement chat (and it takes 4 months because you are using cutting edge languages and frameworks for faster development), that doesn't help to satisfy customers.", "id": "dft2e4r", "parent": "t1_dft1xct", "vote": 2}, {"content": "Sure. If you have the right product and it is slow to the point that it matters to customers and the performance comes to how fast your code is executed and its better/cheaper to fix code than throw more hardware and it is more important than working on new features, than improve that. Until then, you have other things to worry about.", "id": "dft69te", "parent": "t1_dft5fvb", "vote": 1}, {"content": "Why can't both be a problem? The world is not back or white. For our SLAs, execution speed IS important. Just because a certain metric isn't important to you, doesn't mean that this concept is universal.", "id": "dfu12nv", "parent": "t1_dftw049", "vote": 3}, {"content": "Python 3. I did have several revisions in which it actually worked, but it was flaky. If I breathed on it wrong, I'd get encoding errors. And since most of what I dealt with was ASCII, it was rather hard to detect when I broke stuff.\n\nBetter testing would have solved it, but it was intended as a one-off script, and the data was coming from an external website that strongly disliked programmatic access.", "id": "dfvfb5l", "parent": "t1_dfuqa52", "vote": 1}, {"content": "He is clearly trolling. \n\n>You're probably even hurting it a lot.\n\n\nThat's the point. Check his comment history. ", "id": "dfuqca3", "parent": "t1_dft6dye", "vote": 3}, {"content": "It's ok to be negative towards religious zealots. Any dynamic typing fanboy is beyond redemption anyway, there is absolutely no hope to ever convince any of you, so why bother then?\n\nAnd, please note, we're only talking about static vs. dynamic here, no fancy high level stuff at all.", "id": "dft7267", "parent": "t1_dft6dye", "vote": -4}, {"content": "You're not a fanboy though. I use mostly dynamic languages too, but I do not bullshit about unit tests and all that crap.\n\n Mind naming a single \"cranky\" thing I said?", "id": "dftafex", "parent": "t1_dft9t4z", "vote": 2}, {"content": "I know what the method is doing. I know the meaning of the thing it returns. I need to know what fields and methods it has so that I can meaningfully interact with it.\n\nThis is when writing code, not just when debugging.", "id": "dfu43rt", "parent": "t1_dftzn3i", "vote": 2}, {"content": "You're awfully incompetent. \n\nPython is low level, get over it. You're not mentally equipped to comprehend the explanation (given you skipped it completely).", "id": "dftjrtl", "parent": "t1_dftiphp", "vote": -1}, {"content": "Unless you want to copypaste to mimick generics (which is absurd) you use interface{} or not?", "id": "dfv7t8b", "parent": "t1_dfv77fa", "vote": 1}, {"content": "I didn't know that, thanks for the information.", "id": "dftl783", "parent": "t1_dftjop3", "vote": 3}, {"content": "I totally agree. But if your familiar language is say, Java, wouldn't it be worth learning something like e Python so that when the day comes, you have something productive under your belt?", "id": "dfv1hhl", "parent": "t1_dft2nd3", "vote": 2}, {"content": "Huh, I find that Python 3 handles unicode fairly well. Not sure what you meant by \"weak typing is the rest\" given that python doesn't do implicit conversion. Certainly not between byte arrays and strings (which are always unicode). ", "id": "dfvg5ka", "parent": "t1_dfvfb5l", "vote": 1}, {"content": "What an idiot.\n\nI know all too well that there is no point in arguing with retards. Anyone who claim superiority of dynamically typed languages is a retard, so all I can do is to point out at their level of retardation to the others. I have absolutely nothing to talk about with the religious scum.", "id": "dfurs3v", "parent": "t1_dfuqca3", "vote": -1}, {"content": "> It's ok to be negative towards religious zealots.\n\n(...)\n\n> beyond redemption\n\nYou sound a lot like a religious zealot. The worst kind actually, an extremist.", "id": "dft7vg5", "parent": "t1_dft7267", "vote": 1}, {"content": "The guy writing the documentation must hate you. But then maybe he doesn't exist.", "id": "dfu55ym", "parent": "t1_dfu43rt", "vote": -1}, {"content": "[deleted]", "id": "dfugbdq", "parent": "t1_dftjrtl", "vote": 2}, {"content": "Definitely a valid point, but I would argue the majority of the need around making generics is in libraries and frameworks (not your typical business logic in a web app, as this conversation is mostly around). But in the cases you do need it in business logic, which you will at some point, it does become less than ideal.", "id": "dfv9189", "parent": "t1_dfv7t8b", "vote": 1}, {"content": "That goes for all languages. Even if you're familiar with Perl, Ruby, and C, you should still strive to be better productive using existing and new languages. It's all about finding the right tools for the job, and the more tools you know to use, the better equipped you are to find it, and utilize it.", "id": "dfvp23q", "parent": "t1_dfv1hhl", "vote": 1}, {"content": "ROTFL. What a moron. I am referring to an obvious fact that you're dumb, and dumb people stay dumb, there is no way to improve.", "id": "dft8mn0", "parent": "t1_dft7vg5", "vote": -2}, {"content": "Some of my coworkers are women.\n\nI write the documentation for my code. When I write libraries for others to consume, I tend to document them decently well.\n\nI depend on code that other people wrote, and they are responsible for documenting it. When it's for internal consumption only, a one-line doc comment is unusual, and explicitly documenting return types is rare.\n\nI could have created issues against everyone else's Python code asking them to explicitly document return types. They'd probably be resolved as wontfix pretty much immediately.\n\nSo what's your point?", "id": "dfu5pww", "parent": "t1_dfu55ym", "vote": 2}, {"content": "Ah, so it's the human's job to walk through every possible execution path and document the return type. \n\nIt's too bad computers don't know how to casework a graph, am I right? :P", "id": "dfuqe7r", "parent": "t1_dfu55ym", "vote": 1}, {"content": "Python is fairly close to the hardware in comparison to the high level languages. This is exactly how PL researchers define what \"low\" or \"high\" level means, while wikipedia gives a laymen version.\n\nAnd yes, Python is a shitty language which is not expressive, not extensible and just very poor in general. Only a complete retard will argue against the facts. But here we are only talking about its level of abstraction and nothing else. And this level is very low, it does not go too far from the semantics of the actual hardware.", "id": "dfurcq2", "parent": "t1_dfugbdq", "vote": 2}, {"content": "He is clearly trolling. Check his comment history. ", "id": "dfuqf56", "parent": "t1_dfugbdq", "vote": 1}, {"content": "I pity you", "id": "dft9akc", "parent": "t1_dft8mn0", "vote": 1}, {"content": "The point is you create your own problems. There are plenty of ways for you to do it. Those kind of problems are worse when writing Python code, but don't be fooled that they would be disappearing magically by switching to C#.", "id": "dfu7092", "parent": "t1_dfu5pww", "vote": 0}, {"content": "comments like this are just depressing... it's like seeing communities of flat earth believers popping up around the internet. people who are not only very very wrong, but hold those wrong positions with such confidence that they seem to challenge the very idea of objective reality.\n\npython: the language of one liner lazily-evaluated generator comprehensions. the language famous for being able to type\n\n    import antigravity\n\n and then boom, you can [use antigravity](https://xkcd.com/353/), no extra boilerplate, no worrying about namespaces vs. included files (because they are the same thing), no com.stupidfuckingnestedshit.enterprise.whatever. the language for which \"foreach\" is just \"for\", doesn't make you type a parentheses, semicolon or curly brace. the language for which getting the fifth to second-to-last elements of a list is\n\n     list[4:-2]\n\nand on and on, is somehow low level, not very expressive. \n\nyeah i guess the internet lets people who believe the world is flat bask in their own self-righteous glory, type scathing remarks at the sheep who would dare challenge their enlightened knowledge... but it's the absolute death of reasoning. how can you even have a discussion with someone like that?", "id": "dfwh07m", "parent": "t1_dfurcq2", "vote": 2}, {"content": "Ah, gotchya.", "id": "dfv6ryh", "parent": "t1_dfuqf56", "vote": 2}, {"content": "And another incompetent code monkey.\n\nFuck off or *prove* that Python has any features that make its semantics too distant from the hardware to be considered a low level language.", "id": "dfurq1k", "parent": "t1_dfuqf56", "vote": -2}, {"content": "ROTFL. No form of existence is more lowly and hopeless than a life of a python fanboy.", "id": "dft9m62", "parent": "t1_dft9akc", "vote": -3}, {"content": "Please, in your infinite wisdom, explain to me how I can make undocumented code that people on another team wrote several years before I joined the company instantly understandable in terms of the types involved in a method. Explain to me how I can do that faster than just trawling through the code and working through the callstack. Explain to me how failures are my fault.\n\nI'm all ears.", "id": "dfu95gx", "parent": "t1_dfu7092", "vote": 3}, {"content": "And another idiot here. Do you even know what does the word \"semantics\" mean? No? Good. Then why the fuck you think you're qualified for a discussion about the levels of abstraction you stupid code monkey?!?\n\nAnd, it is pretty obvious that you've never seen an expressive language if all this primitive pathetic shit is somehow \"expressive\" for you.", "id": "dfwh3mo", "parent": "t1_dfwh07m", "vote": 2}, {"content": "You can document that code while you're reading it through and working on it. It's called \"making a study\" in art.\n\nIt's not the fault of the language if you're not doing that already.", "id": "dfua4kr", "parent": "t1_dfu95gx", "vote": 0}, {"content": "ok nevermind, just a troll.\n\nbut in case you are actually curious about how powerful python can be, here's a pretty neat spellchecker written almost entirely in list and set comprehensions. short, powerful and readable: http://norvig.com/spell-correct.html\n\nhave a good one.", "id": "dfwhcdx", "parent": "t1_dfwh3mo", "vote": 2}, {"content": "> You can document that code while you're reading it through and working on it.\n\nI take it you've never worked at a company with a codebase shared by over a thousand developers.\n\nIf it were my team's code, I would have done that, and it would have taken a minute or two of extra time. And I would have been familiar with it already. But it was another team's code, so that's at least 30 minutes, several context switches, and a search for someone who can do a code review. Success is far from guaranteed.\n\nThis is a problem of organization standards, but I was hired fifteen years after the company was founded. I don't have a time machine to retroactively change the Python style guide to require documentation comments specifying parameter and return value types for every function.\n\nUsing a language with enforced explicit typing would in fact solve this problem, regardless of the style guide.", "id": "dfuc12g", "parent": "t1_dfua4kr", "vote": 3}, {"content": "You're a hopeless idiot. You know nothing at all about languages if you keep calling this pathetic low level shit \"powerful\".\n\nCome back when you can write a BNF which translates into an efficient parser implementation directly in Python. Come back when an ASCII-art table specifying a binary protocol translates into this protocol implementation in Python. Until it happens it is a primitive low level shitty language.\n\nEDIT: and the very fact that you think of generators and comprehensions as something \"high level\" while in reality it is nothing but a very thin syntax sugar tells everything about your level of understanding. You should never program again.", "id": "dfwhf75", "parent": "t1_dfwhcdx", "vote": 1}, {"content": "what, you mean like lisp? that shit gives me a headache, scheme was the first language i learned and it's clean and beautiful but nothing i would want to use if i just wanted to make a thing happen as fast as possible. too many parens, not enough sweet syntactic sugar to get what i want done quickly.\n\nor like yacc? i mean, yeah a domain specific language is always gonna be the best at... its specific domain. but python is a general purpose language. again, when i want to get shit done, i pop open a python file. i'm really not sure what the fuck \"an ASCII-art table specifying a binary protocol translates into this protocol implementation\" is supposed to mean but it doesn't sound like something particularly useful.", "id": "dfwhqfw", "parent": "t1_dfwhf75", "vote": 2}, {"content": "> You're a hopeless idiot\n\nLol Vitaly why are you so mad and angry all the time in this sub? You constantly insult people you have never met and never will, save your time and do something productive instead ", "id": "dfwi8vq", "parent": "t1_dfwhf75", "vote": 0}, {"content": "I mean, Python is so low level and rigid that you even cannot extend it with new high level features.\n\nAs for why those features are important, see an example here:\n\nhttp://www.moserware.com/2008/04/towards-moores-law-software-part-3-of-3.html\n\nThere is no faster way of getting shit done than using very high levels of abstraction. With Python you do not have this option, you're doomed to write tons of boilerplate.\n", "id": "dfwhvln", "parent": "t1_dfwhqfw", "vote": 2}, {"content": "What can be more infuriating than idiots who dare to have an opinion?", "id": "dfwiaqp", "parent": "t1_dfwi8vq", "vote": 2}, {"content": "uh... so you're saying that python has too much boilerplate and then pointing me at some people who wrote multiple different programming languages that are specific to their task? \n\nagain, i write python because i want something done quickly. do you write code?", "id": "dfwiauv", "parent": "t1_dfwhvln", "vote": 2}, {"content": "You're not going to change their minds or make them research the subject, you're just wasting your own time. Just move on to another comment that has a higher intellectual opinion ", "id": "dfwit9z", "parent": "t1_dfwiaqp", "vote": 1}, {"content": "Python is *forcing* you to write boilerplate code instead of getting shit done quickly. Because it is low level and rigid. Instead of solving your problem in the most efficient way possible you're forced to solve Python problems instead.\n\n", "id": "dfwidod", "parent": "t1_dfwiauv", "vote": 1}, {"content": "most of the time, when you are handed a problem to solve, you don't have the time to write an interpreter for a visualization of your problem which translates it into an intermediary form that you also define. \n\nit would be nice, because what they did looks pretty cool actually, but time is limited.\n", "id": "dfwirog", "parent": "t1_dfwidod", "vote": 1}, {"content": "For most of the real world problems it is faster to write a DSL and then solve the problem quickly than solving it with tons of boilerplate. And I am not even factoring the maintenance costs in.\n\nIt is really easy to implement DSL compilers if you use the right tools (i.e., languages that are high level enough).", "id": "dfwiyr1", "parent": "t1_dfwirog", "vote": 1}], "link": "https://www.reddit.com/r/programming/comments/63c9e1/yes_python_is_slow_and_i_dont_care/", "question": {"context": "[deleted]", "id": "63c9e1", "title": "Yes, Python is Slow, and I Don\u2019t Care"}, "resource": "Reddit"}, {"answers": [{"content": "Maybe you're not flushing the stdout buffer?", "id": "derxow1", "parent": "t3_5ypbku", "vote": 1}, {"content": "Did you try benchmarking just output?", "id": "desit1g", "parent": "t3_5ypbku", "vote": 1}, {"content": "[deleted]", "id": "derxtxo", "parent": "t1_derxow1", "vote": 1}, {"content": "i do use the end thing and this does seem right. As i said i am pretty new to this coding thing, how would i go about flushing the buffer? Don't get me wrong it does still print fast around 9 characters per second however on the school computers it is around 3x that\n \nEdit: i did google around and i found the command sys.stdout.flush() to put after my print statement, however it hasn't changed the speed of the output at all, i also tried setting flush=true on my print statement\n\n", "id": "df0qabt", "parent": "t1_derxtxo", "vote": 1}, {"content": "[deleted]", "id": "df0qm0i", "parent": "t1_df0qabt", "vote": 1}, {"content": "yeh sorry i forgot to format, i already tried setting flush to true and the -u flag, still outputs the same :/. Thanks for your help though guess i'll just have to wait a few minutes for my code to output", "id": "df0v03s", "parent": "t1_df0qm0i", "vote": 1}], "link": "https://www.reddit.com/r/Python/comments/5ypbku/python_printing_slow_on_mac/", "question": {"context": "Hey guys, I'm new to python and in school we have been learning it on Windows systems. However when i go home and load python IDLE on my mac, whenever i run the program it seems to be outputting really slowly compared to the windows version. It doesnt have anything to do with the hardware, my mac is more spec'd than the school computers however it seems to be outputting a lot slower. Solution to fix this?", "id": "5ypbku", "title": "Python printing slow on Mac?"}, "resource": "Reddit"}, {"answers": [{"content": "100k iterations isn\u2019t a lot of iterations - would be very curious to know what you\u2019re doing that makes it 20 minutes.\n\nDo you mind posting code?", "id": "i9x22hv", "parent": "t3_uxdgex", "vote": 117}, {"content": "Some options:\n\n1. Optimize the code\n  1. Profile the code and optimize the slow/longer/frequent sections\n  1. Use faster libraries like Numpy and Pandas\n1. Use a faster machine\n1. Distribute the work\n  1. Use multiple processes\n  1. Use multiple processes on multiple computers\n  1. Use multiple AWS EC2 instances\n1. Use a faster language\n  1. PyPy\n  1. Cython\n  1. C\n\nYou may get higher quality feedback if you share your code.", "id": "i9x17rg", "parent": "t3_uxdgex", "vote": 112}, {"content": "In college I took some code that had a lot of loops and was going to take days to finish and rewrote it to use numpy and other libraries instead of loops. Then it took about 15 minutes to finish the entire thing.\n\nWithout knowing more about your specific situation, it's hard to suggest concrete ideas.", "id": "i9xdfs6", "parent": "t3_uxdgex", "vote": 53}, {"content": "Is iteration 2 dependent on Iteration 1?\n\nCan vector math speed things up? E.g. Create a Vector of Random Numbers and use that for all your calculations.", "id": "i9x2phz", "parent": "t3_uxdgex", "vote": 17}, {"content": "You need to *profile* CPU and memory usage in some detail the calculation inside the loop to understand where it spends most the time. There are many possible reasons why it might be \"slow\", two typical ones being:\n\n* numerical computation that is done in pure python / not optimised using numpy or other tuned library\n* doing i/o or other bottlenecks inside the loop (store things in memory, save at the end)", "id": "i9x6rhm", "parent": "t3_uxdgex", "vote": 25}, {"content": "You haven't shared enough information. But most likely the answer will be \"use `numpy`\".\n\nLet's see an example of why this is usually a great idea for random simulations at scale. Below I will generate an array of 500 million random numbers 7 times, then compute how long on average it took to generate such a huge array. Each number is effectively a single simulation.\n\n    In [1]: import numpy as np\n\n    In [2]: %timeit np.random.random(500_000_000)\n    5.2 s \u00b1 57.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nSo yeah, about 5 seconds on average to run 500 million simulations using `numpy` on a standard issue MacBook Pro. Considerably better than several days :)\n\nThere are several ways that this setup could be tweaked to perform more complex simulations. But again, you haven't shared much info on exactly what you need, so I'll leave it here.", "id": "i9y6j8x", "parent": "t3_uxdgex", "vote": 11}, {"content": "You have to post more details. Maybe your code is just inefficient. If the data values are somewhat independent and can be parallelized you could also use the GPU/compute shader to create the 500 million data points. It would also be helpful to know what the data points look like and how large the struct is. If the data points are double you probably have to stream the data to the file system because 500 million double values are 4GB of data, nothing that could/should be stored in a single array/list.", "id": "i9xd5hj", "parent": "t3_uxdgex", "vote": 9}, {"content": "Post the code", "id": "i9x44w9", "parent": "t3_uxdgex", "vote": 10}, {"content": "Look into `concurrent.futures` combined with `numpy`", "id": "i9wyp7a", "parent": "t3_uxdgex", "vote": 14}, {"content": "This question is like calling your doctor and saying \"I'm in pain. How do I get better?\"\n\nUnfortunately I forgot my crystal ball at home today :/", "id": "i9xjfdj", "parent": "t3_uxdgex", "vote": 7}, {"content": "However you speed it up make sure to take advantage of the fact you already have something working when working. \n\nAvoid the temptation to start from scratch. Write some integration tests capturing its current output if easily classified\u2014and make sure changes don\u2019t break these. Or write unit tests for the parts you are changing after working out which are slowest-section these out if they are buried in logic. Make sure to use version control if you are not!", "id": "i9xidxs", "parent": "t3_uxdgex", "vote": 3}, {"content": "Show code", "id": "i9x7kya", "parent": "t3_uxdgex", "vote": 6}, {"content": "How many loops inside of the main loop?", "id": "i9x7d4q", "parent": "t3_uxdgex", "vote": 4}, {"content": "vectorizing will prob be the fastest option if possible. If not just multiprocess it.", "id": "i9xdmhy", "parent": "t3_uxdgex", "vote": 4}, {"content": "Try http://numba.pydata.org/\n\nIt helped with loops and was simple.", "id": "i9xfi63", "parent": "t3_uxdgex", "vote": 4}, {"content": "You can get massive speed gains by using pypy as the interpreter rather than the standard python interpreter.\n\nThe gains will depend on how much of your code is functions from libraries like numpy (they are already optimized). \n\nI brute forced a coding problem and with standard interpreter it was 22 minutes. With pypy it was 40 seconds.", "id": "i9xski7", "parent": "t3_uxdgex", "vote": 4}, {"content": "Numba", "id": "i9xsrj9", "parent": "t3_uxdgex", "vote": 2}, {"content": "Outside of a code context, if you are using a lot of math there might be a way to simplify the mathematics (series expansion of computation heavy finctions, numerical solutions instead if analytical solutions, decoupling ODE's, other first order approximations etc.). Hard to say without knowing the details of what you do.", "id": "i9xw9pm", "parent": "t3_uxdgex", "vote": 2}, {"content": "Details of task would help. The data you create, you probably won't use on laptop. Even saving and transferring it would be a challenge.", "id": "i9xypzp", "parent": "t3_uxdgex", "vote": 2}, {"content": "If your working with numbers go for \"Vectorization\".", "id": "i9z8mop", "parent": "t3_uxdgex", "vote": 2}, {"content": "You could do Vectorization; put all the values into a matrix, and perform the said operation on the matrix which would mean that the said operation is performed on all the values all at once. Numpy and Pandas libraries have good methods for this.", "id": "ia1chaw", "parent": "t3_uxdgex", "vote": 2}, {"content": "Though I haven\u2019t used it could numba help here?", "id": "i9x04go", "parent": "t3_uxdgex", "vote": 3}, {"content": "1. Multithreading/Multiprocessing/Mult instancing\n2. Is there some data that is being reutilized an you can read once before the loop?\n3. Code optimization: You would need to see which particular steps are giving you trouble and  see how to optimize\n4. Run it in C, it is faster than Python", "id": "i9xg7z7", "parent": "t3_uxdgex", "vote": 2}, {"content": "IF this is a one-time thing, run the iterations on a cloud server.  If this is an on-going thing and you need to keep this in-house, run the code on a faster GPU or build your own super-computer with raspberry pi.", "id": "i9xkhis", "parent": "t3_uxdgex", "vote": 2}, {"content": "Use a compiled language.", "id": "i9z1tr2", "parent": "t3_uxdgex", "vote": 1}, {"content": "Use parallel processing-", "id": "i9y1ypx", "parent": "t3_uxdgex", "vote": 0}, {"content": "Try multiprocessing, try multithreading - use the faster one.", "id": "i9yqt27", "parent": "t3_uxdgex", "vote": 0}, {"content": "-> don\u2019t use python", "id": "i9xh0y2", "parent": "t3_uxdgex", "vote": -3}, {"content": "I am a noob myself. From my limited experience:\n\n\\- can you break the loop delliberately at one point (for example when brute forcing numbers from 1 to 1 Million, if success is achieved at 5000, then break and do not proceed)\n\n\\- do you print out a lot of \"feedback\" (sometimes it is not necessarily the code still running, it might just be the IDE printing all the stuff)\n\n\\- simple \"decentralization\" without fancy stuff may work wonders. run the code inside a shared dropbox folder and have different machines handle different ranges of the loop (if your problem / solution allows it. I did this for calculating results for 250 cases which I shared over 5 computers)\n\nif for some reason you must bite the bullet and have it run 500 million times, make sure some \"tiny log-file\" will not multiply 500 million times and clog your machine. also at least log enough so if it should fail you at least know where.", "id": "i9xzrto", "parent": "t3_uxdgex", "vote": 1}, {"content": "If there are any pure functions, you can define them recursively, and then cache them.\n\nIf you need more workers thrown at the task, you can use multiprocessing.\n\nIf you need an overall speed increase you can also use PyPy. Otherwise C++ AND Python might be the way to go.", "id": "i9y7yht", "parent": "t3_uxdgex", "vote": 1}, {"content": "Run it in parallel using joblib: https://joblib.readthedocs.io/en/latest/parallel.html", "id": "i9yd867", "parent": "t3_uxdgex", "vote": 1}, {"content": "Probably use numpy, but depends on your code.  What does your loop look like?", "id": "i9zprsa", "parent": "t3_uxdgex", "vote": 1}, {"content": "You need to time each section of the for-loop so you can determine which areas need tuning.", "id": "i9zqztp", "parent": "t3_uxdgex", "vote": 1}, {"content": "Pretty vague question, you may optimize the generator, or unroll the loop...", "id": "i9zvfq5", "parent": "t3_uxdgex", "vote": 1}, {"content": "Can you use arrays?", "id": "ia02seg", "parent": "t3_uxdgex", "vote": 1}, {"content": "You might want to try Google Colab.", "id": "ia0719b", "parent": "t3_uxdgex", "vote": 1}, {"content": "Depending on what you're trying to do, refactor your code into OOP and split the work between multiple objects. Have each object iterate at i + starting_point, but then each object executes the same core code.", "id": "ia082hi", "parent": "t3_uxdgex", "vote": 1}, {"content": "Could you import the original data to a SQL database and have SQL handle the rest?", "id": "ia0hakp", "parent": "t3_uxdgex", "vote": 1}, {"content": "This is a good thread", "id": "ia0j8c2", "parent": "t3_uxdgex", "vote": 1}, {"content": "Depending if the data needs to be sequential, but you could use multithreading to create separate processes. You can break the data out into chunks and process them simultaneously.", "id": "ia0umev", "parent": "t3_uxdgex", "vote": 1}, {"content": "Binary search if You can sort your data", "id": "ia0v6qm", "parent": "t3_uxdgex", "vote": 1}, {"content": "Or a hashtable/dictionary", "id": "ia0v9cv", "parent": "t3_uxdgex", "vote": 1}, {"content": "Joblib.Parallel or \nMultiprocessing.Pool\n\nCheck those out for concurrent/parallel operations.", "id": "ia17kck", "parent": "t3_uxdgex", "vote": 1}, {"content": "Are you writing to disk, and how often? How much space is the data going to take up? What is the max write speed of your storage? How much ram do you have on the machine? So many questions...", "id": "ia18hnt", "parent": "t3_uxdgex", "vote": 1}, {"content": "multithreading?", "id": "ia1kjbm", "parent": "t3_uxdgex", "vote": 1}, {"content": "In the for loop I have some calculations, including timestamp addition, random value generation.  \nI add a new row of 4 columns in each iteration to a dataframe.", "id": "i9xxekd", "parent": "t1_i9x22hv", "vote": 33}, {"content": "It is 1M iterations. Perhaps OP updated the post with no edit?", "id": "i9xlcxe", "parent": "t1_i9x22hv", "vote": 9}, {"content": "To add to this. If you are working with numerical data one option is using Numba. \n\nYou get a lot of speedup for very little code overhead. \n\nIterating through rows of a pandas data frame is not very computationally efficient", "id": "i9zdqsk", "parent": "t1_i9x17rg", "vote": 5}, {"content": "> Optimize the code\n\n> Profile the code and optimize the slow/longer/frequent sections\n\non that note, I would suggest op taking a look at https://youtube.com/watch?v=oBt53YbR9Kk about dynamical programming (freecodecamp).\n\nit uses memoization and tabulation", "id": "i9yg9xs", "parent": "t1_i9x17rg", "vote": 7}, {"content": "[deleted]", "id": "i9zt4rw", "parent": "t1_i9x17rg", "vote": 2}, {"content": "GoLang!", "id": "i9z3ott", "parent": "t1_i9x17rg", "vote": -1}, {"content": "This.  \n\n\nnumpy, pandas, and other libraries uses C under the hood, and C is much faster than python on loops and other stuff, this will be my first attempt to speed this up.", "id": "i9xqqvn", "parent": "t1_i9xdfs6", "vote": 31}, {"content": "I have some calculation logics including timestamp operations, random value calculations in the for loop.  \nI'm not sure if I could done it without a loop.", "id": "i9xxjax", "parent": "t1_i9xdfs6", "vote": 3}, {"content": "I think (hope) that is the case, as it makes it non-trivial and much more interesting.\n\nI remember using lambda function in pandas and optimizing it. I think he the key to speed would be to generate an empty numpy array of a target size (millions of routes), fill in first row with the starting point data and process then.\n\nI'm not a computer science expert though, just something that was faster enough for me in the past.", "id": "i9x97vm", "parent": "t1_i9x2phz", "vote": 7}, {"content": "for profilers:\n\nonly on linux: Scalene (imo the best)\n\nalso works on windows: VizTracer, py-spy\n\nor just cProfile which is built in. (visualise cprofile with snakeviz)", "id": "ia03v1z", "parent": "t1_i9x6rhm", "vote": 2}, {"content": "Not, you don't need to profile anything.\n\nYou review the task. Review tools suitable for the job. Maybe try few. At some point you may consider profiling, but most likely you won't.", "id": "i9xxx6b", "parent": "t1_i9x6rhm", "vote": -18}, {"content": "I came to say Numpy as well.", "id": "ia0bt8a", "parent": "t1_i9y6j8x", "vote": 2}, {"content": "Numba definitely sounds like it could help here", "id": "i9xllb6", "parent": "t1_i9x04go", "vote": 1}, {"content": "Unhelpful", "id": "i9z7epl", "parent": "t1_i9z1tr2", "vote": 2}, {"content": "Still 4 CPUs, 5 minutes * 500", "id": "i9yve6a", "parent": "t1_i9yqt27", "vote": 2}, {"content": "You can write inefficient code in any language, there are plenty of things to look at before deciding to change horses.", "id": "i9zycxz", "parent": "t1_i9xh0y2", "vote": 2}, {"content": "Python with pypy can be almost as fast as C.", "id": "i9xsjc7", "parent": "t1_i9xh0y2", "vote": 1}, {"content": "How would an array help?", "id": "ia0uxr1", "parent": "t1_ia02seg", "vote": 1}, {"content": "Since this is a python sub, suggesting another language is kind of out of scope. Unless you want to recommend CPython/Cython - then okay.", "id": "ia0v52s", "parent": "t1_ia082hi", "vote": 1}, {"content": "Definitely, an OP usually doesn't get this much play without code.", "id": "ia0lyw7", "parent": "t1_ia0j8c2", "vote": 1}, {"content": "Appending to a data frame a little bit at a time is one of the slowest things you can do in Python. If you put everything into a list and then concat at the end it will be orders of magnitude faster most likely.", "id": "i9y67aq", "parent": "t1_i9xxekd", "vote": 134}, {"content": "Don't add to the dataframe in each iteration, that's slow. Add to a dict, then in the end, convert it to a dataframe.", "id": "i9y2782", "parent": "t1_i9xxekd", "vote": 173}, {"content": "Are you iterating through the dataframe, and do you absolutely need to?\n\nThere's even a warning on the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#iteration) that iterating through pandas objects is very slow. \n\nIt's hard to say without seeing exactly what you're doing, but if it's just timestamps, random values, and creating columns, you should be able to do it with df.apply() and maybe a lambda function or two.", "id": "ia00ig0", "parent": "t1_i9xxekd", "vote": 3}, {"content": "I am very willing to guess, there is a lot of wasted steps in their.", "id": "i9yps4a", "parent": "t1_i9xxekd", "vote": 11}, {"content": ">I add a new row of 4 columns in each iteration to a dataframe\n\nTo expand on why this is slow, pretty much every time you do this, you have to recopy the entire dataframe.", "id": "ia0g2bv", "parent": "t1_i9xxekd", "vote": 2}, {"content": "> It took 20 minutes to run 100000 iterations \n\n100,000", "id": "i9xns2h", "parent": "t1_i9xlcxe", "vote": 21}, {"content": "From pain. The first time I started a python program, got bored, ctrl-ced it, added a timer, and saw the time estimate of multiple years, I started learning about ways to speed it up.\n\nMy favorite method that isn't listed here is to write just the time crucial parts in C/C++, and use ctypes to use that part from python.", "id": "ia0ffhj", "parent": "t1_i9zt4rw", "vote": 5}, {"content": "It\u2019s almost certainly the dataframe manipulations you\u2019re doing.  \n\nYou may find it faster to accumulate columns in another variable then join it to the main dataframe every 10 or 100 or X number of iterations.  \n\nI think Numba may also be beneficial, though I don\u2019t have any personal experience with it. \n\nEdit: Here\u2019s a handy reference, https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "id": "i9y28xk", "parent": "t1_i9xxjax", "vote": 12}, {"content": "From what you've posted so far, I'm really guessing this has to do with just misusing dataframe operations. You shouldn't be looping over a dataframe, you should be using some dataframe functionality that vectorizes whatever operation you're attempting. Without you posting an example, it's really hard to help.", "id": "i9ye3th", "parent": "t1_i9xxjax", "vote": 9}, {"content": "We can only suggest very general ides without complete details.\n\nAre you sure every operation you do row?\n\nWhy the timestamp?  How often to you run that timestamp?  Could it be replaced with the iteration number?  Do you run it once during the loop or for every item in a given loop?\n\nRandom calculations. What is up with that?\n\nDo you have time wasters in there?  Is there a check, one field, that shows if something has changed from the previous loop and therefore might not need to be handled on this iteration...", "id": "i9yqfz4", "parent": "t1_i9xxjax", "vote": 2}, {"content": "Do you mean random calculations as in arbitrary calculations or as in they're literally using random numbers?\n\nI ask because for both situations, numpy has a few options that might significantly boost the speed of what you're doing.", "id": "i9z6egm", "parent": "t1_i9xxjax", "vote": 1}, {"content": "Don't say that about yourself.  Just keep repeating: I'm good enough, I'm smart enough, and doggone it, people like me", "id": "i9zc8c8", "parent": "t1_i9z7epl", "vote": -2}, {"content": "Rent a VM from your favorite cloud provider. You can rent a 60 CPU machine from Google Cloud for $2.51 / hr.\n\n(100000 / 20) (iterations per minute per CPY) \\* \n\n60 (CPUs) = 300000 iterations / minute\n\n 500,000,000 / 300 = 1666 (minutes) \n\n1666 /60 = 27 (hrs)\n\n27 \\* $2.51 = \\~ $80", "id": "i9zpv8f", "parent": "t1_i9yve6a", "vote": 1}, {"content": "Yep, considering right now it may just be a really obese jockey on the horse.", "id": "ia0v35a", "parent": "t1_i9zycxz", "vote": 2}, {"content": "Yeah let\u2019s overload rookie with more python-specific advanced topics instead of giving him the easiest solution, just for the sake of Python XD", "id": "i9xuyzp", "parent": "t1_i9xsjc7", "vote": -5}, {"content": "Rather than calculating each value separately, things usually go much faster if you can pass the entire array.", "id": "ia1hmj8", "parent": "t1_ia0uxr1", "vote": 1}, {"content": "I did this very thing recently using python... Maybe some of my terminology was wrong in my reply, but I'm not intending to suggest anything outside of python.", "id": "ia210wy", "parent": "t1_ia0v52s", "vote": 1}, {"content": "That speed up in a large effect!  \nI append a list to list each iteration, and save to dataframe at the end!", "id": "ia185nt", "parent": "t1_i9y67aq", "vote": 7}, {"content": "This is the way.", "id": "ia000j7", "parent": "t1_i9y2782", "vote": 17}, {"content": "I Love dict", "id": "ia1g230", "parent": "t1_i9y2782", "vote": 1}, {"content": "I am very willing to guess, there is a wasted their in there.", "id": "i9z6h12", "parent": "t1_i9yps4a", "vote": 58}, {"content": "Oh shit I counted 6 zeros\u2026. Please excuse my absent mindedness", "id": "i9xos5f", "parent": "t1_i9xns2h", "vote": 31}, {"content": "100_000", "id": "i9z9x60", "parent": "t1_i9xns2h", "vote": 5}, {"content": "Noob question here  - I feel like I saw on the anaconda distribution site a notebook (temperature vs number of bike rides ) , at the very end they loop through the df after running multiple algorithms to see which perform bests.\nIs your comment directed at large df's?\n\nI did excel monte carlo simulation with about 500k random numbers (rows) (box Mueller method ), a couple of formulas and it never took more than 5 minutes.\nMust be missing something:)", "id": "ia0hjro", "parent": "t1_i9ye3th", "vote": 1}, {"content": "I won't, thanks for the pep talk", "id": "i9zclqo", "parent": "t1_i9zc8c8", "vote": 1}, {"content": "> rookie\n\n> giving him the easiest solution \n\nSo in your mind, the easiest solution for a rookie is somehow telling them to learn a new language? Curious way of defining \"easiest\".", "id": "i9xvxxy", "parent": "t1_i9xuyzp", "vote": 10}, {"content": "You didn't even give a solution. \"Don't use python\" isn't a solution. It will take way longer to do it with pencil and paper.\n\n\"Don't use python\" is advice towards a solution, at best.", "id": "i9yg8cg", "parent": "t1_i9xuyzp", "vote": 2}, {"content": "What? My solution is extremely good.\n\nI am offering OP potentially massive speed gains and all he/she has to do is to download the pypy interpreter and use that instead of the stock one. \n\n* No learning a new language\n* No refactoring existing code\n* No learning a new feature in python such as multiprocessing\n* No spinning up multiple AWS instances", "id": "i9y9jn0", "parent": "t1_i9xuyzp", "vote": 0}, {"content": "right, but you still have to iterate through the array....", "id": "ia1kwol", "parent": "t1_ia1hmj8", "vote": 1}, {"content": "Great to hear!", "id": "ia2b9c8", "parent": "t1_ia185nt", "vote": 2}, {"content": "Can you guys ELI5 to us noobs?", "id": "ia0edfl", "parent": "t1_ia000j7", "vote": 11}, {"content": "Wheir?", "id": "i9znbbv", "parent": "t1_i9z6h12", "vote": 8}, {"content": "roasted lmao", "id": "i9zqtsa", "parent": "t1_i9z6h12", "vote": 0}, {"content": "this is the way", "id": "ia0bgz5", "parent": "t1_i9z9x60", "vote": 2}, {"content": "> Is your comment directed at large df's\n\nMy comment is only in regards to Pandas DF's. I really can't speak too much to efficiency in Excel. I use it, but only for small things. At the end of the day, looping is occurring. When talking about vectorizing the operation in pandas, it has to do with feeding pandas the function and the target data set. This let's it drop down into C code to efficiently handle it. \n\nIf you're looping in Python, you're using inefficient python code, and then paying a heavy cost to context switch for pandas to actually do its thing, and you're doing it frequently. I can't speak to any example not posted, because small details can change a lot of context.", "id": "ia0mqje", "parent": "t1_ia0hjro", "vote": 2}, {"content": "You were right about the compiled language though.\n\nI was just kidding with you.\n\nHave fun.\n\nOP, we need real information.  Post your code.", "id": "i9zels8", "parent": "t1_i9zclqo", "vote": 1}, {"content": "Yeah, because the correct solution should be dictating the technology not other way around.", "id": "i9xx3q6", "parent": "t1_i9xvxxy", "vote": -7}, {"content": "He asked for an advice not a solution lol", "id": "i9ygmqr", "parent": "t1_i9yg8cg", "vote": 1}, {"content": "No, the point is that you pass the array without having to iterate over it. That's why array based operations usually run much faster than when your try doing the same using an iterative approach.\n\nBut without knowing what OP's model looks like this is all a bit abstract, I'll admit.", "id": "ia3bpbf", "parent": "t1_ia1kwol", "vote": 1}, {"content": "Let\u2019s say you\u2019re copying stuff into a spreadsheet. If you copy things one at a time, you have to hit ctrl+c and ctrl+v a lot, and that takes time. If you copy it all at once, you only have to hit those keys once, which saves a lot of time. \n\nDatabase operations are much slower than things that happen in memory because it\u2019s an external system, so the rule of thumb is to hit the database sparingly. Collect the data in memory, then hit the database once.", "id": "ia0g1ii", "parent": "t1_ia0edfl", "vote": 17}, {"content": "Think of a dict or a list like a stack of cups.\n\nIf you tried to purchase a 1,000 cups for your party and load them into your car.  \n\nYou can purchase them one at a time.\n\n \\- take one cup off the stack\n\n \\- bring it to the register\n\n \\- pay for it\n\n \\- go to your car\n\n \\- put it in your car\n\n \\- go back to the stack of cups on the shelf.\n\nDo this 1,000 times.\n\nOr, you can take 10 sleeves of 100 cups and only go through the register once and make one trip to your car.\n\nDo you see the difference?\n\nEach time you access the dataframe it is like going through the register.\n\nHope that helps! \n\n:D\n\nalso hope it's correct.  ;)", "id": "ia0i5cv", "parent": "t1_ia0edfl", "vote": 8}, {"content": "Not an ELI5, but basically a dataframe has tons of information about the data stored in it, it's not just the data. For instance, it has an index, info about number of rows and columns in the dataframe, etc. In addition to that, each column is basically a separate series. So every time a row is added, Pandas has to recalculate and rewrite all those properties. It is also modifying n different series where n is the number of column.\n\nIn contrast, a dictionary is just data. When you modify a dictionary, you're just adding one piece of data to a dictionary and it's done. So it's much faster.\n\nIn general, using native Python data types is often faster.", "id": "ia0j743", "parent": "t1_ia0edfl", "vote": 1}, {"content": "Right heir.", "id": "ia0bqsy", "parent": "t1_i9znbbv", "vote": 6}, {"content": "Weir Everywhere! (r/GratefulDead)", "id": "ia0guje", "parent": "t1_i9znbbv", "vote": 2}, {"content": "[deleted]", "id": "i9znj9b", "parent": "t1_i9znbbv", "vote": -1}, {"content": "Gotcha and thx for the background.", "id": "ia0nyls", "parent": "t1_ia0mqje", "vote": 1}, {"content": "I said nothing about compiled languages.\n\nHaha.\n\nI will.\n\nAgreed.", "id": "i9zf9u5", "parent": "t1_i9zels8", "vote": 1}, {"content": "That's nice and all and certainly correct in a professional context but not really helpful to someone who only knows Python and wants to improve in that field.\n\nIt's like someone asking \"how can I make pizza in my electric oven\" and you going \"you shouldn't use an electric oven but an original stone pizza oven\". Ya, that's really great. Only one small problem: I DONT HAVE A STONE PIZZA OVEN! And I want to eat pizza now, not in ten years when I'm able to afford one.", "id": "i9xy4e0", "parent": "t1_i9xx3q6", "vote": 8}, {"content": "You know what? Fair. But still, just saying \"Don't use Python\" isn't really constructive.", "id": "i9yk29b", "parent": "t1_i9ygmqr", "vote": 2}, {"content": "Dataframes *are* in memory? They're just numpy arrays behind the scenes. No need to introduce network latency. I think the main issue with dataframes for many appends is that they have to reallocate often if you don't reserve enough space to start. Not sure if dicts are better in that regard but they might be, though simply reserving a properly sized numpy array would be better, if the data fits in ram.", "id": "ia0iqg2", "parent": "t1_ia0g1ii", "vote": 0}, {"content": "People were talking about dataframes, not databases", "id": "ia3ccf8", "parent": "t1_ia0g1ii", "vote": 1}, {"content": "This is one of the best ELI5's I've ever read!", "id": "ia1msq6", "parent": "t1_ia0i5cv", "vote": 2}, {"content": "Everywheir!", "id": "ia23abg", "parent": "t1_ia0guje", "vote": 2}, {"content": "\nThere <> Their    \nWhere <> Wheir    \n\nIt's no longer funny if I have to explain it.....", "id": "i9znvak", "parent": "t1_i9znj9b", "vote": 2}, {"content": "Mkay good point", "id": "i9xz7x3", "parent": "t1_i9xy4e0", "vote": 1}, {"content": "If you look at the code for [append in pandas](https://github.com/pandas-dev/pandas/blob/v0.21.1/pandas/core/frame.py#L5073-L5194) it's doing quite a bit, e.g. many isInstance checks, converting columns to a list, reindexing, and more. I think better to think pandas has a number of safety checks which you don't need if you are controlling the data better.", "id": "ia0qlts", "parent": "t1_ia0iqg2", "vote": 4}, {"content": "TBH I hadn\u2019t heard of a dataframe before commenting, and I\u2019m not much of a Python developer, but I know Java and databases. All of your points are valid. I didn\u2019t mean to imply a network connection, but a more general idea: a line of code that adds two numbers takes no time, but a line of code that invokes a library could take significantly longer depending on what it\u2019s doing underneath. You can measure performance using a profiler, or more simply by printing messages with time stamps to a log file.", "id": "ia0kuei", "parent": "t1_ia0iqg2", "vote": 2}, {"content": "awe shucks\n\n:D", "id": "ia1uy0w", "parent": "t1_ia1msq6", "vote": 1}, {"content": "[deleted]", "id": "i9znzx2", "parent": "t1_i9znvak", "vote": 1}, {"content": "I may have read your username as \u201cyour brain on Jizz\u201d\u2026", "id": "ia0h099", "parent": "t1_i9znzx2", "vote": 1}], "link": "https://www.reddit.com/r/learnpython/comments/uxdgex/i_have_a_for_loop_with_millions_iterations_to/", "question": {"context": "I have a task to create millions of simulation data with a for loop.  \n\n\nIt took 20 minutes to run 100000 iterations, and I need to run 500 million iterations, which may take several days.\n\n&#x200B;\n\nWhat is the way that I could speed up the iteration? I'm running the code on my Macbook.\n\n&#x200B;\n\nAny advice is appreciated! Thank you", "id": "uxdgex", "title": "I have a for loop with millions iterations to create simulate data, how could I speed up?"}, "resource": "Reddit"}, {"answers": [{"content": "Have you tried marking directories as excluded that aren't part of your main code base? Also disabling all the annoying popups and messages makes it feel a bit faster", "id": "etgfakn", "parent": "t3_cbl4r0", "vote": 21}, {"content": "I would make a post over on the jetbrains website, not here.  pycharm does take several seconds to fire up, but I usually just have it running all the time, so I don't have to wait for the full bootup all that often.", "id": "etgb643", "parent": "t3_cbl4r0", "vote": 17}, {"content": "I use PyCharm too, but might wanna look into VS Code. It's super fast and beautiful, I just haven't gotten round to it yet", "id": "etgnocl", "parent": "t3_cbl4r0", "vote": 16}, {"content": "As others have said, use VS Code. There are all of the Python plugins you could possibly want, and Microsoft's Python language server is very good. I use it for pretty much all of my development now (Vue.js, Python, .NET   \nCore)", "id": "ethasop", "parent": "t3_cbl4r0", "vote": 10}, {"content": "Perhaps your computer is terribly under powered for what you are trying to do with it? Mine loads up in under 3 seconds for first run. Honestly outlook takes longer to load.", "id": "eth6q0v", "parent": "t3_cbl4r0", "vote": 6}, {"content": "[deleted]", "id": "etgmwxw", "parent": "t3_cbl4r0", "vote": 8}, {"content": "Upgrade your computer hardware! Or use another program. PyCharm is a huge application and it takes up a lot of system resources. \nI use Sublime Text, which is lightweight and super fast.", "id": "etheui5", "parent": "t3_cbl4r0", "vote": 6}, {"content": "If you create a project with pycharm, the first load is slow while it indexes all the files.  Then subsequent loads only take the amount of time it takes to open pycharm, which is about 15-20 seconds.  You might benefit from a quick walkthrough on how to use pycharm.  You might not even need or use all the benefits it offers, in which case, something like sublimetext or even jupyter notebooks might be sufficient for you.  You apparently learnt Matlab, so pycharm or another decent editor will be a walk in the park.", "id": "etgu0c1", "parent": "t3_cbl4r0", "vote": 3}, {"content": "Others have suggested VS Code and I would suggest it as well. I started out with Atom (which I liked), then switched to pycharm (which I found to be overwhelming and slow) because that's what the internet recommended for python. I stuck with pycharm for a while even though I wasn't a huge fan because I didn't want to have to switch IDE/editors again, but I kept hearing about VS Code so I finally bit the bullet. VS Code has been great! I find that it can still be a little slow on larger projects, but otherwise I have not looked back once.\n\nUnless there is some specific reason you need/want pycharm, I really would recommend looking at VS Code.", "id": "etixiay", "parent": "t3_cbl4r0", "vote": 2}, {"content": "I recommend having PyCharm on an SSD.", "id": "ethhndu", "parent": "t3_cbl4r0", "vote": 3}, {"content": "VS Code ftw", "id": "eth37jf", "parent": "t3_cbl4r0", "vote": 2}, {"content": "VSCODE!! \n\nVery fast!! Microsoft just launched a \u201cPython update\u201d with better support for plots and more when running through its terminal!!! \n\nI love it.", "id": "etgws0i", "parent": "t3_cbl4r0", "vote": 3}, {"content": "or try out Thonny! Depends how much IDE support you really need during your coding.", "id": "ethqi7q", "parent": "t3_cbl4r0", "vote": 1}, {"content": "Use vim", "id": "ethrccu", "parent": "t3_cbl4r0", "vote": 1}, {"content": "Most if not all of the things in engineering have tradeoffs, so does IDEs/Editors. PyCharm does a lot for you out of the box, but it eats your resources.\n\nAlso try disabling/removing unwanted plugins, and see if you can disable antivirus scan of your own projects or PyCharm itself. (Remember to scan when you get new libraries to the venv though)\n\nIf you move to VSCode there is a plugin to enable a subset of PyCharm keyboard shortcuts.", "id": "eti1ycy", "parent": "t3_cbl4r0", "vote": 1}, {"content": "Best bet is to not use PyCharm, just use a text editor + terminal...", "id": "etgbo8j", "parent": "t3_cbl4r0", "vote": -6}, {"content": "TIL people use IDEs for python", "id": "ethqmao", "parent": "t3_cbl4r0", "vote": -1}, {"content": "Why are you closing it and reopening it?", "id": "ethzd3i", "parent": "t3_cbl4r0", "vote": 0}, {"content": "I haven\u2019t, that\u2019s a good idea, thanks. Is there a good free resource (article or YouTube) on setting up and using PyCharm as a beginner? I kind of just dove into it not knowing what I was doing, so I\u2019m sure my setup is a mess", "id": "etgiz69", "parent": "t1_etgfakn", "vote": 8}, {"content": "> several seconds\n\nI think you are underestimating the severity of the issue. I've never got it fire up in under a minute. I've gone and made a sandwich and would have it half eaten before PyCharm was ready. I'll just stick with a plain text editor.", "id": "ethwt6o", "parent": "t1_etgb643", "vote": 2}, {"content": "I made that switch and don't miss PyCharm at all. I had to tweak VS Code a little to setup the virtual environment correctly, but once that was done, it has been fantastic to use. The debug tools are also very easy to use and extremely helpful.", "id": "etix5oo", "parent": "t1_etgnocl", "vote": 2}, {"content": "I assume you can set up different environments with vscode? Sometimes I have to be in Python 2.7 (arcpy for ArcGIS must be 2.7)", "id": "ethiyl8", "parent": "t1_ethasop", "vote": 4}, {"content": "I have found VS Code to be a bit slow also. Whenever I open code the test runner extensions take a bit of time to load and the icon to show up. \nI'm not sure if it is the extensions or code's fault.", "id": "ethjid9", "parent": "t1_ethasop", "vote": 2}, {"content": "Asking the right questions.", "id": "ethpxwr", "parent": "t1_etgmwxw", "vote": 1}, {"content": "Switch to vim. You'll thank me later.", "id": "ethqo0t", "parent": "t1_etheui5", "vote": 3}, {"content": "I once used sublime but have since changed to atom and have found it to be superior in a number of ways. Check it out. I recently also started using pycharm as I've been moving to larger projects in my new job but there are a few aspects to it that I find frustrating. It's probably just a matter of working out the the new ways to do the same things. Having said all this, my first love is vim. Coupled with pudb and jedi, there's not much else you could need! Also you feel like a hacker when using vim.", "id": "etht2qi", "parent": "t1_etheui5", "vote": 1}, {"content": "Speaking from experience, it's still really slow on an SSD, even compared to a HDD. Probs helps a bit though.", "id": "ethrkv3", "parent": "t1_ethhndu", "vote": 1}, {"content": "I feel like most people who say that don't know how to use PyCharm to its fullest. \n\nThere are of course cases where a text editor and terminal (or just a terminal and shell) are preferable/easier; but for almost anything bigger than ~200 lines a proper IDE can make things way easier.\n\n---\n\nTo clarify, I'm talking about programs and applications, not necessarily scripts. One-offs or standalone scripts are (IMO) one of those cases where a text editor can be better.", "id": "etgpvdl", "parent": "t1_etgbo8j", "vote": 13}, {"content": "Well no shit - why wouldn't they?", "id": "etix8lh", "parent": "t1_ethqmao", "vote": 1}, {"content": "Not much that I am aware of. Is there a particular reason that you are using pycharm?\n\n&#x200B;\n\nIf you are not working on a big project with lots of files, an IDE like Pycharm might be an overkill if you are not utilizing its debugging tools and other stuff that comes with it. If you're just getting started, something like Jupyter Notebook might be the best environment to code in to get more immediate feedback", "id": "etgtxjc", "parent": "t1_etgiz69", "vote": 7}, {"content": "Well, there is the Pycharm documentation...\nhttps://www.jetbrains.com/pycharm/documentation/", "id": "ethr4tr", "parent": "t1_etgiz69", "vote": 2}, {"content": "Def make a post over on jetbrains, a min to launch is far too long.", "id": "etii6em", "parent": "t1_ethwt6o", "vote": 1}, {"content": "Yes you can set up multiple environments and configure almost anything you would ever want. It is so much faster than pycharm", "id": "ethmgso", "parent": "t1_ethiyl8", "vote": 3}, {"content": "Yes you can set up multiple environments and configure almost anything you would ever want. It is so much faster than pycharm", "id": "ethmhjm", "parent": "t1_ethiyl8", "vote": 1}, {"content": "Yes, and if you are using pipenv (try it) or virtualenv and you configure Code with the path where you keep your virtual environments it will automatically detect and activate the venv for that environment when you open the integrated terminal.\n\nCode + pipenv has been a very satisfying development experience.", "id": "etib0bd", "parent": "t1_ethiyl8", "vote": 1}, {"content": "I need to switch to Linux first. Then I will try Vim.", "id": "ethssqt", "parent": "t1_ethqo0t", "vote": 1}, {"content": "I like vim too, but its not that simple.", "id": "etikao7", "parent": "t1_ethqo0t", "vote": 1}, {"content": "Atom is also a nice code editor but it is slower than Sublime Text.", "id": "ethtz9w", "parent": "t1_etht2qi", "vote": 3}, {"content": "Maybe your running a slow SSD?  Mine on PCIe 2.0 (sata) maxes out at around 550MB/s up and down constant, which is as fast as my motherboard will allow for.  Years of use and no slow down.  I've got [this](https://www.amazon.com/Samsung-500GB-Internal-MZ-76E500B-AM/dp/B0781Z7Y3S/ref=sxin_1_ac_d_pm?keywords=samsung+ssd&pd_rd_i=B0781Z7Y3S&pd_rd_r=d30bb808-34b1-4100-9463-de6ebb14d8f5&pd_rd_w=nP4E2&pd_rd_wg=6IKW0&pf_rd_p=64aaff2e-3b89-4fee-a107-2469ecbc5733&pf_rd_r=7SB6D3JY24STHVRP95FM&qid=1562832658&s=gateway).\n\nBefore my current SSD my previous one would slow down to 20MB/s after years of use.  It's a massive 20x speed increase from the previous gen ssds.\n\nThe next gen PCIe 3.0 ones run at gigabytes a second ([see](https://www.amazon.com/Samsung-970-EVO-500GB-MZ-V7E500BW/dp/B07BN4NJ2J/ref=sr_1_4?keywords=samsung+ssd+970&qid=1562832878&s=gateway&sr=8-4)), and the current ones coming out for PCIe 4.0 run at around 5.5GB/s, a 10x speed increase from my generation.\n\nNot all SSDs are equal.", "id": "ethzrgi", "parent": "t1_ethrkv3", "vote": 1}, {"content": "[deleted]", "id": "etgydvb", "parent": "t1_etgpvdl", "vote": -5}, {"content": "I have written tons of full applications, modules, and libraries, many of them including thousands of lines of code across dozens of files, and have never needed anything more than Atom and iTerm. Especially for beginners, an IDE is a mistake. Just look at how many threads are on here with beginners having trouble getting their IDE to work (like this one). That's time and effort wasted that could have been spent coding.", "id": "etgumly", "parent": "t1_etgpvdl", "vote": -10}, {"content": "Becaus it's a lot of overhead for (as far as I can see) very little additional value.", "id": "etk9tkb", "parent": "t1_etix8lh", "vote": 1}, {"content": "Not that Linux is required, but it is fun to explore:  May I recommend [Mint](https://linuxmint.com/)?  It's imho the best place to start.", "id": "ethzho7", "parent": "t1_ethssqt", "vote": 2}, {"content": "You should switch to Linux, it's very nice, but vim is cross platform. If you install vim on a machine running Windows or MacOS, gvim is effectively the same as vim for almost all use cases, and vim can run as an application in the command prompt/powershell/terminal, if, for some reason, you find gvim upsetting.", "id": "etht6zx", "parent": "t1_ethssqt", "vote": 1}, {"content": "Indeed, in the context of the conversation, yours is a more salient suggestion.", "id": "ethuq55", "parent": "t1_ethtz9w", "vote": 2}, {"content": "I've got a Crucial MX500 plugged into the standard SATA port on my laptop motherboard. It's not a high spec machine, the SSD is brand new and quoted at 560mB/sec read, so it might well be my i3 slowing everything down. Everything else is super quick on that machine.", "id": "eti5nug", "parent": "t1_ethzrgi", "vote": 1}, {"content": "IMO the power of autocomplete isn't that it saves you some typing; it's that it tells you what you have available. You never have to think \"wait, which method is it called in this object?\" or \"hold on, what type is this variable?\" - the IDE tells you, right there. You never have to ask, \"what's the order of function arguments here?\" - the IDE tells you, right there. It reduces the amount of mental effort you have to do to keep track of everything. \n\nYou also get easier access to documentation; in PyCharm, anyway, you look at the thing and press Ctrl+Q. If that's not enough, you can Ctrl+Click the thing to view its source code.\n\nI totally get preference to disable autocompletion - it's helpful but sometimes it gets in the way - and every IDE I know lets you turn it off, but the code inspection tools are IMO worth it.", "id": "etgyviq", "parent": "t1_etgydvb", "vote": 14}, {"content": "I work on a project with 210K lines of code. Using just text editor in such a big project is nonsense. Try to debug, navigate, autocomplete, lint, refactor etc. anything in such big code base just using a text editor and terminal. Pycharm need like 2-3 minutes to index all the files in the project and then works just smooth.", "id": "ethvps9", "parent": "t1_etgydvb", "vote": 3}, {"content": "> never needed anything more than Atom and iTerm\n\nYou don't ***need*** anything more than sh and cat. That doesn't mean it's the most efficient way to do things.\n\nThe power of an IDE comes from its debugging, profiling, refactoring, and inspection features. Each of those tasks are certainly possible with other tools, but an IDE serves as a one-stop-shop where (ideally, anyway) everything works together in a consistent way. It's an *Integrated* Development Environment. \n\nNow, some of those things are terribly (if at all) important to a beginner, or to someone writing short-ish scripts. They aren't a requirement for anyone, for that matter; but if you learn to use those features then you're going to be a more efficient programmer.\n\nYou also don't *need* a full IDE to use features like this; there exist extensions and other programs which let you profile, debug, or whatever else in terminal/Atom/Vim/etc. My question here is: is it easier for a beginner to set up a profiler in Atom or in PyCharm?", "id": "etgvk8n", "parent": "t1_etgumly", "vote": 7}, {"content": "Weirdest gatekeeping ever. Guess it makes sense in a field with a lot of uh... eccentrics.", "id": "eth3d1o", "parent": "t1_etgumly", "vote": 5}, {"content": "I agree on some points but I will say that the static analyzer in PyCharm really helped me out with writing idiomatic Python code when I was starting out. It also caught and highlighted a lot of the small mistakes that would frustrate a new person with just a text editor. (like reusing variable names already used in an outer scope).", "id": "etgvdm1", "parent": "t1_etgumly", "vote": 5}, {"content": "Then you've never worked on code in a professional environment and/or never worked on a large project.", "id": "etlhdmm", "parent": "t1_etk9tkb", "vote": 2}, {"content": ">I once used sublime but have since changed to atom and have found it to be superior in a number of ways.\n\nIt's funny that I had just the opposite experience. I would say, I once used Atom but have since changed to Sublime Text and have found it to be superior in a number of ways! Ha ha! That's personal experience. On my system, Atom didn't even install correctly the first time.", "id": "ethv4yr", "parent": "t1_ethuq55", "vote": 1}, {"content": "Probably.  On my i5 loading PyCharm maxes out my four cores.  I suspect an i7 would make a noticeable difference.", "id": "eti8hfl", "parent": "t1_eti5nug", "vote": 2}, {"content": "[deleted]", "id": "etiigu1", "parent": "t1_ethvps9", "vote": -1}, {"content": "Why does a beginner need a profiler? \n\nIDE just puts a confusing complicated layer between a beginner and their code.", "id": "eth7nga", "parent": "t1_etgvk8n", "vote": -2}, {"content": "It's not gate keeping, it's experience. Sorry if my experience doesn't corroborate your opinion", "id": "eth7gvt", "parent": "t1_eth3d1o", "vote": -6}, {"content": "\"I'm a big, tough, programmer. I don't use no IDE.\"", "id": "etil4rt", "parent": "t1_etiigu1", "vote": 1}, {"content": "Haha you're such a goof.", "id": "eth8282", "parent": "t1_eth7gvt", "vote": 3}, {"content": "Hi a big, tough, programmer. i don't use no ide.\", I'm dad.", "id": "etila1k", "parent": "t1_etil4rt", "vote": 1}], "link": "https://www.reddit.com/r/learnpython/comments/cbl4r0/pycharm_is_frustratingly_slow/", "question": {"context": "PyCharm is insanely slow to boot, and then the first run is always at least 100x slower than subsequent (yeah, I wait until it\u2019s done opening, which feels like forever). Opening is about 10x longer than booting up Matlab. Anyone else have this issue? Thx", "id": "cbl4r0", "title": "PyCharm is frustratingly slow"}, "resource": "Reddit"}, {"answers": [{"content": "That's awesome! I'll be trying it out tomorrow!", "id": "jaaqw6d", "parent": "t3_11dhjw1", "vote": 2}, {"content": "Hey guys just added this feature to reloadium [https://github.com/reloadware/reloadium](https://github.com/reloadware/reloadium)\n\nPython debugger is a great tool which can save a lot of development time. Unfortunately it is extremely slow which makes a lot of people reluctant to use for bigger projects.\n\nI decided to add some serious optimisations to it and got some amazing results which is about 80x speed up. Now execution and startup time is the same as without using debugger.\n\nThis comes with a limitation though:- Only functions that have breakpoints in them will be debugged (by default python debugger traces everything even standard library)\n\nThis feature should still be very useful when we only want to investigate couple of functions.Other reloadium features like hot reloading will also work in fast debug mode.", "id": "ja8m8kk", "parent": "t3_11dhjw1", "vote": 2}, {"content": "I have installed plug-in but haven\u2019t used it much (even though loved the idea). Does this play well with remote shells & remote debugging? The hot reloading is probably limited by the deployment?", "id": "ja8suqd", "parent": "t3_11dhjw1", "vote": 1}, {"content": "I am testing it right now and it's working great!\n\nJust one question: Debugger seems to break on exceptions (even they are handled later on in the code\n\nfor example something like that\n\n```\ndef my_error():\n    raise Exception(\"Hello\")\n\ntry:\n    my_error()\nexcept Exception:\n    pass\n```\n\nThis code would still break at the function `my_error` and highlight the `raise Exception` line", "id": "jabz9fy", "parent": "t3_11dhjw1", "vote": 1}, {"content": "Does this work with Django and PyCharm Professional, together?", "id": "ja8nqmq", "parent": "t1_ja8m8kk", "vote": 1}, {"content": "[deleted]", "id": "ja8p0s0", "parent": "t1_ja8m8kk", "vote": 1}, {"content": "I'm getting a \"no module named relodium\" after I installed it and restarted the IDE. On github, some people said remaking the virtualenv might help, but I'm working with a large codebase and remaking my venv would take awhile", "id": "jaivrnr", "parent": "t1_ja8m8kk", "vote": 1}, {"content": "Yes, it does work with remote debugging. Starting from the newest release it automatically uploads the reloadium package too.", "id": "ja8udkp", "parent": "t1_ja8suqd", "vote": 2}, {"content": "I can't reproduce it.\n\nCould you provide a full reproducer (including line number of the breakpoint)?\n\nIs it run in the fast debug mode?", "id": "jabzyby", "parent": "t1_jabz9fy", "vote": 1}, {"content": "Yup, that's my main use case actually.", "id": "ja8o6gu", "parent": "t1_ja8nqmq", "vote": 2}, {"content": "It's fairly stable now although I added a lot of new features for the newest release. Could you paste the traceback?", "id": "ja8pdb0", "parent": "t1_ja8p0s0", "vote": 1}, {"content": "I think I know what's the cause. With that release I made it compatible with the new UI and I guess that broke it for older PyCharm versions. Updating pycharm should do the trick.", "id": "ja8qcjg", "parent": "t1_ja8p0s0", "vote": 1}, {"content": "Is it a remote interpreter? Also is your plugin version 0.9.6?", "id": "jajuyld", "parent": "t1_jaivrnr", "vote": 1}, {"content": "Maybe I misunderstand but remote shell is not the same as this vs code style ide running on the server. Pycharm has both and I lent the first one approach. That\u2019s probably clear to you but the \u201euploading the reloadium package\u201c confused me a bit.", "id": "ja8vwia", "parent": "t1_ja8udkp", "vote": 1}, {"content": "It's run in Reloadium debug mode. Fast mode actually works fine.\n\nThe example was just to explain what I meant, I am testing with complex project.\n\nOne real world example is when using (Django)\n\n`serializer.is_valid(raise_exception=True)`\n\nThis would instantly break if data provided to serializer isn't valid.\n\nIts not possible to send you the code because its proprietary", "id": "jac2py0", "parent": "t1_jabzyby", "vote": 1}, {"content": "I managed to solve the problem by changing default value for \"Break on unhandled exception\" to \"Breakpoint present\".\n\nIn my opinion, this should be the default.\n\nIts working very well now.\n\nThanks a lot", "id": "jac3gmg", "parent": "t1_jabzyby", "vote": 1}, {"content": "Guess I'll have to give it a try. Thanks!", "id": "ja8owfa", "parent": "t1_ja8o6gu", "vote": 1}, {"content": "Ok, so I've tried it out. A few questions:\n\nWhen should I use `fast debug w/ reloadium` vs `debug w/ reloadium` vs the default `debug`?  \n\nHow do I get the profiling to work? I right-clicked Enabled the `Profiling Details` and put a breakpoint in a function, but whether tracing thru or just running thru, I don't seem to get any details about how each line of a function performed.", "id": "ja965gz", "parent": "t1_ja8o6gu", "vote": 1}, {"content": "No it's a local interpreter. Yes I'm on 0.9.6. I'm also on Linux if that makes a difference.", "id": "jam6563", "parent": "t1_jajuyld", "vote": 1}, {"content": "If by remote shells you mean remote interpreters than yes it is supported.\n\n\"Uploading the reloadium package\" Means that now you don't need to install reloadium package which is needed for the plugin on the remote with \\`pip install reloadium\\` it automatically uploads for you.", "id": "jaad11u", "parent": "t1_ja8vwia", "vote": 2}, {"content": "Reloadium analyzes whether an exception will be handled in the future or not but it can only go as far as users code (or watched files) is. If exception is handled beyond that you will get unwanted breaking on exception. \n\nIn most cases this is what you want since you want to fix errors in your code base.  In other cases like yours it might be annoying.\n\nWhat I'm thinking of adding is \"Never break on this exception again\" button when it happens.\n\nI'm open to suggestions how this could be improved in a different way though!", "id": "jadez6t", "parent": "t1_jac3gmg", "vote": 1}, {"content": "\\`fast debug with reloadium\\`  - use it when starting up your project under normal debugger is slow or you want to debug functions with heavy computation. Comes with some limitations (no profiling for example, will add more documentation on that)\n\n\\`debug with relodium - use it when you want to have hot reloading and profiling enabled\\` \n\n\\`normal debug\\` - pretty much obsolete at this point. It's slow, no hot reloading, no profiling, no runtime completion.", "id": "jaacitw", "parent": "t1_ja965gz", "vote": 2}, {"content": "Hmm, not sure why that's happening. Could you make an issue on GitHub? I might need to investigate this more.", "id": "jam9ztu", "parent": "t1_jam6563", "vote": 1}, {"content": "Is it possible to break after the exception has already been raised (that is, it wasn't handled by any code)?", "id": "jadr2hh", "parent": "t1_jadez6t", "vote": 1}, {"content": "Thanks so much. I'll continue to use it and give feedback as I learn more. \ud83d\ude42", "id": "jacgaqi", "parent": "t1_jaacitw", "vote": 2}, {"content": "Unfortunately it's not possible. It's too late, the frame is already dropped from the stack.", "id": "jaflmfk", "parent": "t1_jadr2hh", "vote": 1}, {"content": "Maybe let the exception raise and if it does, you could re-run the code and pause at that exception?\n\nIts the only way I can think of to accomplish this without compromising the main selling point of the product (speed)", "id": "jagqr1a", "parent": "t1_jaflmfk", "vote": 1}, {"content": "It would only work with fully deterministic systems which is rarely the case unfortunately. The idea was to break on exceptions that happen once in a while to further investigate.", "id": "jah9vx8", "parent": "t1_jagqr1a", "vote": 2}], "link": "https://www.reddit.com/r/pycharm/comments/11dhjw1/speeding_up_python_debugger_80x/", "question": {"context": "", "id": "11dhjw1", "title": "Speeding up Python debugger 80x"}, "resource": "Reddit"}]}